{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import os\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sparse\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "RUNNING_IN_KAGGLE = 'linux' in platform.platform().lower()\n",
    "IMAGE_PATH = \"../input/osic-pulmonary-fibrosis-progression/\" if RUNNING_IN_KAGGLE else '/Users/Macbook/datasets/KaggleOSICPulmonaryFibrosisProgression'\n",
    "PROCESSED_PATH = 'FIX IT!' if RUNNING_IN_KAGGLE else '/Users/Macbook/datasets/processed-data/'  # TODO: fix this line\n",
    "\n",
    "dtype = torch.float32\n",
    "USE_GPU = True\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTDataset(Dataset):\n",
    "    _ReturnValue = namedtuple('ReturnValue', ['weeks', 'fvcs', 'features', 'masks', 'images'])\n",
    "    \n",
    "    def __init__(\n",
    "            self, root, csv_path, train=True, test_size=0.25, random_state=42):\n",
    "        \"\"\"\n",
    "        :param dataset:\n",
    "\n",
    "        :param root:\n",
    "        :param train:\n",
    "        :param train_test_split:\n",
    "        :param random_state:\n",
    "        \"\"\"\n",
    "        assert test_size is not None\n",
    "        \n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        self.csv_path = csv_path\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        if not os.path.exists(self.root):\n",
    "            raise ValueError('Data is missing')\n",
    "        \n",
    "        self._patients = list(sorted(os.listdir(self.root)))\n",
    "        \n",
    "        if self.test_size == 0:\n",
    "            self._train_patients, self._test_patients = self._patients, []\n",
    "        else:\n",
    "            self._train_patients, self._test_patients = train_test_split(\n",
    "                self._patients, test_size=self.test_size, random_state=random_state\n",
    "            )\n",
    "        \n",
    "        self._table_features = dict()\n",
    "        table_data = pd.read_csv(self.csv_path)\n",
    "        for patient in self._patients:\n",
    "            patient_data = table_data[table_data.Patient == patient]\n",
    "            \n",
    "            all_weeks = patient_data.Weeks.tolist()\n",
    "            all_fvcs = patient_data.FVC.tolist()\n",
    "            \n",
    "            all_weeks, all_fvcs = zip(*sorted(zip(all_weeks, all_fvcs), key=lambda x: x[0]))\n",
    "            \n",
    "            age = sorted(zip(*np.unique(patient_data.Age, return_counts=True)), key=lambda x: x[1])[-1][0]\n",
    "            sex = sorted(zip(*np.unique(patient_data.Sex, return_counts=True)), key=lambda x: x[1])[-1][0]\n",
    "            smoking_status = sorted(zip(*np.unique(patient_data.SmokingStatus, return_counts=True)), key=lambda x: x[1])[-1][0]\n",
    "\n",
    "            sex = [0, 1] if sex == 'Female' else [1, 0]\n",
    "            smoking_status = (\n",
    "                [1, 0, 0] if smoking_status == 'Ex-smoker' else\n",
    "                [0, 1, 0] if smoking_status == 'Never smoked' else\n",
    "                [0, 0, 1] if smoking_status == 'Currently smokes' else\n",
    "                [0, 0, 0]\n",
    "            )\n",
    "            self._table_features[patient] = (\n",
    "                all_weeks, all_fvcs, [age] + sex + smoking_status\n",
    "            )\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        patient = self._train_patients[index] if self.train else self._test_patients[index]\n",
    "        base_path = os.path.join(self.root, patient)\n",
    "\n",
    "        meta = np.load(os.path.join(base_path, 'meta.npy'), allow_pickle=True).tolist()\n",
    "        masks = sparse.load_npz(os.path.join(base_path, 'masks.npz'))\n",
    "        images = np.load(os.path.join(base_path, 'images.npy'))\n",
    "        \n",
    "        meta_processed = dict()\n",
    "        for key, values in meta.items():\n",
    "            if key in {'SliceLocation', 'InstanceNumber'}:\n",
    "                continue\n",
    "            else:\n",
    "                unique_values, values_cnt = np.unique(values, return_counts=True, axis=0)\n",
    "                most_frequent = sorted(zip(unique_values, values_cnt), key=lambda x: x[1])[-1][0]\n",
    "                most_frequent = np.array(most_frequent).reshape(-1)\n",
    "                if key in {\n",
    "                    'SliceThickness', 'TableHeight', 'WindowCenter', 'WindowWidth'\n",
    "                }:\n",
    "                    meta_processed[key] = most_frequent[0]\n",
    "                elif key == 'PixelSpacing':\n",
    "                    if len(most_frequent) == 1:\n",
    "                        meta_processed['PixelSpacingX'], meta_processed['PixelSpacingY'] = (\n",
    "                            most_frequent[0], most_frequent[0]\n",
    "                        )\n",
    "                    else:\n",
    "                        meta_processed['PixelSpacingX'], meta_processed['PixelSpacingY'] = (\n",
    "                            most_frequent[0], most_frequent[1]\n",
    "                        )\n",
    "                elif key == 'PatientPosition':\n",
    "                    pass\n",
    "                elif key == 'PositionReferenceIndicator':\n",
    "                    pass\n",
    "                    \n",
    "        all_weeks, all_fvcs, features = self._table_features[patient]\n",
    "        features = [value for key, value in meta_processed.items()] + features\n",
    "        \n",
    "        return CTDataset._ReturnValue(weeks=all_weeks, fvcs=all_fvcs, features=features, masks=masks, images=images)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._train_patients if self.train else self._test_patients)\n",
    "\n",
    "    def __repr__(self):\n",
    "        fmt_str = 'OSIC Pulmonary Fibrosis Progression Dataset ' + self.__class__.__name__ + '\\n'\n",
    "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
    "        tmp = 'train' if self.train is True else 'test'\n",
    "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
    "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
    "        return fmt_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CTDataset(\n",
    "    f'{PROCESSED_PATH}/train',\n",
    "    f'{IMAGE_PATH}/train.csv',\n",
    "    train=True, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "val_dataset = CTDataset(\n",
    "    f'{PROCESSED_PATH}/train',\n",
    "    f'{IMAGE_PATH}/train.csv',\n",
    "    train=False, test_size=0.25, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = train_dataset[0]\n",
    "len(data.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132, 44)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "torch.Size([1, 1280, 1100, 256])\n",
      "torch.Size([1, 1280, 35, 8])\n"
     ]
    }
   ],
   "source": [
    "# model = EfficientNet.from_pretrained('efficientnet-b0', in_channels=1280)\n",
    "\n",
    "# img = torch.rand(1, 1280, 1100, 256)\n",
    "# print(img.shape) \n",
    "# features = model.extract_features(img)\n",
    "# print(features.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace_loss(y_true, y_pred, log_sigma):\n",
    "    losses = np.sqrt(2) * (y_true - y_pred).abs() / log_sigma.exp() + log_sigma + np.log(2) / 2\n",
    "    return losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2D(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Net2D, self).__init__()\n",
    "        \n",
    "        self.efficient_net_1 = EfficientNet.from_pretrained('efficientnet-b0', in_channels=1)\n",
    "        self.efficient_net_2 = EfficientNet.from_pretrained('efficientnet-b0', in_channels=1280)\n",
    "\n",
    "        self.fc_1 = nn.Linear(2560 + 14, 1000)\n",
    "        self.fc_2 = nn.Linear(1000, 500)\n",
    "        self.fc_3 = nn.Linear(500, 5)\n",
    "    \n",
    "    def forward(self, X, meta_X):\n",
    "        \"\"\"\n",
    "        X: tensor (s, h, w): s - slices\n",
    "        meta_X: tensor (n, 12)\n",
    "        \"\"\"\n",
    "        s, h, w = X.shape\n",
    "        \n",
    "        X = X.unsqueeze(1) # add chanel axis (s, 1, h, w)\n",
    "        \n",
    "        X = self.efficient_net_1.extract_features(X) # shape (s, 1280, 8, 8)\n",
    "        X = X.view(s, 1280, 64) # shape (s, 1280, 64)\n",
    "        \n",
    "        X = X.unsqueeze(0) # shape (1, s, 1280, 64)\n",
    "        X = X.transpose(1, 2) # shape (1,  1280, s, 64)\n",
    "        \n",
    "        X = self.efficient_net_2.extract_features(X) # shape (1, 1280, ?, 2)\n",
    "        \n",
    "        X = torch.mean(X, dim=2) # shape (1, 1280, 2)\n",
    "        X = X.view(1, 2560)\n",
    "        \n",
    "        X = torch.cat([X.repeat(meta_X.shape[0], 1), meta_X], dim=1) \n",
    "        \n",
    "        X = F.relu(self.fc_1(X))\n",
    "        X = F.relu(self.fc_2(X))\n",
    "        y = self.fc_3(X)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polynom(coords, coefs):\n",
    "    # coords shape (n, )\n",
    "    # coefs shape (4, )\n",
    "    \n",
    "    poly_coords = torch.empty((coords.shape[0], 4)).to(device)\n",
    "    poly_coords[:, 3] = 1\n",
    "    poly_coords[:, 2] = coords\n",
    "    poly_coords[:, 1] = coords**2\n",
    "    poly_coords[:, 0] = coords**3\n",
    "    return (poly_coords * coefs.unsqueeze(0)).sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, loss_func, val_dataset):\n",
    "    running_loss = 0.0\n",
    "    model.eval()    \n",
    "    for i, data in enumerate(val_dataset):\n",
    "        #prepare lungs\n",
    "        masks = data.masks \n",
    "        images = data.images\n",
    "\n",
    "        lungs = -1000 * (1.0 - masks) + masks * images\n",
    "        X = transforms(lungs).to(device)\n",
    "\n",
    "        # prepare features\n",
    "        weeks = torch.tensor(data.all_weeks).to(device)\n",
    "        fvcs = torch.tensor(data.all_fvcs).to(device)\n",
    "        features = torch.tensor(data.features).to(device)\n",
    "\n",
    "        num_weeks = len(weeks)\n",
    "        meta_X = torch.concat([weeks.unsqueeze(1), fvcs.unsqueeze(1), \n",
    "                               features.unsqueeze(0).repeat(num_weeks, 1)], dim=1)\n",
    "\n",
    "        preds = model(X, meta_X) # shape (num_weeks, 5)\n",
    "\n",
    "        coefs = pred[: 0:4]\n",
    "        log_sigma = pred[:, 5]\n",
    "\n",
    "        loss = 0\n",
    "        for i in range(num_weeks):\n",
    "            fwc_pred = polynom(weeks, coefs[i])\n",
    "            loss += loss_func(fwc, fwc_pred, log_sigma[i])\n",
    "\n",
    "        loss /= num_weeks\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    return running_loss / len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, loss_func, train_dataset, val_dataset, epochs, scheduler=None):\n",
    "\n",
    "    def get_lr(optimizer):\n",
    "        for param_group in optimizer.param_groups:\n",
    "            return param_group['lr']\n",
    "\n",
    "    val_loss_min = np.inf\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for i, data in enumerate(train_dataset):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            #prepare lungs\n",
    "            masks = data.masks \n",
    "            images = data.images\n",
    "            \n",
    "            lungs = -1000 * (1.0 - masks) + masks * images\n",
    "            X = torch.tensor(lungs, dtype=dtype).to(device)\n",
    "\n",
    "            # prepare features\n",
    "            weeks = torch.tensor(data.weeks, dtype=dtype).to(device)\n",
    "            fvcs = torch.tensor(data.fvcs, dtype=dtype).to(device)\n",
    "            features = torch.tensor(data.features, dtype=dtype).to(device)\n",
    "            \n",
    "            num_weeks = len(weeks)\n",
    "            meta_X = torch.cat([weeks.unsqueeze(1), fvcs.unsqueeze(1), \n",
    "                                   features.unsqueeze(0).repeat(num_weeks, 1)], dim=1)\n",
    "\n",
    "            preds = model(X, meta_X) # shape (num_weeks, 5)\n",
    "\n",
    "            \n",
    "            coefs = preds[:, 0:4]\n",
    "            log_sigma = preds[:, 4]\n",
    "            \n",
    "            loss = 0.0\n",
    "            for i in range(num_weeks):\n",
    "                fvcs_pred = polynom(weeks, coefs[i])\n",
    "                loss += loss_func(fvcs, fvcs_pred, log_sigma[i])\n",
    "            \n",
    "            loss /= num_weeks\n",
    "            loss.backward() \n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "           \n",
    "            print(\"Epoch: {} \".format(epoch + 1),\n",
    "                  \"Iteration: {} \".format(i),\n",
    "                  \"lr: {:.6f} \".format(get_lr(optimizer)),\n",
    "                  \"Loss: {:.6f}.\".format(running_loss / (i + 1) ))\n",
    "            \n",
    "        running_loss /= len(train_dataset)  \n",
    "        \n",
    "#         val_loss = eval_model(model, loss_func, val_dataset)\n",
    "    \n",
    "        \n",
    "#         print(\"Epoch: {}/{}...\".format(epoch + 1, n_epochs),\n",
    "#               \"lr: {:.6f}...\".format(get_lr(optimizer)),\n",
    "#               \"Loss: {:.6f}...\".format(running_loss.item()),\n",
    "#               \"Val Loss: {:.6f}\".format(val_loss))\n",
    "#         print('------------------------------')\n",
    "\n",
    "        \n",
    "#         if val_loss <= val_loss_min:\n",
    "#             torch.save(model.state_dict(), f'./state_dict{epoch}.pt')\n",
    "#             print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(val_loss_min, val_loss))\n",
    "#             val_loss_min = val_loss\n",
    "        \n",
    "#         if scheduler is not None:\n",
    "#             scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "model = Net2D().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9, weight_decay=5e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  Iteration: 8  lr: 0.050000  Loss: 443540.250000.\n"
     ]
    }
   ],
   "source": [
    "train_model(model, optimizer, laplace_loss, train_dataset, val_dataset, epochs=1, scheduler=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
