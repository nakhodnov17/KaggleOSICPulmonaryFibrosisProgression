{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add crop for log_sigma and for abs diff for FVC\n",
    "# TODO: add normalization for data\n",
    "# TODO: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you use TorchIO for your research, please cite the following paper:\n",
      "Pérez-García et al., TorchIO: a Python library for efficient loading,\n",
      "preprocessing, augmentation and patch-based sampling of medical images\n",
      "in deep learning. Credits instructions: https://torchio.readthedocs.io/#credits\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%config Completer.use_jedi = False\n",
    "\n",
    "import os\n",
    "import platform\n",
    "from collections import namedtuple\n",
    "import time\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import tabulate\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sparse\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "# from torchvision import transforms\n",
    "# from torchsummary import summary\n",
    "# from efficientnet_pytorch_3d import EfficientNet3D\n",
    "from my_efficientnet_pytorch_3d import EfficientNet3D\n",
    "import torchio\n",
    "\n",
    "from utils import CTDataset\n",
    "\n",
    "\n",
    "########################\n",
    "\n",
    "RUNNING_IN_KAGGLE = 'linux' in platform.platform().lower()\n",
    "IMAGE_PATH = \"../input/osic-pulmonary-fibrosis-progression/\" if RUNNING_IN_KAGGLE else 'data/'\n",
    "PROCESSED_PATH = 'FIX IT!' if RUNNING_IN_KAGGLE else 'data/processed-data/'  # TODO: fix this line\n",
    "\n",
    "dtype = torch.float32\n",
    "USE_GPU = True\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeLayer(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.squeeze()\n",
    "\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, net):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net.extract_features(x.unsqueeze(0).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(torch.nn.Module):\n",
    "    _vgg_configurations = {\n",
    "        'small': [8, 'M', 8, 'M', 16, 'M', 16, 'M'],  # , 64, 'M', 64, 'M'],  # , 512, 'M', 512, 'M'],\n",
    "        8: [64, 'M', 128, 'M', 256, 'M', 512, 'M', 512, 'M'],\n",
    "        11: [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "        13: [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "        16: [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "        19: [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "    }\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_layers(cfg, batch_norm):\n",
    "        layers = []\n",
    "        in_channels = 1\n",
    "        for v in cfg:\n",
    "            if v == 'M':\n",
    "                layers += [torch.nn.MaxPool3d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [torch.nn.Conv3d(in_channels, v, kernel_size=3, padding=1)]\n",
    "                if batch_norm:\n",
    "                    layers += [torch.nn.BatchNorm3d(v)]\n",
    "                layers += [torch.nn.ReLU(inplace=True)]\n",
    "                in_channels = v\n",
    "        return layers\n",
    "\n",
    "    def __init__(self, VGG_version, batch_norm):  # num_classes, \n",
    "        super().__init__()\n",
    "        self.VGG_version = VGG_version\n",
    "        self.batch_norm = batch_norm\n",
    "#         self.num_classes = num_classes\n",
    "\n",
    "#         self.layers = torch.nn.ModuleList([\n",
    "#             # Convolution Layers\n",
    "#             *make_layers(_vgg_configurations[VGG_version], batch_norm),\n",
    "\n",
    "#             torch.nn.modules.flatten.Flatten(),\n",
    "\n",
    "#             # Fully Connected Layers\n",
    "#             torch.nn.Dropout(),\n",
    "#             torch.nn.Linear(512, 512),\n",
    "#             torch.nn.ReLU(inplace=True),\n",
    "#             torch.nn.Dropout(),\n",
    "#             torch.nn.Linear(512, 512),\n",
    "#             torch.nn.ReLU(inplace=True),\n",
    "#             torch.nn.Linear(512, num_classes),\n",
    "#         ])\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            *VGG._make_layers(self._vgg_configurations[VGG_version], batch_norm)\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "#         for module in self.modules():\n",
    "#             if isinstance(module, torch.nn.Conv3d):\n",
    "#                 torch.nn.init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
    "#                 if module.bias is not None:\n",
    "#                     torch.nn.init.constant_(module.bias, 0)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv3d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.kernel_size[2] * m.out_channels\n",
    "                m.weight.data.normal_(0, np.sqrt(2. / n))\n",
    "            elif isinstance(m, torch.nn.BatchNorm3d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):  # , target_layers: Optional[set] = None):\n",
    "#         intermediate_layers = []\n",
    "#         for idx, layer in enumerate(self.layers):\n",
    "#             if target_layers is not None and idx in target_layers:\n",
    "#                 x.requires_grad_(True)\n",
    "#                 intermediate_layers.append(x)\n",
    "#             x = layer(x)\n",
    "#         if target_layers is not None and len(self.layers) in target_layers:\n",
    "#             x.requires_grad_(True)\n",
    "#             intermediate_layers.append(x)\n",
    "\n",
    "#         if target_layers is not None:\n",
    "#             return x, intermediate_layers\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaplaceLoss(nn.Module):  # _Loss):\n",
    "    def forward(self, y_true, preds, log_sigma, metric=False):\n",
    "        abs_diff = (y_true - preds).abs()\n",
    "\n",
    "        log_sigma.clamp_(-5, 5)\n",
    "\n",
    "        if metric:\n",
    "            abs_diff.clamp_max_(1000)\n",
    "            log_sigma.clamp_(-np.log(70), np.log(70))  # TODO: min bound is strange??\n",
    "\n",
    "#         log_sigma.clamp_min_(-5)\n",
    "\n",
    "        losses = np.sqrt(2) * abs_diff / log_sigma.exp() + log_sigma + np.log(2) / 2\n",
    "        return losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OSICNet(nn.Module):\n",
    "    def __init__(self, dtype, device, efficient_net_model_number, hidden_size, dropout_rate):  # , output_size\n",
    "        super().__init__()\n",
    "\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "\n",
    "        self.CT_features_extractor = nn.Sequential(\n",
    "#             FeatureExtractor(\n",
    "#                 EfficientNet3D.from_name(\n",
    "#                     f'efficientnet-b{efficient_net_model_number}', override_params={'num_classes': 1}, in_channels=1\n",
    "#                 )\n",
    "#             ),\n",
    "            VGG('small', True),\n",
    "            nn.AdaptiveAvgPool3d(1),\n",
    "            SqueezeLayer()\n",
    "        )\n",
    "\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(16 + 4, hidden_size),  # 16 +   # 1294\n",
    "            nn.ReLU(),  # nn.Tanh(),  # \n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),  # nn.Tanh(),  # \n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, 2)  # FVC & log_sigma  # output_size\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "        self.CT_features_extractor.to(self.device)\n",
    "        self.predictor.to(self.device)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv3d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.kernel_size[2] * m.out_channels\n",
    "                m.weight.data.normal_(0, np.sqrt(2. / n))\n",
    "            elif isinstance(m, torch.nn.BatchNorm3d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        \n",
    "#     def forward(self, data):\n",
    "#         mean_dataset, std_dataset = -971.4692260919278, 117.84143467421829\n",
    "#         lungs = -1000 * (1.0 - data.masks) + data.masks * data.images\n",
    "#         lungs = (lungs - mean_dataset) / std_dataset\n",
    "#         lungs = torch.tensor(lungs, dtype=self.dtype, device=self.device)\n",
    "#         lungs_features = self.CT_features_extractor(lungs)\n",
    "\n",
    "#         data_weeks = torch.tensor(data.weeks, dtype=self.dtype)\n",
    "#         weeks = torch.empty(len(data.weeks), 4, dtype=self.dtype)\n",
    "#         weeks[:, 3] = 1\n",
    "#         weeks[:, 2] = data_weeks\n",
    "#         weeks[:, 1] = data_weeks ** 2\n",
    "#         weeks[:, 0] = data_weeks ** 3\n",
    "\n",
    "#         agg_loss = 0\n",
    "#         for week, FVC in zip(data.weeks, data.fvcs):\n",
    "#             table_features = torch.tensor(np.r_[week, FVC, data.features], dtype=self.dtype, device=self.device)\n",
    "#             X = lungs_features  # torch.cat([lungs_features, table_features])\n",
    "\n",
    "#             pred_numbers = self.predictor(X).cpu()\n",
    "#             coefs = pred_numbers[:4]\n",
    "#             log_sigma = pred_numbers[4]\n",
    "\n",
    "#             FVC_preds = (weeks * coefs).sum(dim=1)\n",
    "#             FVC_true = torch.tensor(data.fvcs, dtype=self.dtype)\n",
    "\n",
    "#             agg_loss += LaplaceLoss()(FVC_true, FVC_preds, log_sigma)\n",
    "\n",
    "#         return agg_loss / len(data.weeks)\n",
    "\n",
    "    def _normalize_data(self, data):\n",
    "        percents, weeks, FVCs, features, masks, images = data\n",
    "\n",
    "        lungs_mean, lungs_std = -971.4692260919278, 117.84143467421829\n",
    "        lungs = -1000 * (1.0 - masks) + masks * images\n",
    "        lungs = (lungs - lungs_mean) / lungs_std\n",
    "#         lungs = lungs.type(self.dtype)\n",
    "#         lungs = torch.tensor(lungs, dtype=self.dtype, device=self.device)\n",
    "#         lungs_features = self.CT_features_extractor(lungs)\n",
    "\n",
    "        percents_mean, percents_std = 77.6726, 19.8233\n",
    "        weeks_mean, weeks_std = 31.861846352485475, 23.240045178171002\n",
    "        FVCs_mean, FVCs_std = 2690.479018721756, 832.5021066817238\n",
    "#         other_ftrs_mean = np.array([\n",
    "#             2.76561876e+00,  1.42373805e+00,  1.25608294e+00,  1.33766080e+02,\n",
    "#             -5.23857955e+02, -1.24154545e+03,  6.72613636e+01,  7.89772727e-01,\n",
    "#             2.10227273e-01,  6.70454545e-01,  2.78409091e-01,  5.11363636e-02\n",
    "#         ])\n",
    "#         other_ftrs_std = np.array([\n",
    "#             2.34777445e+00, 1.47563586e-01, 9.90841780e-01, 5.85989667e+01,\n",
    "#             1.92059435e+02, 8.48268563e+02, 7.06784382e+00, 4.07469958e-01,\n",
    "#             4.07469958e-01, 4.70048134e-01, 4.48215873e-01, 2.20275818e-01\n",
    "#         ])\n",
    "\n",
    "        percents = (percents - percents_mean) / percents_std\n",
    "        weeks = (weeks - weeks_mean) / weeks_std\n",
    "        FVCs = (FVCs - FVCs_mean) / FVCs_std\n",
    "#         features = (features - other_ftrs_mean) / other_ftrs_std\n",
    "        features = features.type(self.dtype)\n",
    "#         masks = lungs\n",
    "#         images = None\n",
    "\n",
    "        return percents, weeks, FVCs, features, lungs, images\n",
    "\n",
    "    def forward(self, data):\n",
    "        percents, weeks, FVCs, features, lungs, images = self._normalize_data(data)\n",
    "\n",
    "        ###############################\n",
    "        lungs = lungs.to(dtype=self.dtype, device=self.device)\n",
    "        lungs = lungs.unsqueeze(0)  # .to(self.device)  # TODO: uncomment\n",
    "        lungs_features = self.CT_features_extractor(lungs)\n",
    "        ###############################\n",
    "\n",
    "#         weeks_mean, weeks_std = 31.861846352485475, 23.240045178171002\n",
    "#         FVCs_mean, FVCs_std = 2690.479018721756, 832.5021066817238\n",
    "#         other_ftrs_mean = np.array([\n",
    "#             2.76561876e+00,  1.42373805e+00,  1.25608294e+00,  1.33766080e+02,\n",
    "#             -5.23857955e+02, -1.24154545e+03,  6.72613636e+01,  7.89772727e-01,\n",
    "#             2.10227273e-01,  6.70454545e-01,  2.78409091e-01,  5.11363636e-02\n",
    "#         ])\n",
    "#         other_ftrs_std = np.array([\n",
    "#             2.34777445e+00, 1.47563586e-01, 9.90841780e-01, 5.85989667e+01,\n",
    "#             1.92059435e+02, 8.48268563e+02, 7.06784382e+00, 4.07469958e-01,\n",
    "#             4.07469958e-01, 4.70048134e-01, 4.48215873e-01, 2.20275818e-01\n",
    "#         ])\n",
    "\n",
    "#         data._replace(weeks = (np.array(data.weeks) - weeks_mean) / weeks_std)\n",
    "#         data._replace(fvcs = (np.array(data.fvcs) - FVCs_mean) / FVCs_std)\n",
    "#         data._replace(features = (np.array(data.features) - other_ftrs_mean) / other_ftrs_std)\n",
    "\n",
    "#         data_weeks = torch.tensor(data.weeks, dtype=self.dtype)\n",
    "#         weeks = torch.empty(len(data.weeks), 4, dtype=self.dtype)\n",
    "#         weeks[:, 3] = 1\n",
    "#         weeks[:, 2] = data_weeks\n",
    "#         weeks[:, 1] = data_weeks ** 2\n",
    "#         weeks[:, 0] = data_weeks ** 3\n",
    "\n",
    "#         agg_loss = 0\n",
    "        all_preds = []\n",
    "        for base_percent, base_week, base_FVC in zip(percents, weeks, FVCs):\n",
    "            table_features = torch.cat([\n",
    "                torch.tensor([base_percent]),\n",
    "                torch.tensor([base_week]),\n",
    "                torch.tensor([base_FVC]),\n",
    "#                 features\n",
    "            ]).to(self.device)  # torch.tensor([week]),\n",
    "\n",
    "            all_features = torch.cat([lungs_features, table_features])  # lungs_features,  # TODO: uncomment\n",
    "\n",
    "            X = torch.cat([all_features.repeat(weeks.shape[0], 1), weeks.unsqueeze(1).to(self.device)], dim=1)\n",
    "\n",
    "            preds = self.predictor(X).cpu()\n",
    "            \n",
    "#             for week in weeks:\n",
    "#                 table_features = torch.cat([torch.tensor([base_week]), torch.tensor([base_FVC]),\n",
    "#                                             torch.tensor([week]), features]).to(self.device)\n",
    "# #                                            dtype=self.dtype, device=self.device)\n",
    "#                 X = torch.cat([lungs_features, table_features])  # .to(self.device)  # lungs_features\n",
    "\n",
    "#                 pred_numbers = self.predictor(X).cpu()\n",
    "#                 preds = self.predictor(X).cpu()\n",
    "            all_preds.append(preds)\n",
    "#             coefs = pred_numbers[:4]\n",
    "#             log_sigma = pred_numbers[4]\n",
    "\n",
    "#             FVC_preds = (weeks * coefs).sum(dim=1)\n",
    "#             FVC_true = torch.tensor(data.fvcs, dtype=self.dtype)\n",
    "            \n",
    "#             agg_loss += LaplaceLoss()(FVC_true, FVC_preds, log_sigma)\n",
    "\n",
    "#         return agg_loss / len(data.weeks)\n",
    "        return all_preds\n",
    "\n",
    "#     def forward(self, data):\n",
    "#         lungs_mean, lungs_std = -971.4692260919278, 117.84143467421829\n",
    "#         lungs = -1000 * (1.0 - data.masks) + data.masks * data.images\n",
    "#         lungs = (lungs - lungs_mean) / lungs_std\n",
    "#         lungs = torch.tensor(lungs, dtype=self.dtype, device=self.device)\n",
    "#         lungs_features = self.CT_features_extractor(lungs)\n",
    "\n",
    "#         weeks_mean, weeks_std = 31.861846352485475, 23.240045178171002\n",
    "#         FVCs_mean, FVCs_std = 2690.479018721756, 832.5021066817238\n",
    "#         other_ftrs_mean = np.array([\n",
    "#             2.76561876e+00,  1.42373805e+00,  1.25608294e+00,  1.33766080e+02,\n",
    "#             -5.23857955e+02, -1.24154545e+03,  6.72613636e+01,  7.89772727e-01,\n",
    "#             2.10227273e-01,  6.70454545e-01,  2.78409091e-01,  5.11363636e-02\n",
    "#         ])\n",
    "#         other_ftrs_std = np.array([\n",
    "#             2.34777445e+00, 1.47563586e-01, 9.90841780e-01, 5.85989667e+01,\n",
    "#             1.92059435e+02, 8.48268563e+02, 7.06784382e+00, 4.07469958e-01,\n",
    "#             4.07469958e-01, 4.70048134e-01, 4.48215873e-01, 2.20275818e-01\n",
    "#         ])\n",
    "\n",
    "#         data._replace(weeks = (np.array(data.weeks) - weeks_mean) / weeks_std)\n",
    "#         data._replace(fvcs = (np.array(data.fvcs) - FVCs_mean) / FVCs_std)\n",
    "#         data._replace(features = (np.array(data.features) - other_ftrs_mean) / other_ftrs_std)\n",
    "\n",
    "# #         data_weeks = torch.tensor(data.weeks, dtype=self.dtype)\n",
    "# #         weeks = torch.empty(len(data.weeks), 4, dtype=self.dtype)\n",
    "# #         weeks[:, 3] = 1\n",
    "# #         weeks[:, 2] = data_weeks\n",
    "# #         weeks[:, 1] = data_weeks ** 2\n",
    "# #         weeks[:, 0] = data_weeks ** 3\n",
    "\n",
    "# #         agg_loss = 0\n",
    "#         all_preds = []\n",
    "#         for week, FVC in zip(data.weeks, data.fvcs):\n",
    "#             table_features = torch.tensor(np.r_[week, FVC, data.features], dtype=self.dtype, device=self.device)\n",
    "#             X = torch.cat([lungs_features, table_features])  # lungs_features\n",
    "\n",
    "#             pred_numbers = self.predictor(X).cpu()\n",
    "#             all_preds.append(pred_numbers)\n",
    "# #             coefs = pred_numbers[:4]\n",
    "# #             log_sigma = pred_numbers[4]\n",
    "\n",
    "# #             FVC_preds = (weeks * coefs).sum(dim=1)\n",
    "# #             FVC_true = torch.tensor(data.fvcs, dtype=self.dtype)\n",
    "            \n",
    "# #             agg_loss += LaplaceLoss()(FVC_true, FVC_preds, log_sigma)\n",
    "\n",
    "# #         return agg_loss / len(data.weeks)\n",
    "#         return all_preds\n",
    "\n",
    "\n",
    "class LinearDecayLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, start_epoch, stop_epoch, start_lr, stop_lr, last_epoch=-1):\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.start_epoch = start_epoch\n",
    "        self.stop_epoch = stop_epoch\n",
    "\n",
    "        self.start_lr = start_lr\n",
    "        self.stop_lr = stop_lr\n",
    "\n",
    "        self.last_epoch = last_epoch\n",
    "\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self) -> list:\n",
    "        if self.last_epoch < self.start_epoch:\n",
    "            new_lr = self.start_lr\n",
    "        elif self.last_epoch > self.stop_epoch:\n",
    "            new_lr = self.stop_lr\n",
    "        else:\n",
    "            new_lr = self.start_lr + (\n",
    "                (self.stop_lr - self.start_lr) *\n",
    "                (self.last_epoch - self.start_epoch) /\n",
    "                (self.stop_epoch - self.start_epoch)\n",
    "            )\n",
    "        return [new_lr for _ in self.optimizer.param_groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = torchio.transforms.Compose([\n",
    "    torchio.transforms.RandomAffine(\n",
    "        degrees=(10, 10),\n",
    "        translation=(-10, -10),\n",
    "        isotropic=False,\n",
    "        default_pad_value='minimum',\n",
    "        image_interpolation='linear',\n",
    "    ),\n",
    "#     torchio.transforms.RandomElasticDeformation(\n",
    "#         max_displacement=3.0\n",
    "#     )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CTDataset(\n",
    "    f'{PROCESSED_PATH}/train',\n",
    "    f'{IMAGE_PATH}/train.csv',\n",
    "    train=True,\n",
    "    transform=transforms,\n",
    "    test_size=0.25,\n",
    "    padding_mode='edge',\n",
    "    random_state=42,\n",
    "    pad_global=False,\n",
    ")\n",
    "\n",
    "test_dataset = CTDataset(\n",
    "    f'{PROCESSED_PATH}/train',\n",
    "    f'{IMAGE_PATH}/train.csv',\n",
    "    train=False,\n",
    "    transform=transforms,\n",
    "    test_size=0.25,\n",
    "    padding_mode='edge',\n",
    "    random_state=42,\n",
    "    pad_global=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# dl = DataLoader(train_dataset, batch_size=1, num_workers=4)\n",
    "# # next(iter(dl))\n",
    "\n",
    "# from tqdm.notebook import tqdm\n",
    "\n",
    "# # %%time\n",
    "\n",
    "# # for _ in tqdm(dl):\n",
    "# #     pass\n",
    "\n",
    "# # %%time\n",
    "\n",
    "# # for _ in train_dataset:\n",
    "# #     pass\n",
    "\n",
    "# # # _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OSICNet(dtype=dtype, device=device, efficient_net_model_number=0, hidden_size=100, dropout_rate=0)  # 512, 0.5)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=1e-8, momentum=0.9)  # , weight_decay=5e-4)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)  # 3e-4\n",
    "# optimizer = optim.SGD(model.parameters(), lr=1e-3)  # 3e-4\n",
    "\n",
    "\n",
    "### TMP ###\n",
    "# optimizer = optim.SGD(model.parameters(), lr=1e-9, momentum=0, weight_decay=0)  # 0.9, 5e-4)\n",
    "# lr_scheduler = LinearDecayLR(optimizer, )\n",
    "\n",
    "\n",
    "\n",
    "# for g in optimizer.param_groups:\n",
    "#     g['lr'] /= 10\n",
    "\n",
    "\n",
    "\n",
    "# from torchsummary import summary\n",
    "# summary(model.CT_features_extractor[0].net)\n",
    "\n",
    "# mmm = FeatureExtractor(\n",
    "#     EfficientNet3D.from_name(\n",
    "#         f'efficientnet-b{0}', override_params={'num_classes': 1}, in_channels=1\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# summary(mmm)\n",
    "\n",
    "# preds = model(train_dataset[0])\n",
    "# preds\n",
    "\n",
    "\n",
    "# torch.cat([torch.tensor([1]), torch.tensor([2]), torch.tensor([3]), torch.tensor([1, 2, 3])])\n",
    "\n",
    "# lungs_features.shape\n",
    "\n",
    "# preds\n",
    "# preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "log_writer = SummaryWriter(log_dir='logs/CT_features/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(writer, epoch, cur_iter, loss, metric, metric_last_3, start_time,\n",
    "                  cur_start_time, cur_end_time, log_sigmas):\n",
    "    writer.add_scalar('loss', loss.item(), epoch)\n",
    "    writer.add_scalar('metric', metric, epoch)\n",
    "    writer.add_scalar('metric (last 3)', metric_last_3, epoch)\n",
    "    writer.add_scalar('log sigma', log_sigmas.detach().mean().item(), epoch)\n",
    "    \n",
    "    columns = [\n",
    "        'Epoch', \n",
    "        'Iter', \n",
    "        'Loss', \n",
    "        'Metric', \n",
    "        'Metric (last 3)', \n",
    "        'Cur iter time', \n",
    "        'Elapsed time', \n",
    "        'Log sigma'\n",
    "    ]\n",
    "\n",
    "    values = [\n",
    "        f'{epoch + 1}',\n",
    "        f'{cur_iter + 1}',\n",
    "        f'{loss.item():0.6f}',\n",
    "        f'{metric:0.6f}',\n",
    "        f'{metric_last_3:0.6f}',\n",
    "        f'{cur_end_time - cur_start_time:0.1f} sec',\n",
    "        f'{cur_end_time - start_time:0.1f} sec',\n",
    "        f'{log_sigmas.detach().mean().item():0.3f}'\n",
    "    ]\n",
    "    table = tabulate.tabulate([values], columns, tablefmt='simple', floatfmt='8.4f')\n",
    "    if cur_iter % 40 == 0:\n",
    "        table = table.split('\\n')\n",
    "        table = '\\n'.join([table[1]] + table)\n",
    "    else:\n",
    "        table = table.split('\\n')[2]\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8a9820b56f64a65b9388b77323f7114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------  ------  --------  --------  -----------------  ---------------  --------------  -----------\n",
      "  Epoch    Iter      Loss    Metric    Metric (last 3)  Cur iter time    Elapsed time      Log sigma\n",
      "-------  ------  --------  --------  -----------------  ---------------  --------------  -----------\n",
      "      1       1    9.5931   14.8240            16.5052  0.3 sec          1.6 sec              5.0000\n",
      "      1       2    7.5358   10.4622            10.3931  0.3 sec          3.1 sec              5.0000\n",
      "      1       3   11.5412   18.7511            19.4223  0.3 sec          4.6 sec              5.0000\n",
      "      1       4    9.0299   13.6300            14.2126  0.3 sec          6.0 sec              5.0000\n",
      "      1       5    8.7546   13.0463            13.4227  0.3 sec          7.5 sec              5.0000\n",
      "      1       6   13.7582   20.2894            20.2894  0.3 sec          9.1 sec              5.0000\n",
      "      1       7    5.3495    5.8269             6.8870  0.2 sec          10.7 sec             5.0000\n",
      "      1       8    8.7831   13.3790            14.3217  0.3 sec          12.2 sec             5.0000\n",
      "      1       9    5.2620    5.6414             6.6912  0.3 sec          13.7 sec             5.0000\n",
      "      1      10    5.8861    6.9645             6.2335  0.3 sec          15.2 sec             5.0000\n",
      "      1      11    7.0413    9.4138             7.9320  0.3 sec          16.7 sec             5.0000\n",
      "      1      12    5.4972    6.1400             5.9063  0.3 sec          18.2 sec             5.0000\n",
      "      1      13    7.2347    9.8238            10.3386  0.3 sec          19.4 sec             5.0000\n",
      "      1      14    6.3928    8.1613             6.1564  0.2 sec          20.7 sec             5.0000\n",
      "      1      15   28.0491   20.2894            20.2894  0.3 sec          22.0 sec             5.0000\n",
      "      1      16    7.5922   10.5817             8.7456  0.3 sec          23.4 sec             5.0000\n",
      "      1      17    5.8735    6.9378             7.1517  0.3 sec          24.8 sec             5.0000\n",
      "      1      18   17.2862   19.8385            19.8385  0.3 sec          26.0 sec             5.0000\n",
      "      1      19    4.8175    4.6990             4.3138  0.3 sec          27.4 sec             5.0000\n",
      "      1      20   14.0925   20.2894            20.2894  0.3 sec          28.7 sec             5.0000\n",
      "      1      21    5.1772    5.4616             4.5561  0.3 sec          30.0 sec             5.0000\n",
      "      1      22    5.7661    6.8327             7.1831  0.2 sec          31.3 sec             5.0000\n",
      "      1      23    7.9487   11.3377            11.6739  0.3 sec          32.6 sec             5.0000\n",
      "      1      24   14.1382   20.6651            20.6651  0.3 sec          33.9 sec             5.0000\n",
      "      1      25    8.9265   13.4107            12.1849  0.2 sec          35.3 sec             5.0000\n",
      "      1      26   12.1575   19.4902            19.6017  0.3 sec          36.5 sec             4.9550\n",
      "      1      27    6.9082    9.1315            11.2394  0.3 sec          37.8 sec             5.0000\n",
      "      1      28   12.6641   20.1276            20.2659  0.2 sec          39.1 sec             5.0000\n",
      "      1      29    5.0232    5.1350             5.2015  0.2 sec          40.4 sec             5.0000\n",
      "      1      30    6.3501    7.9484             7.2603  0.3 sec          41.7 sec             5.0000\n",
      "      1      31   16.5229   19.8617            19.4931  0.3 sec          43.1 sec             5.0000\n",
      "      1      32    9.0642   13.8254            13.8183  0.3 sec          44.3 sec             5.0000\n",
      "      1      33    9.3802   14.3728            14.8963  0.3 sec          45.7 sec             5.0000\n",
      "      1      34   13.2129   19.7782            19.8385  0.3 sec          47.0 sec             5.0000\n",
      "      1      35    9.4056   14.3657            14.2368  0.3 sec          48.2 sec             5.0000\n",
      "      1      36    8.3626   12.2153            12.3399  0.3 sec          49.5 sec             5.0000\n",
      "      1      37    9.2438   14.0203            16.3254  0.3 sec          50.7 sec             5.0000\n",
      "      1      38    5.5322    6.2142             5.4273  0.3 sec          52.0 sec             5.0000\n",
      "      1      39    7.6994   10.8292            14.4976  0.3 sec          53.3 sec             5.0000\n",
      "      1      40    7.8214   10.8986             9.5297  0.3 sec          54.5 sec             5.0000\n",
      "-------  ------  --------  --------  -----------------  ---------------  --------------  -----------\n",
      "  Epoch    Iter      Loss    Metric    Metric (last 3)  Cur iter time    Elapsed time      Log sigma\n",
      "-------  ------  --------  --------  -----------------  ---------------  --------------  -----------\n",
      "      1      41    5.2965    5.7145             6.2137  0.2 sec          55.9 sec             5.0000\n",
      "      1      42    8.8892   13.4543            11.5166  0.3 sec          57.1 sec             5.0000\n",
      "      1      43   12.7277   18.1139            16.3099  0.3 sec          58.4 sec             5.0000\n",
      "      1      44    6.1428    7.6313             8.8796  0.2 sec          59.7 sec             5.0000\n",
      "      1      45   16.5937   20.2894            20.2894  0.2 sec          61.0 sec             5.0000\n",
      "      1      46    6.2737    7.7863             6.1815  0.3 sec          62.4 sec             5.0000\n",
      "      1      47    7.9070   11.2492            11.1496  0.3 sec          63.7 sec             5.0000\n",
      "      1      48    5.6264    6.3117             5.1112  0.3 sec          65.2 sec             5.0000\n",
      "      1      49   10.5961   16.7806            14.6403  0.2 sec          66.5 sec             5.0000\n",
      "      1      50   14.6229   19.8525            18.6876  0.3 sec          67.9 sec             5.0000\n",
      "      1      51    8.6274   12.8992            15.3171  0.3 sec          69.3 sec             5.0000\n",
      "      1      52    5.0675    5.2290             4.6028  0.3 sec          70.6 sec             5.0000\n",
      "      1      53    5.5132    6.1739             6.0919  0.3 sec          71.8 sec             5.0000\n",
      "      1      54    6.8888    9.2131            10.3142  0.3 sec          73.2 sec             5.0000\n",
      "      1      55   14.8329   20.2894            20.2894  0.3 sec          74.4 sec             5.0000\n",
      "      1      56    4.8385    4.7434             4.7256  0.2 sec          75.8 sec             5.0000\n",
      "      1      57   10.8878   17.5059            17.7616  0.3 sec          77.1 sec             5.0000\n",
      "      1      58    7.0808    9.4975             8.9468  0.3 sec          78.5 sec             5.0000\n",
      "      1      59    6.7347    8.7637             9.8887  0.3 sec          79.8 sec             5.0000\n",
      "      1      60    5.4987    6.1433             6.2838  0.3 sec          81.2 sec             5.0000\n",
      "      1      61   12.1270   19.3469            20.2010  0.3 sec          82.5 sec             5.0000\n",
      "      1      62    6.4517    8.1637             9.6040  0.3 sec          83.8 sec             5.0000\n",
      "      1      63   13.4192   20.0556            20.2894  0.3 sec          85.1 sec             5.0000\n",
      "      1      64    6.1321    7.4861            10.4377  0.3 sec          86.4 sec             5.0000\n",
      "      1      65    5.9922    7.1895             6.6889  0.3 sec          87.7 sec             5.0000\n",
      "      1      66   16.5316   18.5986            18.5986  0.2 sec          89.0 sec             5.0000\n",
      "      1      67    5.3012    5.9968             4.5877  0.3 sec          90.3 sec             5.0000\n",
      "      1      68   15.3476   20.2894            20.2894  0.3 sec          91.5 sec             4.9980\n",
      "      1      69    7.8779   11.3101            13.3255  0.3 sec          92.8 sec             5.0000\n",
      "      1      70   10.7420   16.7678            14.3303  0.3 sec          94.1 sec             5.0000\n",
      "      1      71    6.9427    9.2048             7.2237  0.3 sec          95.5 sec             5.0000\n",
      "      1      72    9.1087   13.7970            12.7032  0.3 sec          97.0 sec             5.0000\n",
      "      1      73   13.5970   20.6651            20.6651  0.3 sec          98.4 sec             5.0000\n",
      "      1      74    5.6807    6.4269             6.5173  0.2 sec          99.6 sec             5.0000\n",
      "      1      75    7.7260   10.8656            13.7263  0.3 sec          100.9 sec            5.0000\n",
      "      1      76    5.7366    6.9200             8.8026  0.2 sec          102.2 sec            5.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1      77    5.8934    6.9800             5.3111  0.3 sec          103.5 sec            5.0000\n",
      "      1      78    8.8179   13.1804            13.2985  0.3 sec          104.9 sec            5.0000\n",
      "      1      79    7.5025   10.6639            11.6907  0.3 sec          106.3 sec            5.0000\n",
      "      1      80   10.4028   16.5406            18.5834  0.3 sec          107.6 sec            5.0000\n",
      "-------  ------  --------  --------  -----------------  ---------------  --------------  -----------\n",
      "  Epoch    Iter      Loss    Metric    Metric (last 3)  Cur iter time    Elapsed time      Log sigma\n",
      "-------  ------  --------  --------  -----------------  ---------------  --------------  -----------\n",
      "      1      81    5.0811    5.2579             5.4649  0.3 sec          108.9 sec            5.0000\n",
      "      1      82    9.1990   13.9885            14.0535  0.3 sec          110.2 sec            5.0000\n",
      "      1      83    7.1502    9.6446             9.5076  0.3 sec          111.5 sec            5.0000\n",
      "      1      84   13.7384   19.8385            19.8385  0.2 sec          112.8 sec            5.0000\n",
      "      1      85    8.1895   11.8481            13.7060  0.3 sec          114.1 sec            5.0000\n",
      "      1      86   13.0818   20.2894            20.2894  0.3 sec          115.3 sec            5.0000\n",
      "      1      87   15.9319   20.2894            20.2894  0.3 sec          116.6 sec            5.0000\n",
      "      1      88    6.8402    8.8852             8.0176  0.3 sec          117.8 sec            5.0000\n",
      "      1      89    7.3213   10.0074            11.3487  0.3 sec          119.1 sec            5.0000\n",
      "      1      90    9.3664   14.3434            16.8404  0.3 sec          120.4 sec            5.0000\n",
      "      1      91    6.1698    7.6886             9.0743  0.3 sec          121.6 sec            5.0000\n",
      "      1      92    8.4175   12.3317            10.8521  0.3 sec          122.9 sec            5.0000\n",
      "      1      93    6.1577    7.6630            10.2250  0.3 sec          124.3 sec            5.0000\n",
      "      1      94   13.6078   17.7425            15.6106  0.3 sec          125.7 sec            5.0000\n",
      "      1      95   10.9272   17.3631            17.0604  0.3 sec          127.0 sec            5.0000\n",
      "      1      96    6.2688    7.7759             5.9410  0.2 sec          128.2 sec            5.0000\n",
      "      1      97    6.8319    8.9698            10.2446  0.3 sec          129.5 sec            5.0000\n",
      "      1      98    5.0260    5.1410             4.8993  0.3 sec          130.8 sec            5.0000\n",
      "      1      99    9.9148   15.6287            15.5463  0.3 sec          132.1 sec            5.0000\n",
      "      1     100   11.4159   18.0771            17.8067  0.3 sec          133.4 sec            5.0000\n",
      "      1     101   13.0794   20.2621            20.2894  0.3 sec          134.8 sec            5.0000\n",
      "      1     102    7.9989   11.4440            10.8831  0.3 sec          136.2 sec            5.0000\n",
      "      1     103   15.3021   20.2894            20.2894  0.3 sec          137.4 sec            5.0000\n",
      "      1     104    7.7533   10.9233            12.7746  0.3 sec          138.7 sec            5.0000\n",
      "      1     105   10.9542   17.7374            18.4821  0.2 sec          140.0 sec            5.0000\n",
      "      1     106   14.0886   20.2894            20.2894  0.3 sec          141.2 sec            5.0000\n",
      "      1     107    5.5003    6.1466             7.0018  0.3 sec          142.5 sec            5.0000\n",
      "      1     108    4.8160    4.6956             4.9975  0.3 sec          143.8 sec            5.0000\n",
      "      1     109    7.0360    9.6748            11.3470  0.3 sec          145.0 sec            5.0000\n",
      "      1     110    8.5850   12.6867            13.9378  0.3 sec          146.3 sec            5.0000\n",
      "      1     111    8.6159   12.7523            12.4777  0.3 sec          148.7 sec            5.0000\n",
      "      1     112   12.0711   19.4562            19.8050  0.3 sec          150.3 sec            5.0000\n",
      "      1     113   10.7112   17.0222            18.4368  0.3 sec          151.9 sec            5.0000\n",
      "      1     114    6.9949    9.3155             8.0228  0.3 sec          153.6 sec            5.0000\n",
      "      1     115   10.7281   17.0698            13.9161  0.2 sec          155.2 sec            5.0000\n",
      "      1     116   10.0368   15.7647            13.3167  0.3 sec          156.8 sec            5.0000\n",
      "      1     117    6.6350    8.5523            10.4346  0.3 sec          158.5 sec            5.0000\n",
      "      1     118    8.6515   12.7680            16.4371  0.2 sec          160.1 sec            5.0000\n",
      "      1     119   15.4515   20.2894            20.2894  0.3 sec          161.6 sec            5.0000\n",
      "      1     120    9.2915   14.1846            13.3241  0.3 sec          163.3 sec            5.0000\n",
      "-------  ------  --------  --------  -----------------  ---------------  --------------  -----------\n",
      "  Epoch    Iter      Loss    Metric    Metric (last 3)  Cur iter time    Elapsed time      Log sigma\n",
      "-------  ------  --------  --------  -----------------  ---------------  --------------  -----------\n",
      "      1     121   10.0648   15.7220            14.3253  0.3 sec          165.0 sec            5.0000\n",
      "      1     122   14.3162   20.2882            20.2851  0.2 sec          166.7 sec            5.0000\n",
      "      1     123    5.6177    6.3955             6.8405  0.3 sec          168.3 sec            5.0000\n",
      "      1     124    8.8603   13.2704            10.1837  0.2 sec          170.0 sec            5.0000\n",
      "      1     125   13.3125   19.7201            18.2022  0.3 sec          171.6 sec            5.0000\n",
      "      1     126    5.4184    5.9729             5.5346  0.3 sec          173.3 sec            5.0000\n",
      "      1     127   13.2486   20.2859            20.2894  0.3 sec          175.0 sec            5.0000\n",
      "      1     128   18.0659   20.5845            20.3427  0.3 sec          176.7 sec            5.0000\n",
      "      1     129   12.1345   19.6118            20.2894  0.3 sec          178.3 sec            5.0000\n",
      "      1     130   19.2519   20.2894            20.2894  0.3 sec          179.9 sec            5.0000\n",
      "      1     131    6.0587    7.3305             6.2155  0.3 sec          181.5 sec            5.0000\n",
      "      1     132   10.5943   17.0242            17.5178  0.3 sec          183.1 sec            5.0000\n",
      "-------  ------  --------  --------  -----------------  ---------------  --------------  -----------\n",
      "  Epoch    Iter      Loss    Metric    Metric (last 3)  Cur iter time    Elapsed time      Log sigma\n",
      "-------  ------  --------  --------  -----------------  ---------------  --------------  -----------\n",
      "      2       1    9.2597   14.1171            15.7735  0.3 sec          184.4 sec            5.0000\n",
      "      2       2    7.7465   10.9089            10.1685  0.3 sec          185.7 sec            5.0000\n",
      "      2       3   11.2058   18.1642            18.7740  0.3 sec          186.9 sec            5.0000\n",
      "      2       4    8.7041   12.9392            13.4966  0.3 sec          188.2 sec            5.0000\n",
      "      2       5    8.4490   12.3983            12.7506  0.3 sec          189.4 sec            5.0000\n",
      "      2       6   13.4003   20.2817            20.2894  0.2 sec          190.7 sec            5.0000\n",
      "      2       7    5.2201    5.5524             6.2687  0.3 sec          192.0 sec            5.0000\n",
      "      2       8    8.4895   12.7565            13.6837  0.2 sec          193.3 sec            5.0000\n",
      "      2       9    5.2146    5.5408             6.4662  0.3 sec          194.5 sec            5.0000\n",
      "      2      10    6.1476    7.5189             6.8048  0.3 sec          195.8 sec            5.0000\n",
      "      2      11    7.3461   10.0600             8.6016  0.3 sec          197.0 sec            5.0000\n",
      "      2      12    5.5590    6.2711             5.8618  0.3 sec          198.3 sec            5.0000\n",
      "      2      13    6.9402    9.1995             9.6942  0.3 sec          199.6 sec            5.0000\n",
      "      2      14    6.6703    8.7498             6.7654  0.2 sec          200.8 sec            5.0000\n",
      "      2      15   28.3469   20.2894            20.2894  0.3 sec          202.2 sec            5.0000\n",
      "      2      16    7.8840   11.2004             9.3854  0.3 sec          203.5 sec            5.0000\n",
      "      2      17    5.6931    6.5554             6.4754  0.3 sec          204.8 sec            5.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2      18   17.0295   19.8385            19.8385  0.2 sec          206.1 sec            5.0000\n",
      "      2      19    5.0724    5.2393             4.8852  0.3 sec          207.4 sec            5.0000\n",
      "      2      20   13.8299   20.2894            20.2894  0.3 sec          208.6 sec            5.0000\n",
      "      2      21    5.2837    5.6873             4.2881  0.3 sec          209.9 sec            5.0000\n",
      "      2      22    5.8155    6.9374             7.0113  0.3 sec          211.2 sec            5.0000\n",
      "      2      23    7.6856   10.7798            11.0973  0.3 sec          212.5 sec            5.0000\n",
      "      2      24   13.9012   20.6651            20.6651  0.3 sec          213.8 sec            5.0000\n",
      "      2      25    9.1644   13.9151            12.7062  0.3 sec          215.1 sec            5.0000\n",
      "      2      26   11.9313   19.1898            19.4520  0.3 sec          216.4 sec            4.9950\n",
      "      2      27    6.6349    8.5522            10.6417  0.3 sec          217.7 sec            5.0000\n",
      "      2      28   12.4502   19.9258            20.1034  0.3 sec          218.9 sec            5.0000\n",
      "      2      29    4.9229    4.9224             5.0121  0.2 sec          220.1 sec            5.0000\n",
      "      2      30    6.5299    8.3295             7.4273  0.3 sec          221.5 sec            5.0000\n",
      "      2      31   16.7217   19.9404            19.6436  0.3 sec          222.8 sec            5.0000\n",
      "      2      32    8.8316   13.3320            13.3105  0.3 sec          224.0 sec            5.0000\n",
      "      2      33    9.1417   13.8671            14.3739  0.3 sec          225.3 sec            5.0000\n",
      "      2      34   12.9956   19.7212            19.8011  0.3 sec          226.6 sec            5.0000\n",
      "      2      35    9.1697   13.9109            13.7210  0.3 sec          227.9 sec            5.0000\n",
      "      2      36    8.5743   12.6641            12.8054  0.2 sec          229.2 sec            5.0000\n",
      "      2      37    9.0524   13.6525            16.0455  0.3 sec          230.4 sec            5.0000\n",
      "      2      38    5.4335    6.0050             5.2653  0.3 sec          231.7 sec            5.0000\n",
      "      2      39    7.4792   10.4096            14.1747  0.3 sec          232.9 sec            5.0000\n",
      "      2      40    7.6417   10.5519             9.1326  0.3 sec          234.2 sec            5.0000\n",
      "-------  ------  --------  --------  -----------------  ---------------  --------------  -----------\n",
      "  Epoch    Iter      Loss    Metric    Metric (last 3)  Cur iter time    Elapsed time      Log sigma\n",
      "-------  ------  --------  --------  -----------------  ---------------  --------------  -----------\n",
      "      2      41    5.3921    5.9171             6.0656  0.3 sec          235.5 sec            5.0000\n",
      "      2      42    9.0652   13.8274            11.9035  0.3 sec          236.7 sec            5.0000\n",
      "      2      43   12.8426   18.0410            16.1704  0.3 sec          238.1 sec            5.0000\n",
      "      2      44    5.9393    7.1999             8.4344  0.3 sec          239.4 sec            5.0000\n",
      "      2      45   16.7827   20.2894            20.2894  0.3 sec          240.7 sec            5.0000\n",
      "      2      46    6.4413    8.1417             6.3538  0.3 sec          242.0 sec            5.0000\n",
      "      2      47    7.8489   11.1261            10.9921  0.3 sec          243.3 sec            5.0000\n",
      "      2      48    5.7870    6.6523             5.5610  0.3 sec          244.7 sec            5.0000\n",
      "      2      49   10.7627   17.0714            15.0093  0.3 sec          245.9 sec            5.0000\n",
      "      2      50   14.8333   19.9351            18.9905  0.3 sec          247.2 sec            5.0000\n",
      "      2      51    8.4409   12.5038            14.9091  0.3 sec          248.5 sec            5.0000\n",
      "      2      52    5.0871    5.2706             4.7615  0.3 sec          249.8 sec            5.0000\n",
      "      2      53    5.5504    6.2528             6.2458  0.3 sec          251.0 sec            5.0000\n",
      "      2      54    6.6890    8.7894             9.8784  0.2 sec          252.3 sec            5.0000\n",
      "      2      55   14.6678   20.2894            20.2894  0.3 sec          253.6 sec            5.0000\n",
      "      2      56    4.9563    4.9931             4.8501  0.3 sec          254.8 sec            5.0000\n",
      "      2      57   10.7025   17.1682            17.3562  0.3 sec          256.1 sec            5.0000\n",
      "      2      58    7.2565    9.8701             9.3339  0.3 sec          257.5 sec            5.0000\n",
      "      2      59    6.5864    8.4493             9.5607  0.3 sec          258.7 sec            5.0000\n",
      "      2      60    5.6663    6.4986             6.6524  0.3 sec          260.0 sec            5.0000\n",
      "      2      61   11.9278   19.0370            20.0504  0.3 sec          261.3 sec            5.0000\n",
      "      2      62    6.2981    7.8382             9.1934  0.3 sec          262.6 sec            5.0000\n",
      "      2      63   13.5829   20.0874            20.2894  0.3 sec          263.9 sec            5.0000\n",
      "      2      64    5.9475    7.0946            10.0328  0.3 sec          265.1 sec            5.0000\n",
      "      2      65    6.0073    7.2215             6.5705  0.3 sec          266.4 sec            5.0000\n",
      "      2      66   16.3671   18.5986            18.5986  0.3 sec          267.7 sec            5.0000\n",
      "      2      67    5.4022    6.2109             4.4684  0.2 sec          269.0 sec            5.0000\n",
      "      2      68   15.1964   20.2894            20.2894  0.3 sec          270.2 sec            5.0000\n",
      "      2      69    7.7165   10.9678            12.9718  0.3 sec          271.5 sec            5.0000\n",
      "      2      70   10.9205   16.9655            14.5863  0.3 sec          272.9 sec            5.0000\n",
      "      2      71    7.1204    9.5815             7.6122  0.3 sec          274.3 sec            5.0000\n",
      "      2      72    9.2893   14.1800            13.0996  0.3 sec          275.8 sec            5.0000\n",
      "      2      73   13.7763   20.6651            20.6651  0.3 sec          277.2 sec            5.0000\n",
      "      2      74    5.8263    6.7356             6.8408  0.3 sec          278.4 sec            5.0000\n",
      "      2      75    7.5324   10.4550            13.3029  0.3 sec          279.8 sec            5.0000\n",
      "      2      76    5.7786    7.0090             8.6956  0.3 sec          281.2 sec            5.0000\n",
      "      2      77    6.0254    7.2599             5.5414  0.3 sec          282.5 sec            5.0000\n",
      "      2      78    8.9874   13.5398            13.6711  0.3 sec          283.9 sec            5.0000\n",
      "      2      79    7.3499   10.3403            11.3581  0.3 sec          285.2 sec            5.0000\n",
      "      2      80   10.2094   16.1308            18.1615  0.3 sec          286.6 sec            5.0000\n",
      "-------  ------  --------  --------  -----------------  ---------------  --------------  -----------\n",
      "  Epoch    Iter      Loss    Metric    Metric (last 3)  Cur iter time    Elapsed time      Log sigma\n",
      "-------  ------  --------  --------  -----------------  ---------------  --------------  -----------\n",
      "      2      81    5.0950    5.2873             5.3584  0.3 sec          287.9 sec            5.0000\n",
      "      2      82    9.0218   13.6128            13.6682  0.3 sec          289.3 sec            5.0000\n",
      "      2      83    7.2672    9.8927             9.7671  0.3 sec          290.5 sec            5.0000\n",
      "      2      84   13.5783   19.8385            19.8385  0.3 sec          291.8 sec            5.0000\n",
      "      2      85    8.0467   11.5454            13.3914  0.3 sec          293.1 sec            5.0000\n",
      "      2      86   13.2095   20.2894            20.2894  0.3 sec          294.5 sec            5.0000\n",
      "      2      87   16.0578   20.2894            20.2894  0.3 sec          295.8 sec            5.0000\n",
      "      2      88    6.9597    9.1386             8.1939  0.3 sec          297.1 sec            5.0000\n",
      "      2      89    7.4408   10.2609            11.6140  0.3 sec          298.4 sec            5.0000\n",
      "      2      90    9.2314   14.0573            16.5433  0.3 sec          299.7 sec            5.0000\n",
      "      2      91    6.0377    7.4085             8.7850  0.3 sec          301.0 sec            5.0000\n",
      "      2      92    8.5695   12.6539            11.1839  0.3 sec          302.3 sec            5.0000\n",
      "      2      93    6.1805    7.7113            10.1496  0.3 sec          303.6 sec            5.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2      94   13.7375   17.7940            15.7095  0.3 sec          305.0 sec            5.0000\n",
      "      2      95   11.0585   17.5666            17.2561  0.3 sec          306.3 sec            5.0000\n",
      "      2      96    6.3731    7.9970             5.8994  0.3 sec          307.7 sec            5.0000\n",
      "      2      97    6.6983    8.6865             9.9505  0.3 sec          309.0 sec            5.0000\n",
      "      2      98    5.0856    5.2673             4.8029  0.3 sec          310.3 sec            5.0000\n",
      "      2      99    9.7579   15.2961            15.2074  0.3 sec          311.7 sec            5.0000\n",
      "      2     100   11.5198   18.2360            17.9566  0.3 sec          313.0 sec            5.0000\n",
      "      2     101   13.2233   20.2882            20.2894  0.3 sec          314.4 sec            5.0000\n",
      "      2     102    7.8600   11.1496            10.5790  0.3 sec          315.9 sec            5.0000\n",
      "      2     103   15.1940   20.2894            20.2894  0.3 sec          317.2 sec            5.0000\n",
      "      2     104    7.6467   10.6972            12.5383  0.3 sec          318.5 sec            5.0000\n",
      "      2     105   10.8256   17.4924            18.2932  0.3 sec          319.8 sec            5.0000\n",
      "      2     106   13.9866   20.2894            20.2894  0.3 sec          321.2 sec            5.0000\n",
      "      2     107    5.6132    6.3859             7.2513  0.3 sec          322.5 sec            5.0000\n",
      "      2     108    4.9265    4.9300             5.2483  0.3 sec          323.9 sec            5.0000\n",
      "      2     109    6.9148    9.4180            11.0843  0.3 sec          325.1 sec            5.0000\n",
      "      2     110    8.4449   12.3896            13.6308  0.3 sec          326.4 sec            5.0000\n",
      "      2     111    8.7600   13.0578            12.7928  0.3 sec          327.7 sec            5.0000\n",
      "      2     112   11.9465   19.2994            19.7099  0.3 sec          329.0 sec            5.0000\n",
      "      2     113   10.8040   17.1538            18.6055  0.3 sec          330.3 sec            5.0000\n",
      "      2     114    7.0933    9.5240             8.2405  0.3 sec          331.5 sec            5.0000\n",
      "      2     115   10.8581   17.3208            14.2013  0.3 sec          333.0 sec            5.0000\n",
      "      2     116   10.1812   16.0710            13.6312  0.3 sec          334.3 sec            5.0000\n",
      "      2     117    6.6968    8.6833            10.3661  0.3 sec          335.6 sec            5.0000\n",
      "      2     118    8.5450   12.5650            16.2794  0.3 sec          336.8 sec            5.0000\n",
      "      2     119   15.3430   20.2894            20.2894  0.3 sec          338.0 sec            4.9900\n",
      "      2     120    9.1747   13.9371            13.0682  0.3 sec          339.5 sec            5.0000\n",
      "-------  ------  --------  --------  -----------------  ---------------  --------------  -----------\n",
      "  Epoch    Iter      Loss    Metric    Metric (last 3)  Cur iter time    Elapsed time      Log sigma\n",
      "-------  ------  --------  --------  -----------------  ---------------  --------------  -----------\n",
      "      2     121   10.1605   15.9250            14.5378  0.3 sec          340.8 sec            5.0000\n",
      "      2     122   14.4276   20.2894            20.2894  0.3 sec          342.3 sec            5.0000\n",
      "      2     123    5.5678    6.2897             6.6015  0.3 sec          343.6 sec            5.0000\n",
      "      2     124    8.7354   13.0057             9.9108  0.3 sec          345.1 sec            5.0000\n",
      "      2     125   13.4036   19.7560            18.3337  0.3 sec          346.4 sec            5.0000\n",
      "      2     126    5.4942    6.1336             5.6969  0.3 sec          347.7 sec            5.0000\n",
      "      2     127   13.1413   20.2658            20.2894  0.3 sec          349.1 sec            5.0000\n",
      "      2     128   18.1643   20.6024            20.4142  0.3 sec          350.5 sec            5.0000\n",
      "      2     129   12.0002   19.4499            20.2605  0.3 sec          351.9 sec            5.0000\n",
      "      2     130   19.3562   20.2894            20.2894  0.3 sec          353.2 sec            5.0000\n",
      "      2     131    5.9995    7.2050             6.1430  0.3 sec          354.6 sec            5.0000\n",
      "      2     132   10.4757   16.8097            17.2599  0.3 sec          355.9 sec            5.0000\n",
      "-------  ------  --------  --------  -----------------  ---------------  --------------  -----------\n",
      "  Epoch    Iter      Loss    Metric    Metric (last 3)  Cur iter time    Elapsed time      Log sigma\n",
      "-------  ------  --------  --------  -----------------  ---------------  --------------  -----------\n",
      "      3       1    9.1428   13.8694            15.5177  0.3 sec          357.3 sec            5.0000\n",
      "      3       2    7.8220   11.0691            10.0878  0.3 sec          358.6 sec            5.0000\n",
      "      3       3   11.0977   17.9558            18.5363  0.3 sec          359.9 sec            5.0000\n",
      "      3       4    8.5759   12.6675            13.2160  0.2 sec          361.1 sec            5.0000\n",
      "      3       5    8.3413   12.1701            12.5139  0.3 sec          362.4 sec            5.0000\n",
      "      3       6   13.2862   20.2602            20.2894  0.3 sec          363.6 sec            5.0000\n",
      "      3       7    5.1729    5.4525             6.0437  0.3 sec          364.9 sec            5.0000\n",
      "      3       8    8.3768   12.5177            13.4391  0.3 sec          366.3 sec            5.0000\n",
      "      3       9    5.2235    5.5597             6.3979  0.3 sec          367.7 sec            5.0000\n",
      "      3      10    6.2423    7.7198             7.0147  0.2 sec          368.9 sec            5.0000\n"
     ]
    }
   ],
   "source": [
    "MAX_EPOCHS = 10\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in tqdm(range(MAX_EPOCHS)):\n",
    "    for cur_iter, data in enumerate(train_dataset):  # tqdm(, desc='Iteration over dataset'):\n",
    "#         if cur_iter >= 1:\n",
    "#             break\n",
    "        cur_start_time = time.time()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "#         loss = model(data)\n",
    "        percents, weeks, FVC_true, features, lungs, images = data\n",
    "        FVCs_mean, FVCs_std = 2690.479018721756, 832.5021066817238\n",
    "\n",
    "        all_preds = model(data)\n",
    "\n",
    "        agg_loss = 0\n",
    "        agg_metric = 0\n",
    "        agg_metric_last_3 = 0\n",
    "        for preds in all_preds:  # zip(, [None, ]):  # TODO: REMOVE!\n",
    "            FVC_preds, log_sigmas = preds.transpose(0, 1)\n",
    "            FVC_preds = FVC_preds * FVCs_std + FVCs_mean\n",
    "\n",
    "            agg_loss += LaplaceLoss()(FVC_true, FVC_preds, log_sigmas)\n",
    "#             agg_loss += nn.MSELoss()(FVC_true, FVC_preds).pow(0.5)  # , log_sigmas)\n",
    "            with torch.no_grad():\n",
    "                agg_metric += LaplaceLoss()(FVC_true, FVC_preds, torch.tensor(5.), metric=True).item()  # log_sigmas\n",
    "                agg_metric_last_3 += LaplaceLoss()(FVC_true[-3:], FVC_preds[-3:], torch.tensor(5.), metric=True).item()\n",
    "        loss = agg_loss / weeks.shape[0]\n",
    "        metric = agg_metric / weeks.shape[0]\n",
    "        metric_last_3 = agg_metric_last_3 / weeks.shape[0]\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        cur_end_time = time.time()\n",
    "\n",
    "        print_results(log_writer, epoch, cur_iter, loss, metric, metric_last_3,\n",
    "                      start_time, cur_start_time, cur_end_time, log_sigmas)\n",
    "\n",
    "#         print(\n",
    "#             f'Epoch {epoch + 1:3d}, '\n",
    "#             f'iter {cur_iter + 1:4d}, '\n",
    "#             f'loss {loss.item():12.6f}, '\n",
    "#             f'metric {metric:12.6f}, '\n",
    "#             f'metric (last 3) {metric_last_3:12.6f}, '\n",
    "#             f'cur iter time {cur_end_time - cur_start_time:6.1f} sec, '\n",
    "#             f'elapsed time {cur_end_time - start_time:6.1f} sec, '\n",
    "#             f'log sigma {log_sigmas.detach().mean().item()}'\n",
    "#         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in optimizer.param_groups:\n",
    "    g['lr'] /= 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.79811972337355"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(2) * 1000 / 70 + np.log(70) + np.log(2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2390, 2390, 2322, 2305, 2116, 2036, 2062, 1865, 1928, 1901, 2322])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FVC_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2279.0103, 2140.1113, 2132.3945, 2124.6780, 2116.9614, 2109.2446,\n",
       "        2086.0950, 2039.7952, 1989.6372, 1927.9043, 1715.6973],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_preds[0][:, 0] * FVCs_std + FVCs_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs_diff = (all_preds[0][:, 0] * FVCs_std + FVCs_mean - FVC_true).abs()\n",
    "losses = np.sqrt(2) * abs_diff / 70 + np.log(70) + np.log(2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7.7151, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaplaceLoss(nn.Module):  # _Loss):\n",
    "    def forward(self, y_true, preds, log_sigma, metric=False):\n",
    "        abs_diff = (y_true - preds).abs()\n",
    "\n",
    "        log_sigma.clamp_(-5, 5)\n",
    "\n",
    "        if metric:\n",
    "            print('HERE')\n",
    "            abs_diff.clamp_max_(1000)\n",
    "            print(log_sigma)\n",
    "            log_sigma.clamp_(-np.log(70), np.log(70))  # TODO: min bound is strange??\n",
    "            print(log_sigma)\n",
    "\n",
    "        print(log_sigma)\n",
    "#         log_sigma.clamp_min_(-5)\n",
    "\n",
    "        losses = np.sqrt(2) * abs_diff / log_sigma.exp() + log_sigma + np.log(2) / 2\n",
    "        return losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.7567, 5.0000, 5.0000, 5.0000, 5.0000, 5.0000, 5.0000, 5.0000, 5.0000,\n",
       "        5.0000, 5.0000], grad_fn=<AsStridedBackward>)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_sigmas.clamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.248495242049359"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4.7567, 5.0000, 5.0000, 5.0000, 5.0000, 5.0000, 5.0000, 5.0000, 5.0000,\n",
       "        5.0000, 5.0000], grad_fn=<AsStridedBackward>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_sigmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.2485)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(7.7151, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LaplaceLoss()(FVC_true, all_preds[0][:, 0] * FVCs_std + FVCs_mean, torch.tensor(np.log(70)), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HERE\n",
      "tensor([4.7567, 5.0000, 5.0000, 5.0000, 5.0000, 5.0000, 5.0000, 5.0000, 5.0000,\n",
      "        5.0000, 5.0000], grad_fn=<AsStridedBackward>)\n",
      "tensor([4.7567, 5.0000, 5.0000, 5.0000, 5.0000, 5.0000, 5.0000, 5.0000, 5.0000,\n",
      "        5.0000, 5.0000], grad_fn=<AsStridedBackward>)\n",
      "tensor([4.7567, 5.0000, 5.0000, 5.0000, 5.0000, 5.0000, 5.0000, 5.0000, 5.0000,\n",
      "        5.0000, 5.0000], grad_fn=<AsStridedBackward>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(6.8225, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LaplaceLoss()(FVC_true, all_preds[0][:, 0] * FVCs_std + FVCs_mean, log_sigmas, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(25.7638, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LaplaceLoss()(FVC_true, FVC_preds, log_sigmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(25.7757, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LaplaceLoss()(FVC_true, FVC_preds[0], log_sigmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'coefs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-b234d62209fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcoefs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'coefs' is not defined"
     ]
    }
   ],
   "source": [
    "coefs.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weeks_mean ** 3, weeks_mean ** 2, weeks_mean, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaplaceLossTMP(_Loss):\n",
    "    def forward(self, y_true, preds, log_sigma):\n",
    "        abs_diff = (y_true - preds).abs()\n",
    "#         abs_diff.clamp_max_(1_000)\n",
    "        log_sigma.clamp_(-5, 5)  # -np.log(70), np.log(70)\n",
    "#         log_sigma.clamp_min_(-5)\n",
    "        losses = np.sqrt(2) * abs_diff / log_sigma.exp() + log_sigma + np.log(2) / 2\n",
    "        return losses.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6365897236027999"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(-0.4516299068927765)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = train_dataset[0]\n",
    "\n",
    "\n",
    "# weeks_mean, weeks_std = 31.861846352485475, 23.240045178171002\n",
    "# FVCs_mean, FVCs_std = 2690.479018721756, 832.5021066817238\n",
    "\n",
    "# data_weeks = torch.tensor(data.weeks, dtype=dtype)\n",
    "# data_weeks = (data_weeks - weeks_mean) / weeks_std\n",
    "# weeks = torch.empty(len(data.weeks), 4, dtype=dtype)\n",
    "# weeks[:, 0] = 0 * data_weeks ** 3  # TODO: remove\n",
    "# weeks[:, 1] = 0 * data_weeks ** 2  # TODO: remove\n",
    "# weeks[:, 2] = data_weeks\n",
    "# weeks[:, 3] = 1\n",
    "\n",
    "# all_preds = model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x22a6482ee08>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(data.weeks, FVC_preds.detach().cpu().numpy())\n",
    "\n",
    "plt.plot(data.weeks, FVC_true.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_loss = 0\n",
    "for FVC, preds in zip(data.fvcs, all_preds):\n",
    "    coefs = preds[:4]\n",
    "    log_sigma = preds[4]\n",
    "#             print(log_sigma.item())\n",
    "\n",
    "    FVC_preds = (weeks * coefs).sum(dim=1)\n",
    "    FVC_preds = FVC_preds * FVCs_std + FVCs_mean + 1000 - 200\n",
    "    FVC_true = torch.tensor(data.fvcs, dtype=dtype)\n",
    "\n",
    "    agg_loss += LaplaceLossTMP()(FVC_true, FVC_preds, torch.tensor(np.log(70)))\n",
    "loss = agg_loss / len(data.weeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.7861, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_sigma.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# summary(model.CT_features_extractor[0].net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name, p in model.named_parameters():\n",
    "    print(f'{name[20:]:50} : {p.data.min().item():15.3e}, {p.data.max().item():15.3e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = model(train_dataset[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([-0.0239,  0.0427, -0.0162,  0.0765, -0.0407], grad_fn=<CopyBackwards>),\n",
       " tensor([-0.0596,  0.0999,  0.0310,  0.0311, -0.0328], grad_fn=<CopyBackwards>),\n",
       " tensor([-0.0534,  0.0610,  0.0263,  0.1096, -0.0509], grad_fn=<CopyBackwards>),\n",
       " tensor([-0.0468, -0.0004,  0.0322,  0.0902, -0.0749], grad_fn=<CopyBackwards>),\n",
       " tensor([-0.0137,  0.0090,  0.0003,  0.0589, -0.0920], grad_fn=<CopyBackwards>),\n",
       " tensor([-0.0302,  0.0220,  0.0029,  0.1146, -0.0387], grad_fn=<CopyBackwards>),\n",
       " tensor([-0.0218,  0.0516, -0.0411,  0.0951,  0.0158], grad_fn=<CopyBackwards>),\n",
       " tensor([-0.0205,  0.0779, -0.0219,  0.1012, -0.0544], grad_fn=<CopyBackwards>),\n",
       " tensor([ 0.0394,  0.0843, -0.0223,  0.0686, -0.0343], grad_fn=<CopyBackwards>)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_dataset[2]\n",
    "data_weeks = torch.tensor(data.weeks, dtype=dtype)\n",
    "weeks = torch.empty(len(data.weeks), 4, dtype=dtype)\n",
    "weeks[:, 0] = data_weeks ** 3\n",
    "weeks[:, 1] = data_weeks ** 2\n",
    "weeks[:, 2] = data_weeks\n",
    "weeks[:, 3] = 1\n",
    "\n",
    "# all_preds = model(data)\n",
    "\n",
    "agg_loss = 0\n",
    "for week, FVC, preds in zip(data.weeks, data.fvcs, all_preds):\n",
    "    coefs = preds[:4]\n",
    "    log_sigma = preds[4]\n",
    "\n",
    "    FVC_preds = (weeks * coefs).sum(dim=1)\n",
    "    FVC_true = torch.tensor(data.fvcs, dtype=dtype)\n",
    "\n",
    "    agg_loss += LaplaceLoss()(FVC_true, FVC_preds, log_sigma)\n",
    "loss = agg_loss / len(data.weeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4912.0527, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-8b8d69623f00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Has grad but it is None: {name[20:]:50}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{name[20:]:50} : {p.grad.data.cpu().min().item():15.3e}, {p.grad.data.cpu().max().item():15.3e}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'No grad: {name[20:]:50}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure"
     ]
    }
   ],
   "source": [
    "for name, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        if p.grad is None:\n",
    "            print(f'Has grad but it is None: {name[20:]:50}')\n",
    "        else:\n",
    "            print(f'{name[20:]:50} : {p.grad.data.cpu().min().item():15.3e}, {p.grad.data.cpu().max().item():15.3e}')\n",
    "    else:\n",
    "        print(f'No grad: {name[20:]:50}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(train_dataset)):\n",
    "    print(i, train_dataset[i].images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_all = CTDataset(\n",
    "    f'{PROCESSED_PATH}/train',\n",
    "    f'{IMAGE_PATH}/train.csv',\n",
    "    train=True, test_size=0.0, random_state=42\n",
    ")\n",
    "\n",
    "images = [-1000 * (1.0 - dataset_all[i].masks) + dataset_all[i].masks * dataset_all[i].images\n",
    "          for i in range(len(dataset_all))]\n",
    "\n",
    "sum_image = 0\n",
    "sum_sq_image = 0\n",
    "for image in images:\n",
    "    sum_image += image.sum()\n",
    "    sum_sq_image += (image ** 2).sum()\n",
    "\n",
    "N = np.prod((176., 192., 256., 256.))\n",
    "\n",
    "mean = sum_image / N\n",
    "\n",
    "mean\n",
    "\n",
    "var = sum_sq_image / N + mean ** 2 - 2 * mean * sum_image / N\n",
    "\n",
    "std = var ** 0.5\n",
    "\n",
    "mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
