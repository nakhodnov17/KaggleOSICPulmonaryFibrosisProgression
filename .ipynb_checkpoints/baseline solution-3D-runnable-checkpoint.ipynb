{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add crop for log_sigma and for abs diff for FVC\n",
    "# TODO: add normalization for data\n",
    "# TODO: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "\n",
    "import os\n",
    "import platform\n",
    "from collections import namedtuple\n",
    "import time\n",
    "\n",
    "# import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sparse\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "# from torchvision import transforms\n",
    "# from torchsummary import summary\n",
    "# from efficientnet_pytorch_3d import EfficientNet3D\n",
    "from my_efficientnet_pytorch_3d import EfficientNet3D\n",
    "\n",
    "# from utils import *\n",
    "\n",
    "\n",
    "########################\n",
    "\n",
    "RUNNING_IN_KAGGLE = 'linux' in platform.platform().lower()\n",
    "IMAGE_PATH = \"../input/osic-pulmonary-fibrosis-progression/\" if RUNNING_IN_KAGGLE else 'data/'\n",
    "PROCESSED_PATH = 'FIX IT!' if RUNNING_IN_KAGGLE else 'data/processed-data/'  # TODO: fix this line\n",
    "\n",
    "dtype = torch.float32\n",
    "USE_GPU = True\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTDataset(Dataset):\n",
    "    _ReturnValue = namedtuple('ReturnValue', ['weeks', 'fvcs', 'features', 'masks', 'images'])\n",
    "\n",
    "    def __init__(\n",
    "            self, root, csv_path, train=True, test_size=0.25, random_state=42):\n",
    "        \"\"\"\n",
    "        :param dataset:\n",
    "\n",
    "        :param root:\n",
    "        :param train:\n",
    "        :param train_test_split:\n",
    "        :param random_state:\n",
    "        \"\"\"\n",
    "        assert test_size is not None\n",
    "\n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        self.csv_path = csv_path\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "\n",
    "        if not os.path.exists(self.root):\n",
    "            raise ValueError('Data is missing')\n",
    "\n",
    "        self._patients = list(sorted(os.listdir(self.root)))\n",
    "\n",
    "        if self.test_size == 0:\n",
    "            self._train_patients, self._test_patients = self._patients, []\n",
    "        else:\n",
    "            self._train_patients, self._test_patients = train_test_split(\n",
    "                self._patients, test_size=self.test_size, random_state=random_state\n",
    "            )\n",
    "\n",
    "        self._table_features = dict()\n",
    "        table_data = pd.read_csv(self.csv_path)\n",
    "        for patient in self._patients:\n",
    "            patient_data = table_data[table_data.Patient == patient]\n",
    "\n",
    "            all_weeks = patient_data.Weeks.tolist()\n",
    "            all_fvcs = patient_data.FVC.tolist()\n",
    "\n",
    "            all_weeks, all_fvcs = zip(*sorted(zip(all_weeks, all_fvcs), key=lambda x: x[0]))\n",
    "\n",
    "            age = sorted(zip(*np.unique(patient_data.Age, return_counts=True)), key=lambda x: x[1])[-1][0]\n",
    "            sex = sorted(zip(*np.unique(patient_data.Sex, return_counts=True)), key=lambda x: x[1])[-1][0]\n",
    "            smoking_status = sorted(zip(*np.unique(patient_data.SmokingStatus, return_counts=True)), key=lambda x: x[1])[-1][0]\n",
    "\n",
    "            sex = [0, 1] if sex == 'Female' else [1, 0]\n",
    "            smoking_status = (\n",
    "                [1, 0, 0] if smoking_status == 'Ex-smoker' else\n",
    "                [0, 1, 0] if smoking_status == 'Never smoked' else\n",
    "                [0, 0, 1] if smoking_status == 'Currently smokes' else\n",
    "                [0, 0, 0]\n",
    "            )\n",
    "            self._table_features[patient] = (\n",
    "                all_weeks, all_fvcs, [age] + sex + smoking_status\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        patient = self._train_patients[index] if self.train else self._test_patients[index]\n",
    "        base_path = os.path.join(self.root, patient)\n",
    "\n",
    "        meta = np.load(os.path.join(base_path, 'meta.npy'), allow_pickle=True).tolist()\n",
    "        masks = sparse.load_npz(os.path.join(base_path, 'masks.npz'))\n",
    "        images = np.load(os.path.join(base_path, 'images.npy'))\n",
    "\n",
    "        meta_processed = dict()\n",
    "        for key, values in meta.items():\n",
    "            if key in {'SliceLocation', 'InstanceNumber'}:\n",
    "                continue\n",
    "            else:\n",
    "                unique_values, values_cnt = np.unique(values, return_counts=True, axis=0)\n",
    "                most_frequent = sorted(zip(unique_values, values_cnt), key=lambda x: x[1])[-1][0]\n",
    "                most_frequent = np.array(most_frequent).reshape(-1)\n",
    "                if key in {\n",
    "                    'SliceThickness', 'TableHeight', 'WindowCenter', 'WindowWidth'\n",
    "                }:\n",
    "                    meta_processed[key] = most_frequent[0]\n",
    "                elif key == 'PixelSpacing':\n",
    "                    if len(most_frequent) == 1:\n",
    "                        meta_processed['PixelSpacingX'], meta_processed['PixelSpacingY'] = (\n",
    "                            most_frequent[0], most_frequent[0]\n",
    "                        )\n",
    "                    else:\n",
    "                        meta_processed['PixelSpacingX'], meta_processed['PixelSpacingY'] = (\n",
    "                            most_frequent[0], most_frequent[1]\n",
    "                        )\n",
    "                elif key == 'PatientPosition':\n",
    "                    pass\n",
    "                elif key == 'PositionReferenceIndicator':\n",
    "                    pass\n",
    "\n",
    "        all_weeks, all_fvcs, features = self._table_features[patient]\n",
    "        features = [value for key, value in meta_processed.items()] + features\n",
    "\n",
    "        return CTDataset._ReturnValue(weeks=all_weeks, fvcs=all_fvcs, features=features, masks=masks, images=images)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._train_patients if self.train else self._test_patients)\n",
    "\n",
    "    def __repr__(self):\n",
    "        fmt_str = 'OSIC Pulmonary Fibrosis Progression Dataset ' + self.__class__.__name__ + '\\n'\n",
    "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
    "        tmp = 'train' if self.train is True else 'test'\n",
    "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
    "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
    "        return fmt_str\n",
    "\n",
    "\n",
    "class LaplaceLoss(_Loss):\n",
    "    def forward(self, y_true, preds, log_sigma):\n",
    "        abs_diff = (y_true - preds).abs()\n",
    "#         abs_diff.clamp_max_(1_000)\n",
    "        log_sigma.clamp_(-5, 5)  # -np.log(70), np.log(70)\n",
    "#         log_sigma.clamp_min_(-5)\n",
    "        losses = np.sqrt(2) * abs_diff / log_sigma.exp() + log_sigma + np.log(2) / 2\n",
    "        return losses.mean()\n",
    "\n",
    "\n",
    "class SqueezeLayer(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.squeeze()\n",
    "\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, net):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net.extract_features(x.unsqueeze(0).unsqueeze(0))\n",
    "\n",
    "\n",
    "class OSICNet(nn.Module):\n",
    "    def __init__(self, dtype, device, efficient_net_model_number, hidden_size, dropout_rate):  # , output_size\n",
    "        super().__init__()\n",
    "\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "\n",
    "        self.CT_features_extractor = nn.Sequential(\n",
    "            FeatureExtractor(\n",
    "                EfficientNet3D.from_name(\n",
    "                    f'efficientnet-b{efficient_net_model_number}', override_params={'num_classes': 1}, in_channels=1\n",
    "                )\n",
    "            ),\n",
    "            nn.AdaptiveAvgPool3d(1),\n",
    "            SqueezeLayer()\n",
    "        )\n",
    "\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(1280, hidden_size),  # 1294\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, 5)  # output_size\n",
    "        )\n",
    "        \n",
    "        self._initialize_weights()\n",
    "\n",
    "        self.CT_features_extractor.to(self.device)\n",
    "        self.predictor.to(self.device)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv3d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.kernel_size[2] * m.out_channels\n",
    "                m.weight.data.normal_(0, np.sqrt(2. / n))\n",
    "            elif isinstance(m, torch.nn.BatchNorm3d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        \n",
    "#     def forward(self, data):\n",
    "#         mean_dataset, std_dataset = -971.4692260919278, 117.84143467421829\n",
    "#         lungs = -1000 * (1.0 - data.masks) + data.masks * data.images\n",
    "#         lungs = (lungs - mean_dataset) / std_dataset\n",
    "#         lungs = torch.tensor(lungs, dtype=self.dtype, device=self.device)\n",
    "#         lungs_features = self.CT_features_extractor(lungs)\n",
    "\n",
    "#         data_weeks = torch.tensor(data.weeks, dtype=self.dtype)\n",
    "#         weeks = torch.empty(len(data.weeks), 4, dtype=self.dtype)\n",
    "#         weeks[:, 3] = 1\n",
    "#         weeks[:, 2] = data_weeks\n",
    "#         weeks[:, 1] = data_weeks ** 2\n",
    "#         weeks[:, 0] = data_weeks ** 3\n",
    "\n",
    "#         agg_loss = 0\n",
    "#         for week, FVC in zip(data.weeks, data.fvcs):\n",
    "#             table_features = torch.tensor(np.r_[week, FVC, data.features], dtype=self.dtype, device=self.device)\n",
    "#             X = lungs_features  # torch.cat([lungs_features, table_features])\n",
    "\n",
    "#             pred_numbers = self.predictor(X).cpu()\n",
    "#             coefs = pred_numbers[:4]\n",
    "#             log_sigma = pred_numbers[4]\n",
    "\n",
    "#             FVC_preds = (weeks * coefs).sum(dim=1)\n",
    "#             FVC_true = torch.tensor(data.fvcs, dtype=self.dtype)\n",
    "\n",
    "#             agg_loss += LaplaceLoss()(FVC_true, FVC_preds, log_sigma)\n",
    "\n",
    "#         return agg_loss / len(data.weeks)\n",
    "\n",
    "    def forward(self, data):\n",
    "        mean_dataset, std_dataset = -971.4692260919278, 117.84143467421829\n",
    "        lungs = -1000 * (1.0 - data.masks) + data.masks * data.images\n",
    "        lungs = (lungs - mean_dataset) / std_dataset\n",
    "        lungs = torch.tensor(lungs, dtype=self.dtype, device=self.device)\n",
    "        lungs_features = self.CT_features_extractor(lungs)\n",
    "\n",
    "#         data_weeks = torch.tensor(data.weeks, dtype=self.dtype)\n",
    "#         weeks = torch.empty(len(data.weeks), 4, dtype=self.dtype)\n",
    "#         weeks[:, 3] = 1\n",
    "#         weeks[:, 2] = data_weeks\n",
    "#         weeks[:, 1] = data_weeks ** 2\n",
    "#         weeks[:, 0] = data_weeks ** 3\n",
    "\n",
    "#         agg_loss = 0\n",
    "        all_preds = []\n",
    "        for week, FVC in zip(data.weeks, data.fvcs):\n",
    "            table_features = torch.tensor(np.r_[week, FVC, data.features], dtype=self.dtype, device=self.device)\n",
    "            X = lungs_features  # torch.cat([lungs_features, table_features])\n",
    "\n",
    "            pred_numbers = self.predictor(X).cpu()\n",
    "            all_preds.append(pred_numbers)\n",
    "#             coefs = pred_numbers[:4]\n",
    "#             log_sigma = pred_numbers[4]\n",
    "\n",
    "#             FVC_preds = (weeks * coefs).sum(dim=1)\n",
    "#             FVC_true = torch.tensor(data.fvcs, dtype=self.dtype)\n",
    "            \n",
    "#             agg_loss += LaplaceLoss()(FVC_true, FVC_preds, log_sigma)\n",
    "\n",
    "#         return agg_loss / len(data.weeks)\n",
    "        return all_preds\n",
    "\n",
    "\n",
    "class LinearDecayLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, start_epoch, stop_epoch, start_lr, stop_lr, last_epoch=-1):\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.start_epoch = start_epoch\n",
    "        self.stop_epoch = stop_epoch\n",
    "\n",
    "        self.start_lr = start_lr\n",
    "        self.stop_lr = stop_lr\n",
    "\n",
    "        self.last_epoch = last_epoch\n",
    "\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self) -> list:\n",
    "        if self.last_epoch < self.start_epoch:\n",
    "            new_lr = self.start_lr\n",
    "        elif self.last_epoch > self.stop_epoch:\n",
    "            new_lr = self.stop_lr\n",
    "        else:\n",
    "            new_lr = self.start_lr + (\n",
    "                (self.stop_lr - self.start_lr) *\n",
    "                (self.last_epoch - self.start_epoch) /\n",
    "                (self.stop_epoch - self.start_epoch)\n",
    "            )\n",
    "        return [new_lr for _ in self.optimizer.param_groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sqrt(2) * 1000 / np.exp(5) + 5 + np.log(2) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = torch.tensor([1., 2., 3.]).requires_grad_(True)\n",
    "# l1 = (x * 2).sum()\n",
    "# l2 = (x * 3).sum()\n",
    "# l3 = (x * 5).sum()\n",
    "# loss = l1 + l2 + l3\n",
    "# loss.backward()\n",
    "# x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CTDataset(\n",
    "    f'{PROCESSED_PATH}/train',\n",
    "    f'{IMAGE_PATH}/train.csv',\n",
    "    train=True, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "test_dataset = CTDataset(\n",
    "    f'{PROCESSED_PATH}/train',\n",
    "    f'{IMAGE_PATH}/train.csv',\n",
    "    train=False, test_size=0.25, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = OSICNet(dtype=dtype, device=device, efficient_net_model_number=0, hidden_size=512, dropout_rate=0.5)\n",
    "# optimizer = optim.SGD(model.parameters(), lr=1e-5, momentum=0.9)  # , weight_decay=5e-4)\n",
    "\n",
    "### TMP ###\n",
    "# optimizer = optim.SGD(model.parameters(), lr=1e-9, momentum=0, weight_decay=0)  # 0.9, 5e-4)\n",
    "# lr_scheduler = LinearDecayLR(optimizer, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1, iter    1, loss    46.706024, cur iter time    8.5 sec, elapsed time    8.6 sec, \n",
      "Epoch   2, iter    1, loss    35.064163, cur iter time    8.4 sec, elapsed time   17.1 sec, \n",
      "Epoch   3, iter    1, loss    42.277420, cur iter time    8.6 sec, elapsed time   25.8 sec, \n",
      "Epoch   4, iter    1, loss    56.624676, cur iter time    8.4 sec, elapsed time   34.5 sec, \n",
      "Epoch   5, iter    1, loss    52.560135, cur iter time    8.4 sec, elapsed time   43.0 sec, \n",
      "Epoch   6, iter    1, loss    35.330425, cur iter time    8.4 sec, elapsed time   51.6 sec, \n",
      "Epoch   7, iter    1, loss    23.436638, cur iter time    8.5 sec, elapsed time   60.3 sec, \n",
      "Epoch   8, iter    1, loss    54.687748, cur iter time    8.7 sec, elapsed time   69.2 sec, \n",
      "Epoch   9, iter    1, loss    42.766113, cur iter time    8.7 sec, elapsed time   78.0 sec, \n",
      "Epoch  10, iter    1, loss    30.697641, cur iter time    8.7 sec, elapsed time   86.9 sec, \n",
      "Epoch  11, iter    1, loss    29.377777, cur iter time    8.6 sec, elapsed time   95.7 sec, \n",
      "Epoch  12, iter    1, loss    43.125286, cur iter time    8.7 sec, elapsed time  104.5 sec, \n",
      "Epoch  13, iter    1, loss    36.684006, cur iter time    8.7 sec, elapsed time  113.4 sec, \n"
     ]
    }
   ],
   "source": [
    "MAX_EPOCHS = 20\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(MAX_EPOCHS):\n",
    "    for cur_iter, data in enumerate(train_dataset):  # tqdm(, desc='Iteration over dataset'):\n",
    "        if cur_iter == 1:\n",
    "            break\n",
    "        cur_start_time = time.time()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "#         loss = model(data)\n",
    "\n",
    "\n",
    "        data_weeks = torch.tensor(data.weeks, dtype=dtype)\n",
    "        weeks = torch.empty(len(data.weeks), 4, dtype=dtype)\n",
    "        weeks[:, 0] = data_weeks ** 3\n",
    "        weeks[:, 1] = data_weeks ** 2\n",
    "        weeks[:, 2] = data_weeks\n",
    "        weeks[:, 3] = 1\n",
    "\n",
    "        all_preds = model(data)\n",
    "\n",
    "#         log_sigmas = []\n",
    "#         ALL_PREDS = []\n",
    "        agg_loss = 0\n",
    "        for week, FVC, preds in zip(data.weeks, data.fvcs, all_preds):\n",
    "            coefs = preds[:4]\n",
    "            log_sigma = preds[4]\n",
    "#             log_sigmas.append(log_sigma.item())\n",
    "#             print(log_sigma.item())\n",
    "\n",
    "            FVC_preds = (weeks * coefs).sum(dim=1)\n",
    "            FVC_true = torch.tensor(data.fvcs, dtype=dtype)\n",
    "\n",
    "            agg_loss += LaplaceLoss()(FVC_true, FVC_preds, log_sigma)\n",
    "        loss = agg_loss / len(data.weeks)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        cur_end_time = time.time()\n",
    "\n",
    "        print(\n",
    "            f'Epoch {epoch + 1:3d}, '\n",
    "            f'iter {cur_iter + 1:4d}, '\n",
    "            f'loss {loss.item():12.6f}, '\n",
    "            f'cur iter time {cur_end_time - cur_start_time:6.1f} sec, '\n",
    "            f'elapsed time {cur_end_time - start_time:6.1f} sec, '\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# summary(model.CT_features_extractor[0].net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for name, p in model.named_parameters():\n",
    "    print(f'{name[20:]:50} : {p.data.min().item():15.3e}, {p.data.max().item():15.3e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = model(train_dataset[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([-0.0239,  0.0427, -0.0162,  0.0765, -0.0407], grad_fn=<CopyBackwards>),\n",
       " tensor([-0.0596,  0.0999,  0.0310,  0.0311, -0.0328], grad_fn=<CopyBackwards>),\n",
       " tensor([-0.0534,  0.0610,  0.0263,  0.1096, -0.0509], grad_fn=<CopyBackwards>),\n",
       " tensor([-0.0468, -0.0004,  0.0322,  0.0902, -0.0749], grad_fn=<CopyBackwards>),\n",
       " tensor([-0.0137,  0.0090,  0.0003,  0.0589, -0.0920], grad_fn=<CopyBackwards>),\n",
       " tensor([-0.0302,  0.0220,  0.0029,  0.1146, -0.0387], grad_fn=<CopyBackwards>),\n",
       " tensor([-0.0218,  0.0516, -0.0411,  0.0951,  0.0158], grad_fn=<CopyBackwards>),\n",
       " tensor([-0.0205,  0.0779, -0.0219,  0.1012, -0.0544], grad_fn=<CopyBackwards>),\n",
       " tensor([ 0.0394,  0.0843, -0.0223,  0.0686, -0.0343], grad_fn=<CopyBackwards>)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_dataset[2]\n",
    "data_weeks = torch.tensor(data.weeks, dtype=dtype)\n",
    "weeks = torch.empty(len(data.weeks), 4, dtype=dtype)\n",
    "weeks[:, 0] = data_weeks ** 3\n",
    "weeks[:, 1] = data_weeks ** 2\n",
    "weeks[:, 2] = data_weeks\n",
    "weeks[:, 3] = 1\n",
    "\n",
    "# all_preds = model(data)\n",
    "\n",
    "agg_loss = 0\n",
    "for week, FVC, preds in zip(data.weeks, data.fvcs, all_preds):\n",
    "    coefs = preds[:4]\n",
    "    log_sigma = preds[4]\n",
    "\n",
    "    FVC_preds = (weeks * coefs).sum(dim=1)\n",
    "    FVC_true = torch.tensor(data.fvcs, dtype=dtype)\n",
    "\n",
    "    agg_loss += LaplaceLoss()(FVC_true, FVC_preds, log_sigma)\n",
    "loss = agg_loss / len(data.weeks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4912.0527, grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-8b8d69623f00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Has grad but it is None: {name[20:]:50}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{name[20:]:50} : {p.grad.data.cpu().min().item():15.3e}, {p.grad.data.cpu().max().item():15.3e}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'No grad: {name[20:]:50}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure"
     ]
    }
   ],
   "source": [
    "for name, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        if p.grad is None:\n",
    "            print(f'Has grad but it is None: {name[20:]:50}')\n",
    "        else:\n",
    "            print(f'{name[20:]:50} : {p.grad.data.cpu().min().item():15.3e}, {p.grad.data.cpu().max().item():15.3e}')\n",
    "    else:\n",
    "        print(f'No grad: {name[20:]:50}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(train_dataset)):\n",
    "    print(i, train_dataset[i].images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_all = CTDataset(\n",
    "    f'{PROCESSED_PATH}/train',\n",
    "    f'{IMAGE_PATH}/train.csv',\n",
    "    train=True, test_size=0.0, random_state=42\n",
    ")\n",
    "\n",
    "images = [-1000 * (1.0 - dataset_all[i].masks) + dataset_all[i].masks * dataset_all[i].images\n",
    "          for i in range(len(dataset_all))]\n",
    "\n",
    "sum_image = 0\n",
    "sum_sq_image = 0\n",
    "for image in images:\n",
    "    sum_image += image.sum()\n",
    "    sum_sq_image += (image ** 2).sum()\n",
    "\n",
    "N = np.prod((176., 192., 256., 256.))\n",
    "\n",
    "mean = sum_image / N\n",
    "\n",
    "mean\n",
    "\n",
    "var = sum_sq_image / N + mean ** 2 - 2 * mean * sum_image / N\n",
    "\n",
    "std = var ** 0.5\n",
    "\n",
    "mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
