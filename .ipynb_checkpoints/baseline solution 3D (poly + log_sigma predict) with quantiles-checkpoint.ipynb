{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you use TorchIO for your research, please cite the following paper:\n",
      "Pérez-García et al., TorchIO: a Python library for efficient loading,\n",
      "preprocessing, augmentation and patch-based sampling of medical images\n",
      "in deep learning. Credits instructions: https://torchio.readthedocs.io/#credits\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%config Completer.use_jedi = False\n",
    "\n",
    "import os\n",
    "import platform\n",
    "from collections import namedtuple\n",
    "import time\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import tabulate\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sparse\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "# from torchvision import transforms\n",
    "# from torchsummary import summary\n",
    "# from efficientnet_pytorch_3d import EfficientNet3D\n",
    "from my_efficientnet_pytorch_3d import EfficientNet3D\n",
    "import torchio\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from utils import CTDataset\n",
    "\n",
    "\n",
    "########################\n",
    "\n",
    "RUNNING_IN_KAGGLE = 'linux' in platform.platform().lower()\n",
    "IMAGE_PATH = \"../input/osic-pulmonary-fibrosis-progression/\" if RUNNING_IN_KAGGLE else 'data/'\n",
    "PROCESSED_PATH = 'FIX IT!' if RUNNING_IN_KAGGLE else 'data/processed-data/'  # TODO: fix this line\n",
    "\n",
    "dtype = torch.float32\n",
    "USE_GPU = True\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG(torch.nn.Module):\n",
    "    _vgg_configurations = {\n",
    "        'small': [8, 'M', 8, 'M', 16, 'M', 16, 'M'],\n",
    "        8: [64, 'M', 128, 'M', 256, 'M', 512, 'M', 512, 'M'],\n",
    "        11: [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "        13: [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "        16: [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "        19: [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "    }\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_layers(cfg, batch_norm):\n",
    "        layers = []\n",
    "        in_channels = 1\n",
    "        for v in cfg:\n",
    "            if v == 'M':\n",
    "                layers += [torch.nn.MaxPool3d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [torch.nn.Conv3d(in_channels, v, kernel_size=3, padding=1)]\n",
    "                if batch_norm:\n",
    "                    layers += [torch.nn.BatchNorm3d(v)]\n",
    "                layers += [torch.nn.ReLU(inplace=True)]\n",
    "                in_channels = v\n",
    "        return layers\n",
    "\n",
    "    def __init__(self, VGG_version, batch_norm):\n",
    "        super().__init__()\n",
    "        self.VGG_version = VGG_version\n",
    "        self.batch_norm = batch_norm\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            *VGG._make_layers(self._vgg_configurations[VGG_version], batch_norm)\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv3d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.kernel_size[2] * m.out_channels\n",
    "                m.weight.data.normal_(0, np.sqrt(2. / n))\n",
    "            elif isinstance(m, torch.nn.BatchNorm3d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaplaceLoss(nn.Module):  # _Loss):\n",
    "    def forward(self, y_true, preds, log_sigma, metric=False):\n",
    "        abs_diff = (y_true - preds).abs()\n",
    "\n",
    "        log_sigma.clamp_(-np.log(70), np.log(70))\n",
    "\n",
    "        if metric:\n",
    "            abs_diff.clamp_max_(1000)\n",
    "\n",
    "        losses = np.sqrt(2) * abs_diff / log_sigma.exp() + log_sigma + np.log(2) / 2\n",
    "        return losses.mean()\n",
    "\n",
    "\n",
    "class PinballLoss(nn.Module):\n",
    "    def __init__(self, alpha):\n",
    "        super().__init__()\n",
    "        self.alpha = torch.tensor(alpha, dtype=torch.float32)\n",
    "\n",
    "    def forward(self, y, z):\n",
    "        return torch.max((y - z) * self.alpha, (z - y) * (1 - self.alpha)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SqueezeLayer(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.squeeze()\n",
    "\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, net):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net.extract_features(x)\n",
    "\n",
    "\n",
    "class OSICNet(nn.Module):\n",
    "    FVC_MEAN, FVC_STD = 2690.479018721756, 832.5021066817238\n",
    "\n",
    "    def __init__(self, dtype, device, use_poly, use_quantiles, efficient_net_model_number, hidden_size, dropout_rate):  # , output_size\n",
    "        super().__init__()\n",
    "\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "\n",
    "        self.use_poly = use_poly\n",
    "        self.use_quantiles = use_quantiles\n",
    "        assert not (self.use_poly and self.use_quantiles)\n",
    "\n",
    "        self.CT_features_extractor = nn.Sequential(\n",
    "            FeatureExtractor(\n",
    "                EfficientNet3D.from_name(\n",
    "                    f'efficientnet-b{efficient_net_model_number}', override_params={'num_classes': 1}, in_channels=1\n",
    "                )\n",
    "            ),\n",
    "#             VGG('small', True),\n",
    "            nn.AdaptiveAvgPool3d(1),\n",
    "            SqueezeLayer()\n",
    "        )\n",
    "\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(16 + 15 + (1 - self.use_poly), hidden_size),  # 1280\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, (5 if self.use_poly else 2) + self.use_quantiles)  # poly coefs & sigma or FVC & log_sigma\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "        self.CT_features_extractor.to(self.device)\n",
    "        self.predictor.to(self.device)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv3d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.kernel_size[2] * m.out_channels\n",
    "                m.weight.data.normal_(0, np.sqrt(2. / n))\n",
    "            elif isinstance(m, torch.nn.BatchNorm3d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _prepare_data(self, data):\n",
    "        data = list(data)\n",
    "        if data[0].ndim >= 2:\n",
    "            for i in range(len(data)):\n",
    "                data[i] = data[i].squeeze(0)\n",
    "\n",
    "        percents, weeks, FVCs, features, masks, images = data\n",
    "\n",
    "        lungs_mean, lungs_std = -971.4692260919278, 117.84143467421829\n",
    "        lungs = -1000 * (1.0 - masks) + masks * images\n",
    "        lungs = (lungs - lungs_mean) / lungs_std\n",
    "        lungs = lungs.type(self.dtype)\n",
    "\n",
    "        percents_mean, percents_std = 77.6726, 19.8233\n",
    "        weeks_mean, weeks_std = 31.861846352485475, 23.240045178171002\n",
    "\n",
    "        percents = (percents - percents_mean) / percents_std\n",
    "        weeks = (weeks - weeks_mean) / weeks_std\n",
    "        FVCs = (FVCs - self.FVC_MEAN) / self.FVC_STD\n",
    "        features = features.type(self.dtype)\n",
    "        \n",
    "        return percents, weeks, FVCs, features, lungs, images\n",
    "\n",
    "    def forward(self, data):\n",
    "        weeks_unnorm = data[1]\n",
    "        percents, weeks, FVCs, features, lungs, images = self._prepare_data(data)\n",
    "\n",
    "        lungs = lungs.unsqueeze(0).to(self.device)\n",
    "        lungs_features = self.CT_features_extractor(lungs)\n",
    "\n",
    "        all_preds = []\n",
    "        for base_percent, base_week, base_FVC in zip(percents, weeks, FVCs):\n",
    "            table_features = torch.cat([\n",
    "                torch.tensor([base_percent]),\n",
    "                torch.tensor([base_week]),\n",
    "                torch.tensor([base_FVC]),\n",
    "                features\n",
    "            ]).to(self.device)\n",
    "\n",
    "            all_features = torch.cat([lungs_features, table_features])\n",
    "            \n",
    "            if self.use_poly:\n",
    "                X = all_features\n",
    "            else:\n",
    "                X = torch.cat([all_features.repeat(weeks.shape[0], 1), weeks.unsqueeze(1).to(self.device)], dim=1)\n",
    "\n",
    "            preds = self.predictor(X).cpu()\n",
    "\n",
    "            if self.use_poly:\n",
    "                weeks_poly = torch.empty(len(weeks), 4, dtype=self.dtype)\n",
    "                weeks_poly[:, 0] = weeks_unnorm ** 3\n",
    "                weeks_poly[:, 1] = weeks_unnorm ** 2\n",
    "                weeks_poly[:, 2] = weeks_unnorm\n",
    "                weeks_poly[:, 3] = 1\n",
    "\n",
    "                coefs = preds[:4]\n",
    "                log_sigmas = preds[4]\n",
    "\n",
    "                FVC_preds = (weeks_poly * coefs).sum(dim=1)\n",
    "                log_sigmas = log_sigmas.repeat(FVC_preds.shape[0])\n",
    "            else:\n",
    "                if self.use_quantiles:\n",
    "                    FVC_low, FVC_preds, FVC_high = preds.transpose(0, 1) * self.FVC_STD + self.FVC_MEAN\n",
    "                    all_preds.append((FVC_low, FVC_preds, FVC_high))\n",
    "                    continue\n",
    "                FVC_preds, log_sigmas = preds.transpose(0, 1)\n",
    "\n",
    "            FVC_preds = FVC_preds * self.FVC_STD + self.FVC_MEAN\n",
    "\n",
    "            all_preds.append((FVC_preds, log_sigmas))\n",
    "        return all_preds\n",
    "\n",
    "\n",
    "class LinearDecayLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, start_epoch, stop_epoch, start_lr, stop_lr, last_epoch=-1):\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.start_epoch = start_epoch\n",
    "        self.stop_epoch = stop_epoch\n",
    "\n",
    "        self.start_lr = start_lr\n",
    "        self.stop_lr = stop_lr\n",
    "\n",
    "        self.last_epoch = last_epoch\n",
    "\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self) -> list:\n",
    "        if self.last_epoch < self.start_epoch:\n",
    "            new_lr = self.start_lr\n",
    "        elif self.last_epoch > self.stop_epoch:\n",
    "            new_lr = self.stop_lr\n",
    "        else:\n",
    "            new_lr = self.start_lr + (\n",
    "                (self.stop_lr - self.start_lr) *\n",
    "                (self.last_epoch - self.start_epoch) /\n",
    "                (self.stop_epoch - self.start_epoch)\n",
    "            )\n",
    "        return [new_lr for _ in self.optimizer.param_groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_results(mode, writer, iter_num, metrics, log_sigmas):\n",
    "    for metric, values in metrics.items():\n",
    "        value = values[-1]\n",
    "        writer.add_scalar(f'{metric}/{mode}', value, iter_num)\n",
    "    writer.add_scalar(f'sigma/{mode}', np.exp(log_sigmas.detach().mean().item()), iter_num)\n",
    "\n",
    "    columns = [\n",
    "        'Iter', \n",
    "    ]\n",
    "\n",
    "    values = [\n",
    "        f'{iter_num}',\n",
    "    ]\n",
    "    \n",
    "    for metric, cur_values in metrics.items():  # sorted(metrics.items(), key=lambda x: x[0]):\n",
    "        columns.append(metric.replace('laplace_loss', 'll'))\n",
    "        values.append(cur_values[-1])\n",
    "\n",
    "    columns += ['Sigma']\n",
    "    values += [np.exp(log_sigmas.detach().mean().item())]\n",
    "\n",
    "    table = tabulate.tabulate([values], columns, tablefmt='simple', floatfmt='8.4f')\n",
    "    if iter_num % 40 == 1:\n",
    "        table = table.split('\\n')\n",
    "        table = '\\n'.join([table[1]] + table)\n",
    "    else:\n",
    "        table = table.split('\\n')[2]\n",
    "\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = torchio.transforms.Compose([\n",
    "    torchio.transforms.RandomAffine(\n",
    "        degrees=(10, 10),\n",
    "        translation=(-10, -10),\n",
    "        isotropic=False,\n",
    "        default_pad_value='minimum',\n",
    "        image_interpolation='linear',\n",
    "    ),\n",
    "#     torchio.transforms.RandomElasticDeformation(\n",
    "#         max_displacement=3.0\n",
    "#     )\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = CTDataset(\n",
    "    f'{PROCESSED_PATH}/train',\n",
    "    f'{IMAGE_PATH}/train.csv',\n",
    "    train=True,\n",
    "    transform=transforms,\n",
    "    test_size=0.25,\n",
    "    padding_mode=None,  # TODO: 'edge' is good for poly \n",
    "    random_state=42,\n",
    "    pad_global=False,\n",
    ")\n",
    "\n",
    "test_dataset = CTDataset(\n",
    "    f'{PROCESSED_PATH}/train',\n",
    "    f'{IMAGE_PATH}/train.csv',\n",
    "    train=False,\n",
    "    transform=transforms,\n",
    "    test_size=0.25,\n",
    "    padding_mode=None,  # 'edge',\n",
    "    random_state=42,\n",
    "    pad_global=False,\n",
    ")\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1, num_workers=4, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OSICNet(dtype=dtype, device=device, use_poly=False, use_quantiles=True, efficient_net_model_number=0,\n",
    "                hidden_size=256, dropout_rate=0.5)  # 512, 0.5)\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), lr=1e-8, momentum=0.9)  # , weight_decay=5e-4)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)  # 3e-4\n",
    "# optimizer = optim.SGD(model.parameters(), lr=1e-3)  # 3e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# shutil.rmtree('logs/test2/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_writer = SummaryWriter(log_dir='logs/quantile1/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe4167d20b24496fafce5b7336ea4546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch loop', max=10.0, style=ProgressStyle(description_wi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Train iter loop', max=1.0, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------  ---------  ----------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  Iter       loss          ll     ll_70    ll_last_3    ll_70_last_3    ll_first_last_3    ll_70_first_last_3     Sigma\n",
      "------  ---------  ----------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  1001  9451.2059  43729.4850   16.4602   61675.0111         20.5954         53553.7305               20.6022    0.0217\n",
      "  1002  537.3910   94.2131   13.3708      48.0145          9.2343            49.5517                9.2426    7.9106\n",
      "  1003  959.7925   87.7827   20.4663      84.8717         20.1283            77.6440               20.1319   12.9217\n",
      "  1004  590.1169   29.3454   14.4421      33.4292         16.3261            35.7464               16.3518   27.6961\n",
      "  1005  529.2807   24.3578   13.4267      24.9947         13.5468            27.9946               13.5743   28.9930\n",
      "  1006  1311.9501   36.2456   24.5670      37.2247         24.7981            35.8437               24.7981   45.0849\n",
      "  1007  108.3212    7.2902    6.5705       6.3296          6.0080             6.2514                6.0118   44.6387\n",
      "  1008  587.9449   14.5538   14.3941      15.7497         15.4553            16.2843               15.5439   66.9411\n",
      "  1009  135.8648    7.2722    7.1588       7.5901          7.4818             7.4688                7.4718   63.5730\n",
      "  1010  339.1570   11.6057   10.8174      10.4433          9.9070            10.2233                9.9595   61.2156\n",
      "  1011  538.3519   14.4342   14.3995      11.1980         11.1839            11.1758               11.1758   70.0000\n",
      "  1012  195.1198    8.6523    8.6523       7.2959          7.2959             7.3466                7.3466   70.0000\n",
      "  1013  252.3739    8.7434    8.7434       8.0080          8.0080             8.2373                8.2373   70.0000\n",
      "  1014  398.3856   12.3786   12.3786       9.9925          9.9925            10.0786               10.0786   70.0000\n",
      "  1015  3785.7019   24.7981   24.7981      24.7981         24.7981            24.7981               24.7981   70.0000\n",
      "  1016  593.0170   15.8301   15.8301      13.5588         13.5588            13.6995               13.6995   70.0000\n",
      "  1017  117.0329    6.9786    6.9786       5.7136          5.7136             5.6497                5.6497   70.0000\n",
      "  1018  1914.4718   24.7981   24.7981      24.7981         24.7981            24.7981               24.7981   70.0000\n",
      "  1019  189.0768    9.7614    9.7614       9.2760          9.2760             9.1152                9.1152   70.0000\n",
      "  1020  1301.2013   24.7981   24.7981      24.7981         24.7981            24.7981               24.7981   70.0000\n",
      "  1021  210.4175    9.6285    9.6285       8.6829          8.6829             8.6453                8.6453   70.0000\n",
      "  1022  200.4508    8.9087    8.9087       5.6693          5.6693             5.6854                5.6854   70.0000\n",
      "  1023  301.8197   10.3053   10.3053      11.4071         11.4071            11.6107               11.6107   70.0000\n",
      "  1024  1221.7869   24.4711   24.4711      24.2118         24.2118            24.3225               24.3225   70.0000\n",
      "  1025  714.6640   19.1122   19.1122      16.1162         16.1162            16.5515               16.5515   70.0000\n",
      "  1026  1024.9007   21.8459   21.8459      23.3983         23.3983            23.4279               23.4279   70.0000\n",
      "  1027  166.1383    7.0558    7.0558       8.7049          8.7049             8.7024                8.7024   70.0000\n",
      "  1028  978.3109   21.9800   21.9800      22.4657         22.4657            22.5926               22.5926   70.0000\n",
      "  1029  189.9280    8.4842    8.4842       8.3813          8.3813             8.2154                8.2154   70.0000\n",
      "  1030  364.8496   14.1822   14.1822       9.1017          9.1017             9.3831                9.3831   70.0000\n",
      "  1031  2047.5118   24.7981   24.7981      24.7981         24.7981            24.7981               24.7981   70.0000\n",
      "  1032  381.1022   12.5215   12.5215      13.0893         13.0893            12.7104               12.7104   70.0000\n",
      "  1033  383.5814   12.6465   12.6465      14.0136         14.0136            14.8816               14.8816   70.0000\n",
      "  1034  908.3690   21.7751   21.7751      20.8570         20.8570            21.3905               21.3905   70.0000\n",
      "  1035  383.0640   12.2135   12.2135      13.0946         13.0946            13.0690               13.0690   70.0000\n",
      "  1036  610.3696   21.0804   21.0804      20.4527         20.4527            20.4930               20.4930   70.0000\n",
      "  1037  337.3421   10.6135   10.6135      12.3977         12.3977            12.2466               12.2466   70.0000\n",
      "  1038  416.6463   12.5812   12.5812      13.3782         13.3782            13.2449               13.2449   70.0000\n",
      "  1039  299.2854    8.2141    8.2141      12.0998         12.0998            12.7509               12.7509   70.0000\n",
      "  1040  307.5740    8.2915    8.2915       6.4022          6.4022             6.8807                6.8807   70.0000\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  Iter      loss        ll     ll_70    ll_last_3    ll_70_last_3    ll_first_last_3    ll_70_first_last_3     Sigma\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  1041  451.1785   14.6774   14.6774      13.6087         13.6087            13.4735               13.4735   70.0000\n",
      "  1042  727.0292   23.1956   23.1956      21.9693         21.9693            21.7137               21.7137   70.0000\n",
      "  1043  1363.9736   22.7130   22.7130      24.7981         24.7981            24.7981               24.7981   70.0000\n",
      "  1044  335.7613    8.6757    8.6757       7.5099          7.5099             6.4895                6.4895   70.0000\n",
      "  1045  1857.8717   24.7981   24.7981      24.7981         24.7981            24.7981               24.7981   70.0000\n",
      "  1046  442.9151   15.7398   15.7398      14.1607         14.1607            15.0789               15.0789   70.0000\n",
      "  1047  406.7472   13.3791   13.3791      17.3332         17.3332            17.5570               17.5570   70.0000\n",
      "  1048  465.6388   14.0802   14.0802      12.0524         12.0524            13.3354               13.3354   70.0000\n",
      "  1049  855.4195   22.8673   22.8673      20.7200         20.7200            20.9820               20.9820   70.0000\n",
      "  1050  1413.0867   24.7621   24.7621      24.6901         24.6901            24.7981               24.7981   70.0000\n",
      "  1051  399.1513   14.1007   14.1007      16.1698         16.1698            16.3384               16.3384   70.0000\n",
      "  1052  236.0357    6.7402    6.7402       5.8831          5.8831             5.9762                5.9762   70.0000\n",
      "  1053  219.6355    7.0761    7.0761       6.0905          6.0905             6.4373                6.4373   70.0000\n",
      "  1054  321.7711   11.1259   11.1259      14.3882         14.3882            14.6152               14.6152   70.0000\n",
      "  1055  1273.3713   24.7981   24.7981      24.7981         24.7981            24.7981               24.7981   70.0000\n",
      "  1056  201.0393    6.1024    6.1024       6.1111          6.1111             6.2274                6.2274   70.0000\n",
      "  1057  688.9979   22.0041   22.0041      21.4309         21.4309            21.4749               21.4749   70.0000\n",
      "  1058  306.5642   11.4336   11.4336      10.1379         10.1379            10.4821               10.4821   70.0000\n",
      "  1059  305.7406   11.1931   11.1931      12.4284         12.4284            12.3723               12.3723   70.0000\n",
      "  1060  176.1065    7.4081    7.4081       7.8507          7.8507             7.2662                7.2662   70.0000\n",
      "  1061  877.1829   23.6108   23.6108      24.7597         24.7597            24.7981               24.7981   70.0000\n",
      "  1062  249.0907   10.3535   10.3535      13.5660         13.5660            13.1070               13.1070   70.0000\n",
      "  1063  1218.0635   24.3970   24.3970      23.5949         23.5949            23.5514               23.5514   70.0000\n",
      "  1064  261.7235   10.2985   10.2985      15.3863         15.3863            15.1888               15.1888   70.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1065  228.4484    8.5583    8.5583       9.8334          9.8334             9.9567                9.9567   70.0000\n",
      "  1066  1773.8576   24.7981   24.7981      24.7981         24.7981            24.7981               24.7981   70.0000\n",
      "  1067  202.1278    7.5321    7.5321       6.6580          6.6580             6.3784                6.3784   70.0000\n",
      "  1068  1395.0523   24.7981   24.7981      24.7981         24.7981            24.7981               24.7981   70.0000\n",
      "  1069  390.2467   14.2763   14.2763      19.1772         19.1772            19.1732               19.1732   70.0000\n",
      "  1070  826.0294   19.7673   19.7673      14.1396         14.1396            14.2996               14.2996   70.0000\n",
      "  1071  354.6949   12.6694   12.6694       9.9955          9.9955            10.1199               10.1199   70.0000\n",
      "  1072  666.8089   18.0990   18.0990      15.8037         15.8037            15.7623               15.7623   70.0000\n",
      "  1073  1266.6413   24.7892   24.7892      24.7684         24.7684            24.7981               24.7981   70.0000\n",
      "  1074  171.2449    7.4583    7.4583       7.8808          7.8808             7.7696                7.7696   70.0000\n",
      "  1075  359.0078   12.9626   12.9626      18.7056         18.7056            18.9275               18.9275   70.0000\n",
      "  1076  249.8077    9.2426    9.2426      12.1640         12.1640            12.1708               12.1708   70.0000\n",
      "  1077  212.0638    8.0887    8.0887       5.9318          5.9318             5.7344                5.7344   70.0000\n",
      "  1078  546.2892   16.8911   16.8911      17.3117         17.3117            16.8384               16.8384   70.0000\n",
      "  1079  369.2772   14.6066   14.6066      16.4203         16.4203            16.4034               16.4034   70.0000\n",
      "  1080  553.3485   19.4358   19.4358      22.4339         22.4339            22.4585               22.4585   70.0000\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  Iter      loss        ll     ll_70    ll_last_3    ll_70_last_3    ll_first_last_3    ll_70_first_last_3     Sigma\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  1081  193.2963    6.8506    6.8506       8.3168          8.3168             8.8968                8.8968   70.0000\n",
      "  1082  460.5263   17.2527   17.2527      16.3512         16.3512            16.4312               16.4312   70.0000\n",
      "  1083  351.9078   11.5680   11.5680      12.7124         12.7124            12.4093               12.4093   70.0000\n",
      "  1084  1033.3582   24.7678   24.7678      24.7921         24.7921            24.7981               24.7981   70.0000\n",
      "  1085  402.1543   14.5829   14.5829      14.9937         14.9937            14.9067               14.9067   70.0000\n",
      "  1086  1136.8564   24.3713   24.3713      24.0228         24.0228            23.5175               23.5175   70.0000\n",
      "  1087  1481.8402   24.7981   24.7981      24.7981         24.7981            24.7981               24.7981   70.0000\n",
      "  1088  236.7914    8.9497    8.9497       6.5299          6.5299             6.9383                6.9383   70.0000\n",
      "  1089  308.4744   10.9793   10.9793      10.0559         10.0559            10.7555               10.7555   70.0000\n",
      "  1090  472.2010   15.3417   15.3417      18.4184         18.4184            18.8730               18.8730   70.0000\n",
      "  1091  262.1917   10.0730   10.0730      11.6396         11.6396            11.6241               11.6241   70.0000\n",
      "  1092  404.2642   13.3763   13.3763      11.1860         11.1860            10.6102               10.6102   70.0000\n",
      "  1093  293.4421    9.9396    9.9396      13.4215         13.4215            14.3673               14.3673   70.0000\n",
      "  1094  1226.0721   22.7662   22.7662      24.7981         24.7981            24.7981               24.7981   70.0000\n",
      "  1095  582.9387   17.0802   17.0802      18.3411         18.3411            18.1686               18.1686   70.0000\n",
      "  1096  252.9447    7.4614    7.4614       8.8827          8.8827             8.3440                8.3440   70.0000\n",
      "  1097  309.7433   11.2895   11.2895      12.5942         12.5942            12.5095               12.5095   70.0000\n",
      "  1098  225.7663    6.9661    6.9661       7.9299          7.9299             6.6052                6.6052   70.0000\n",
      "  1099  570.1576   18.1525   18.1525      18.6259         18.6259            18.2578               18.2578   70.0000\n",
      "  1100  593.8540   17.6978   17.6978      15.9919         15.9919            16.0996               16.0996   70.0000\n",
      "  1101  737.3543   20.3139   20.3139      20.0854         20.0854            20.8661               20.8661   70.0000\n",
      "  1102  346.2164   11.5534   11.5534      10.1867         10.1867            10.8809               10.8809   70.0000\n",
      "  1103  1006.8126   24.4081   24.4081      23.9953         23.9953            23.7937               23.7937   70.0000\n",
      "  1104  374.8137   13.7799   13.7799      16.9962         16.9962            17.9941               17.9941   70.0000\n",
      "  1105  715.5525   20.4091   20.4091      21.0615         21.0615            21.5549               21.5549   70.0000\n",
      "  1106  750.2714   20.3242   20.3242      22.7883         22.7883            22.6129               22.6129   70.0000\n",
      "  1107  237.0909    7.3026    7.3026       8.2369          8.2369            10.5112               10.5112   70.0000\n",
      "  1108  219.3666    6.6748    6.6748       6.4805          6.4805             7.3603                7.3603   70.0000\n",
      "  1109  505.9917   16.4929   16.4929      19.0422         19.0422            18.9323               18.9323   70.0000\n",
      "  1110  275.8926    6.6395    6.6395       6.8764          6.8764             8.0584                8.0584   70.0000\n",
      "  1111  517.0911   17.0364   17.0364      16.2259         16.2259            15.7611               15.7611   70.0000\n",
      "  1112  386.7987    9.5844    9.5844      11.7301         11.7301            11.3019               11.3019   70.0000\n",
      "  1113  726.4751   20.9192   20.9192      22.3641         22.3641            23.3090               23.3090   70.0000\n",
      "  1114  384.2742   13.5525   13.5525      12.2423         12.2423            10.5633               10.5633   70.0000\n",
      "  1115  560.7296   17.9505   17.9505      14.7745         14.7745            14.7944               14.7944   70.0000\n",
      "  1116  420.6919   13.4789   13.4789      10.1471         10.1471             9.4234                9.4234   70.0000\n",
      "  1117  368.2059   12.8163   12.8163      10.9571         10.9571            10.7080               10.7080   70.0000\n",
      "  1118  301.0481    9.1190    9.1190      13.5631         13.5631            15.5007               15.5007   70.0000\n",
      "  1119  603.6143   14.0892   14.0892      16.9326         16.9326            16.9489               16.9489   70.0000\n",
      "  1120  289.3128   10.2120   10.2120       8.6488          8.6488             6.4323                6.4323   70.0000\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  Iter      loss        ll     ll_70    ll_last_3    ll_70_last_3    ll_first_last_3    ll_70_first_last_3     Sigma\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  1121  543.6971   17.1942   17.1942      15.5469         15.5469            16.8518               16.8518   70.0000\n",
      "  1122  963.3527   22.2961   22.2961      20.2113         20.2113            20.7071               20.7071   70.0000\n",
      "  1123  239.3148    8.3768    8.3768       8.3347          8.3347             8.7959                8.7959   70.0000\n",
      "  1124  406.8522   11.2974   11.2974      13.4784         13.4784            16.0015               16.0015   70.0000\n",
      "  1125  534.2248   13.6644   13.6644      10.4052         10.4052            10.5877               10.5877   70.0000\n",
      "  1126  249.1244    8.0341    8.0341       8.1669          8.1669             6.5781                6.5781   70.0000\n",
      "  1127  353.2118    8.9010    8.9010       8.6958          8.6958            10.3931               10.3931   70.0000\n",
      "  1128  1028.1652   20.3579   20.3579      15.1717         15.1717            17.0082               17.0082   70.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1129  532.0823   13.9432   13.9432      14.9202         14.9202            15.5555               15.5555   70.0000\n",
      "  1130  838.6407   17.5334   17.5334      15.9843         15.9843            19.0516               19.0516   70.0000\n",
      "  1131  443.6952   16.9579   16.9579      12.7402         12.7402            10.6267               10.6267   70.0000\n",
      "  1132  404.3447   12.0610   12.0610      10.9771         10.9771            13.6249               13.6249   70.0000\n",
      "     0  658.8334  347.3869   14.8073     483.0246         14.8413           421.6279               14.9836   70.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Test iter loop', max=1.0, style=Progres…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  Iter      loss        ll     ll_70    ll_last_3    ll_70_last_3    ll_first_last_3    ll_70_first_last_3     Sigma\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  1001  261.6958    6.0322    6.0322       6.1340          6.1340             5.6764                5.6764   70.0000\n",
      "  1002  770.2956   15.8233   15.8233      23.8614         23.8614            24.7981               24.7981   70.0000\n",
      "  1003  392.3278   11.3432   11.3432      14.7894         14.7894            17.1975               17.1975   70.0000\n",
      "  1004  360.4926   13.2309   13.2309      10.1434         10.1434            10.6654               10.6654   70.0000\n",
      "  1005  296.0220    6.8116    6.8116       6.7408          6.7408             6.5976                6.5976   70.0000\n",
      "  1006  275.9772    7.5074    7.5074       7.1885          7.1885             7.0740                7.0740   70.0000\n",
      "  1007  534.4597   18.1217   18.1217      21.9068         21.9068            21.6892               21.6892   70.0000\n",
      "  1008  285.8830   10.1207   10.1207      10.0210         10.0210             8.4797                8.4797   70.0000\n",
      "  1009  243.1923    6.5090    6.5090       7.4500          7.4500             7.6315                7.6315   70.0000\n",
      "  1010  245.5881    8.6509    8.6509       9.8019          9.8019            11.2198               11.2198   70.0000\n",
      "  1011  350.2413   12.7216   12.7216      13.2159         13.2159            14.3004               14.3004   70.0000\n",
      "  1012  298.1311    9.4864    9.4864      12.2572         12.2572            14.2095               14.2095   70.0000\n",
      "  1013  257.7356    8.8072    8.8072      10.7328         10.7328            13.6563               13.6563   70.0000\n",
      "  1014  311.0219    6.9974    6.9974       6.0001          6.0001             6.0075                6.0075   70.0000\n",
      "  1015  287.0596   11.5128   11.5128      15.1367         15.1367            18.7835               18.7835   70.0000\n",
      "  1016  479.7979   15.8045   15.8045      15.8396         15.8396            11.8362               11.8362   70.0000\n",
      "  1017  291.5007   11.7555   11.7555      10.1594         10.1594            12.4034               12.4034   70.0000\n",
      "  1018  293.1466    8.1703    8.1703       6.9459          6.9459             7.4257                7.4257   70.0000\n",
      "  1019  303.7526    6.4010    6.4010       5.6395          5.6395             5.7759                5.7759   70.0000\n",
      "  1020  388.3219   12.0682   12.0682       8.7657          8.7657             8.1192                8.1192   70.0000\n",
      "  1021  289.5235   10.0365   10.0365      11.8003         11.8003            12.7739               12.7739   70.0000\n",
      "  1022  313.4830    8.0897    8.0897       8.3672          8.3672             8.6211                8.6211   70.0000\n",
      "  1023  143.5143    5.6891    5.6891       5.3121          5.3121             5.2822                5.2822   70.0000\n",
      "  1024  625.9264   21.7715   21.7715      21.3595         21.3595            24.6873               24.6873   70.0000\n",
      "  1025  427.7409   13.8562   13.8562      17.0082         17.0082            19.6065               19.6065   70.0000\n",
      "  1026  288.5543   10.9707   10.9707      12.9724         12.9724            14.2372               14.2372   70.0000\n",
      "  1027  731.1804   19.7902   19.7902      18.8383         18.8383            17.8090               17.8090   70.0000\n",
      "  1028  393.6404    9.6183    9.6183       8.4825          8.4825             6.8438                6.8438   70.0000\n",
      "  1029  396.0457   13.9707   13.9707      13.8477         13.8477            13.1203               13.1203   70.0000\n",
      "  1030  288.2683    7.1742    7.1742       5.5261          5.5261             5.5712                5.5712   70.0000\n",
      "  1031  318.2921    7.7526    7.7526       6.8910          6.8910             7.1527                7.1527   70.0000\n",
      "  1032  383.9410   11.3742   11.3742      14.9676         14.9676            16.4317               16.4317   70.0000\n",
      "  1033  183.4960    6.1774    6.1774       7.1615          7.1615             7.5783                7.5783   70.0000\n",
      "  1034  245.6844    8.0863    8.0863       6.2248          6.2248             5.9142                5.9142   70.0000\n",
      "  1035  637.8799   22.9260   22.9260      24.4964         24.4964            23.6674               23.6674   70.0000\n",
      "  1036  312.2976   12.4145   12.4145      14.7909         14.7909            16.0173               16.0173   70.0000\n",
      "  1037  313.9660   10.3140   10.3140      11.0921         11.0921            10.7700               10.7700   70.0000\n",
      "  1038  258.8673    9.6847    9.6847      10.2025         10.2025            19.3056               19.3056   70.0000\n",
      "  1039  260.0950    6.8575    6.8575       6.0924          6.0924             6.0791                6.0791   70.0000\n",
      "  1040  594.8610   17.3536   17.3536      20.2549         20.2549            24.6446               24.6446   70.0000\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  Iter      loss        ll     ll_70    ll_last_3    ll_70_last_3    ll_first_last_3    ll_70_first_last_3     Sigma\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  1041  475.3785    8.5769    8.5769       7.4898          7.4898             6.7371                6.7371   70.0000\n",
      "  1042  519.9140   15.4730   15.4730      21.2098         21.2098            24.7981               24.7981   70.0000\n",
      "  1043  322.0436   13.0077   13.0077      11.5137         11.5137             7.6853                7.6853   70.0000\n",
      "  1044  372.4090   10.1336   10.1336      11.6561         11.6561            14.7074               14.7074   70.0000\n",
      "     0  364.1738   11.1131   11.1131      11.8247         11.8247            12.5815               12.5815   70.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Train iter loop', max=1.0, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  Iter      loss        ll     ll_70    ll_last_3    ll_70_last_3    ll_first_last_3    ll_70_first_last_3     Sigma\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  2001  308.0134   10.0936   10.0936      12.5892         12.5892            19.1643               19.1643   70.0000\n",
      "  2002  377.4642   13.3456   13.3456      18.4157         18.4157            23.4404               23.4404   70.0000\n",
      "  2003  430.9465   11.7744   11.7744      11.2095         11.2095            13.2806               13.2806   70.0000\n",
      "  2004  237.9763    8.4374    8.4374       9.2753          9.2753            12.9546               12.9546   70.0000\n",
      "  2005  179.4886    6.6880    6.6880       6.2541          6.2541             6.3925                6.3925   70.0000\n",
      "  2006  551.4015   18.6249   18.6249      17.5408         17.5408            15.5552               15.5552   70.0000\n",
      "  2007  582.5695   21.3264   21.3264      22.5886         22.5886            23.2218               23.2218   70.0000\n",
      "  2008  638.9873   22.1489   22.1489      21.7411         21.7411            20.7818               20.7818   70.0000\n",
      "  2009  338.0318   13.4806   13.4806      15.6673         15.6673            19.1641               19.1641   70.0000\n",
      "  2010  409.4450   14.8466   14.8466      15.5742         15.5742            18.3399               18.3399   70.0000\n",
      "  2011  329.5704   11.3228   11.3228      12.7457         12.7457            11.1936               11.1936   70.0000\n",
      "  2012  298.7156   11.7324   11.7324      10.4987         10.4987             7.9228                7.9228   70.0000\n",
      "  2013  217.8223   10.6993   10.6993      12.1655         12.1655            11.8282               11.8282   70.0000\n",
      "  2014  239.1108    8.6081    8.6081       7.6942          7.6942            13.0797               13.0797   70.0000\n",
      "  2015  1516.4183   24.3078   24.3078      24.5116         24.5116            24.7981               24.7981   70.0000\n",
      "  2016  235.2261    8.8787    8.8787       8.0846          8.0846             8.8038                8.8038   70.0000\n",
      "  2017  322.8380   13.2067   13.2067      14.3940         14.3940            13.7639               13.7639   70.0000\n",
      "  2018  672.2097   13.0910   13.0910      15.0592         15.0592            20.3158               20.3158   70.0000\n",
      "  2019  363.9904   13.5788   13.5788      13.3851         13.3851            15.7389               15.7389   70.0000\n",
      "  2020  266.9272    8.4558    8.4558       8.2695          8.2695             8.7858                8.7858   70.0000\n",
      "  2021  286.4262   12.1450   12.1450      12.7517         12.7517             9.4263                9.4263   70.0000\n",
      "  2022  197.2344    8.7833    8.7833       8.2512          8.2512             9.2618                9.2618   70.0000\n",
      "  2023  102.0840    6.2891    6.2891       6.0660          6.0660             5.9491                5.9491   70.0000\n",
      "  2024  264.0321    8.6445    8.6445       8.2618          8.2618             7.8257                7.8257   70.0000\n",
      "  2025  294.9805   10.1949   10.1949       8.4096          8.4096             8.5549                8.5549   70.0000\n",
      "  2026  245.8040    8.4067    8.4067       9.5072          9.5072             8.5917                8.5917   70.0000\n",
      "  2027  150.5950    8.0936    8.0936       6.1685          6.1685             5.4375                5.4375   70.0000\n",
      "  2028  504.0072   13.2740   13.2740      14.0593         14.0593            13.0824               13.0824   70.0000\n",
      "  2029  175.8506    8.6728    8.6728       8.7575          8.7575             8.7441                8.7441   70.0000\n",
      "  2030  255.8000    9.6815    9.6815       9.2480          9.2480             5.1196                5.1196   70.0000\n",
      "  2031  554.8972   14.3628   14.3628      15.4938         15.4938            22.3790               22.3790   70.0000\n",
      "  2032  191.1582    8.2990    8.2990       8.1896          8.1896             8.1319                8.1319   70.0000\n",
      "  2033  143.9993    7.3509    7.3509       7.7536          7.7536             7.4675                7.4675   70.0000\n",
      "  2034  552.0695   14.8354   14.8354      13.6232         13.6232             9.8330                9.8330   70.0000\n",
      "  2035  258.5364    9.5442    9.5442       9.6370          9.6370             7.3657                7.3657   70.0000\n",
      "  2036  253.8645    9.2237    9.2237       9.4050          9.4050            10.1476               10.1476   70.0000\n",
      "  2037  180.8874    8.3203    8.3203       9.2133          9.2133            10.4460               10.4460   70.0000\n",
      "  2038  539.5757   15.8839   15.8839      17.2224         17.2224            13.7887               13.7887   70.0000\n",
      "  2039  324.4265   12.0077   12.0077      10.2141         10.2141             6.6650                6.6650   70.0000\n",
      "  2040  272.1727   10.3512   10.3512       9.9876          9.9876             8.7770                8.7770   70.0000\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  Iter      loss        ll     ll_70    ll_last_3    ll_70_last_3    ll_first_last_3    ll_70_first_last_3     Sigma\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  2041  210.4419    9.0829    9.0829       9.6539          9.6539             6.2511                6.2511   70.0000\n",
      "  2042  267.2915    9.4700    9.4700       8.8715          8.8715             8.1980                8.1980   70.0000\n",
      "  2043  701.2428   15.4539   15.4539      15.1875         15.1875            15.7148               15.7148   70.0000\n",
      "  2044  150.1186    7.4198    7.4198       7.2631          7.2631             7.2787                7.2787   70.0000\n",
      "  2045  448.4924   13.2611   13.2611      13.2618         13.2618            13.9703               13.9703   70.0000\n",
      "  2046  218.5611    9.2108    9.2108       8.5944          8.5944             7.1434                7.1434   70.0000\n",
      "  2047  645.8183   15.5656   15.5656      15.7376         15.7376            14.5654               14.5654   70.0000\n",
      "  2048  227.6620    9.0861    9.0861       8.7114          8.7114             8.9849                8.9849   70.0000\n",
      "  2049  221.3127    8.6028    8.6028       8.9949          8.9949             8.6646                8.6646   70.0000\n",
      "  2050  390.1346   12.1897   12.1897       9.9328          9.9328             6.9501                6.9501   70.0000\n",
      "  2051  155.1975    7.5134    7.5134       8.1883          8.1883             7.1684                7.1684   70.0000\n",
      "  2052  181.4967    8.5291    8.5291       7.9678          7.9678            12.7219               12.7219   70.0000\n",
      "  2053  220.2017    9.2839    9.2839       9.5340          9.5340            11.4462               11.4462   70.0000\n",
      "  2054  326.7681   11.5043   11.5043      13.6445         13.6445            16.3813               16.3813   70.0000\n",
      "  2055  311.0508   11.0963   11.0963      11.0017         11.0017            13.7399               13.7399   70.0000\n",
      "  2056  634.3493   18.4202   18.4202      17.8885         17.8885            14.8617               14.8617   70.0000\n",
      "  2057  233.1620    8.6395    8.6395       8.3352          8.3352             5.7631                5.7631   70.0000\n",
      "  2058  147.7025    7.5100    7.5100       7.5731          7.5731             5.7477                5.7477   70.0000\n",
      "  2059  307.4922   11.7356   11.7356      12.1428         12.1428            14.2710               14.2710   70.0000\n",
      "  2060  184.1102    8.3693    8.3693       8.4335          8.4335             7.4040                7.4040   70.0000\n",
      "  2061  220.5646    8.2601    8.2601       9.9825          9.9825             8.3098                8.3098   70.0000\n",
      "  2062  175.4195    8.3329    8.3329       8.9899          8.9899             8.5200                8.5200   70.0000\n",
      "  2063  338.5043   11.1475   11.1475      10.8561         10.8561             9.0657                9.0657   70.0000\n",
      "  2064  166.9465    8.1736    8.1736       9.6231          9.6231            11.4591               11.4591   70.0000\n",
      "  2065  255.9246    9.6284    9.6284       8.1693          8.1693            14.8538               14.8538   70.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2066  582.4047   16.9186   16.9186      16.8772         16.8772            16.7319               16.7319   70.0000\n",
      "  2067  145.6330    7.5362    7.5362       7.5828          7.5828             8.0069                8.0069   70.0000\n",
      "  2068  331.3119   11.5641   11.5641      12.5364         12.5364            13.4320               13.4320   70.0000\n",
      "  2069  178.3491    8.4212    8.4212       9.6612          9.6612             9.5955                9.5955   70.0000\n",
      "  2070  305.4879   10.7926   10.7926      10.0108         10.0108             8.5055                8.5055   70.0000\n",
      "  2071  254.5530    9.6017    9.6017       8.7988          8.7988             8.8959                8.8959   70.0000\n",
      "  2072  270.9368   10.1196   10.1196       8.0312          8.0312             9.6176                9.6176   70.0000\n",
      "  2073  303.9479   10.4045   10.4045      11.0043         11.0043            10.9861               10.9861   70.0000\n",
      "  2074  199.7211    9.1330    9.1330       8.2225          8.2225             8.5273                8.5273   70.0000\n",
      "  2075  179.2954    8.4192    8.4192       8.8728          8.8728            11.0112               11.0112   70.0000\n",
      "  2076  287.3633   10.2529   10.2529      10.2154         10.2154            17.8112               17.8112   70.0000\n",
      "  2077  183.5830    8.8080    8.8080      10.1050         10.1050            12.0561               12.0561   70.0000\n",
      "  2078  233.0751    9.1556    9.1556       8.2190          8.2190             7.2000                7.2000   70.0000\n",
      "  2079  115.9551    6.8267    6.8267       6.7867          6.7867             5.9239                5.9239   70.0000\n",
      "  2080  208.4386    9.3926    9.3926       8.1477          8.1477             7.2046                7.2046   70.0000\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  Iter      loss        ll     ll_70    ll_last_3    ll_70_last_3    ll_first_last_3    ll_70_first_last_3     Sigma\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  2081  128.7276    7.3432    7.3432       7.5467          7.5467             9.4913                9.4913   70.0000\n",
      "  2082  532.8262   14.8542   14.8542      13.0247         13.0247            10.9434               10.9434   70.0000\n",
      "  2083  163.8756    8.1059    8.1059       7.4626          7.4626             6.1799                6.1799   70.0000\n",
      "  2084  201.8140    8.1294    8.1294       8.4018          8.4018             7.2106                7.2106   70.0000\n",
      "  2085  214.0928    8.9241    8.9241       7.7818          7.7818            10.5074               10.5074   70.0000\n",
      "  2086  336.0048   11.2386   11.2386      10.7065         10.7065            13.4855               13.4855   70.0000\n",
      "  2087  339.9217   10.9460   10.9460       9.9786          9.9786            12.3158               12.3158   70.0000\n",
      "  2088  187.4589    8.7321    8.7321      10.1212         10.1212            11.7587               11.7587   70.0000\n",
      "  2089  182.5583    8.1505    8.1505       8.2751          8.2751             9.5587                9.5587   70.0000\n",
      "  2090  188.8209    8.2666    8.2666       8.1207          8.1207            11.5928               11.5928   70.0000\n",
      "  2091  193.5250    8.4014    8.4014       7.7577          7.7577             9.6730                9.6730   70.0000\n",
      "  2092  191.2001    8.4958    8.4958       8.2324          8.2324             9.9910                9.9910   70.0000\n",
      "  2093  317.3909   10.6413   10.6413      12.2296         12.2296            19.2772               19.2772   70.0000\n",
      "  2094  539.6523   14.5554   14.5554      14.3050         14.3050            16.5071               16.5071   70.0000\n",
      "  2095  240.9019    9.1483    9.1483       9.8425          9.8425            10.2674               10.2674   70.0000\n",
      "  2096  141.9444    7.4290    7.4290       7.7374          7.7374             9.1834                9.1834   70.0000\n",
      "  2097  102.9747    6.6570    6.6570       7.0206          7.0206             7.4347                7.4347   70.0000\n",
      "  2098  199.0698    8.3778    8.3778      10.0242         10.0242            12.3001               12.3001   70.0000\n",
      "  2099  223.5889    9.3614    9.3614       9.1275          9.1275             8.0895                8.0895   70.0000\n",
      "  2100  232.3072    9.0851    9.0851       7.6524          7.6524             8.4674                8.4674   70.0000\n",
      "  2101  286.0207   10.2455   10.2455      10.3100         10.3100            10.1128               10.1128   70.0000\n",
      "  2102  113.1188    6.8575    6.8575       6.7403          6.7403             6.9493                6.9493   70.0000\n",
      "  2103  378.7797   12.5118   12.5118      11.1476         11.1476             8.5735                8.5735   70.0000\n",
      "  2104  155.5130    8.0180    8.0180       9.3251          9.3251            10.8936               10.8936   70.0000\n",
      "  2105  429.7052   13.1475   13.1475      13.5147         13.5147            13.8409               13.8409   70.0000\n",
      "  2106  236.7771    9.1235    9.1235      10.1495         10.1495            11.4217               11.4217   70.0000\n",
      "  2107  127.5757    7.1569    7.1569       7.9780          7.9780             8.9116                8.9116   70.0000\n",
      "  2108   80.5386    6.1894    6.1894       6.6864          6.6864             6.9160                6.9160   70.0000\n",
      "  2109  239.8354    9.2684    9.2684      10.8818         10.8818            11.6081               11.6081   70.0000\n",
      "  2110  116.0382    6.7602    6.7602       7.2042          7.2042             8.7034                8.7034   70.0000\n",
      "  2111  173.6517    7.9115    7.9115       7.8994          7.8994             7.0273                7.0273   70.0000\n",
      "  2112  185.8388    8.2724    8.2724       8.3187          8.3187             7.5671                7.5671   70.0000\n",
      "  2113  443.1259   13.0696   13.0696      13.0352         13.0352             8.5761                8.5761   70.0000\n",
      "  2114  166.6625    7.9697    7.9697       8.3329          8.3329             8.5203                8.5203   70.0000\n",
      "  2115  254.8205    9.9452    9.9452       8.8849          8.8849             9.1024                9.1024   70.0000\n",
      "  2116  184.4167    8.4747    8.4747       8.3599          8.3599             8.5258                8.5258   70.0000\n",
      "  2117  335.2645   10.8412   10.8412      13.4729         13.4729            16.2317               16.2317   70.0000\n",
      "  2118  197.2452    8.5287    8.5287      10.3929         10.3929             9.5946                9.5946   70.0000\n",
      "  2119  257.1331    9.4973    9.4973       9.4865          9.4865             8.9636                8.9636   70.0000\n",
      "  2120  136.1409    7.2405    7.2405       7.2180          7.2180             6.2993                6.2993   70.0000\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  Iter      loss        ll     ll_70    ll_last_3    ll_70_last_3    ll_first_last_3    ll_70_first_last_3     Sigma\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  2121  327.4901   11.7566   11.7566      10.8710         10.8710            10.3660               10.3660   70.0000\n",
      "  2122  465.9309   15.0310   15.0310      13.3933         13.3933            12.1852               12.1852   70.0000\n",
      "  2123  180.3264    8.1008    8.1008       7.4273          7.4273            12.3874               12.3874   70.0000\n",
      "  2124  313.4394   10.8021   10.8021      12.5407         12.5407            16.2190               16.2190   70.0000\n",
      "  2125  293.7084   10.7797   10.7797      10.1509         10.1509             9.1506                9.1506   70.0000\n",
      "  2126  160.7681    7.7896    7.7896       7.2107          7.2107             5.5484                5.5484   70.0000\n",
      "  2127  187.3261    7.8992    7.8992       7.6273          7.6273             5.6441                5.6441   70.0000\n",
      "  2128  542.3975   15.7263   15.7263      12.3772         12.3772            13.9373               13.9373   70.0000\n",
      "  2129  272.9826   10.8643   10.8643      12.3634         12.3634            12.6507               12.6507   70.0000\n",
      "  2130  424.9021   13.1824   13.1824      13.0079         13.0079            15.8408               15.8408   70.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2131  384.8843   12.4435   12.4435       9.7310          9.7310            11.8025               11.8025   70.0000\n",
      "  2132  213.1598    9.2680    9.2680       9.1095          9.1095             8.1786                8.1786   70.0000\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  Iter      loss        ll     ll_70    ll_last_3    ll_70_last_3    ll_first_last_3    ll_70_first_last_3     Sigma\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "     1  291.7405   10.4194   10.4194      10.4848         10.4848            10.9617               10.9617   70.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Test iter loop', max=1.0, style=Progres…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  Iter      loss        ll     ll_70    ll_last_3    ll_70_last_3    ll_first_last_3    ll_70_first_last_3     Sigma\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  2001  183.9809    8.7138    8.7138       8.5869          8.5869             7.2689                7.2689   70.0000\n",
      "  2002  552.5208   15.1396   15.1396      23.0810         23.0810            24.7981               24.7981   70.0000\n",
      "  2003  217.5498    9.2869    9.2869      11.5146         11.5146            13.9237               13.9237   70.0000\n",
      "  2004  434.7357   12.8580   12.8580      14.2212         14.2212            18.1984               18.1984   70.0000\n",
      "  2005  197.3116    8.1642    8.1642       8.9032          8.9032             8.2788                8.2788   70.0000\n",
      "  2006  150.6172    6.1576    6.1576       6.0366          6.0366             5.8025                5.8025   70.0000\n",
      "  2007  387.0276   12.4461   12.4461      16.3061         16.3061            16.3982               16.3982   70.0000\n",
      "  2008  296.2859   11.3631   11.3631       9.6420          9.6420             5.9499                5.9499   70.0000\n",
      "  2009  153.2978    8.0186    8.0186       9.7120          9.7120             9.1864                9.1864   70.0000\n",
      "  2010   92.7279    5.9964    5.9964       6.8950          6.8950             6.7105                6.7105   70.0000\n",
      "  2011  337.9998   12.1031   12.1031      11.1419         11.1419            10.8769               10.8769   70.0000\n",
      "  2012  140.9198    7.7873    7.7873      10.4424         10.4424            10.8885               10.8885   70.0000\n",
      "  2013  183.5981    8.3896    8.3896      11.7857         11.7857            13.0571               13.0571   70.0000\n",
      "  2014  198.0315    9.0491    9.0491       7.9638          7.9638             7.2884                7.2884   70.0000\n",
      "  2015  181.9699    8.3955    8.3955       9.1902          9.1902            11.1013               11.1013   70.0000\n",
      "  2016  307.5114   10.9096   10.9096      10.0279         10.0279             5.8836                5.8836   70.0000\n",
      "  2017  262.2443   10.4792   10.4792       8.4438          8.4438             9.3160                9.3160   70.0000\n",
      "  2018  183.6331    8.2978    8.2978       7.2105          7.2105             8.9436                8.9436   70.0000\n",
      "  2019  179.6089    7.8848    7.8848       8.4884          8.4884             8.2186                8.2186   70.0000\n",
      "  2020  188.7429    8.6745    8.6745       8.5595          8.5595             8.6352                8.6352   70.0000\n",
      "  2021  207.1497    9.5049    9.5049      10.9608         10.9608            12.2765               12.2765   70.0000\n",
      "  2022  193.6505    7.9151    7.9151       8.2522          8.2522             8.1423                8.1423   70.0000\n",
      "  2023   73.9624    6.0817    6.0817       6.2413          6.2413             7.3107                7.3107   70.0000\n",
      "  2024  563.1208   15.8124   15.8124      14.1164         14.1164            20.7362               20.7362   70.0000\n",
      "  2025  439.1487   12.8187   12.8187      15.0643         15.0643            15.2282               15.2282   70.0000\n",
      "  2026  135.7617    7.5546    7.5546       8.6498          8.6498             8.4625                8.4625   70.0000\n",
      "  2027  178.2062    8.4421    8.4421       7.2832          7.2832             5.8400                5.8400   70.0000\n",
      "  2028  193.0032    6.2694    6.2694       5.8399          5.8399             7.0260                7.0260   70.0000\n",
      "  2029  335.1464   12.6552   12.6552      11.4363         11.4363             9.5510                9.5510   70.0000\n",
      "  2030  130.4636    6.0209    6.0209       6.6790          6.6790             5.6750                5.6750   70.0000\n",
      "  2031  152.4088    7.0430    7.0430       9.8810          9.8810            10.1411               10.1411   70.0000\n",
      "  2032  367.2613   13.2535   13.2535      16.4937         16.4937            17.0260               17.0260   70.0000\n",
      "  2033  148.4540    8.3574    8.3574       9.3612          9.3612             9.3157                9.3157   70.0000\n",
      "  2034  180.8257    8.5626    8.5626       7.3359          7.3359             9.5187                9.5187   70.0000\n",
      "  2035  230.7875    9.4025    9.4025       9.1466          9.1466             7.4471                7.4471   70.0000\n",
      "  2036  124.0712    7.4608    7.4608       8.1747          8.1747             7.7854                7.7854   70.0000\n",
      "  2037  266.0109   10.8315   10.8315      10.7603         10.7603             9.9089                9.9089   70.0000\n",
      "  2038  164.6355    7.7052    7.7052       6.8109          6.8109            16.0368               16.0368   70.0000\n",
      "  2039  120.6556    6.2018    6.2018       6.4671          6.4671             6.2846                6.2846   70.0000\n",
      "  2040  130.8522    7.5303    7.5303       8.9822          8.9822            11.4258               11.4258   70.0000\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  Iter      loss        ll     ll_70    ll_last_3    ll_70_last_3    ll_first_last_3    ll_70_first_last_3     Sigma\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  2041  275.8305    7.2768    7.2768       7.4732          7.4732             8.8036                8.8036   70.0000\n",
      "  2042  654.0557   16.2184   16.2184      18.5728         18.5728            24.7981               24.7981   70.0000\n",
      "  2043  220.6271    9.1458    9.1458       8.8154          8.8154            13.2013               13.2013   70.0000\n",
      "  2044  299.9413   11.8430   11.8430      13.2073         13.2073            16.0709               16.0709   70.0000\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  Iter      loss        ll     ll_70    ll_last_3    ll_70_last_3    ll_first_last_3    ll_70_first_last_3     Sigma\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "     1  241.2806    9.4096    9.4096      10.0945         10.0945            10.8804               10.8804   70.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Train iter loop', max=1.0, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  Iter      loss        ll     ll_70    ll_last_3    ll_70_last_3    ll_first_last_3    ll_70_first_last_3     Sigma\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  3001  181.2149    8.6211    8.6211      10.9659         10.9659            12.7970               12.7970   70.0000\n",
      "  3002  402.7416   12.6393   12.6393      15.6312         15.6312            22.8234               22.8234   70.0000\n",
      "  3003  264.1142   10.0137   10.0137       8.8446          8.8446             7.3234                7.3234   70.0000\n",
      "  3004  166.3980    8.2133    8.2133       8.2878          8.2878            10.6336               10.6336   70.0000\n",
      "  3005  107.9286    6.6125    6.6125       6.8701          6.8701             8.7056                8.7056   70.0000\n",
      "  3006  224.7436    8.7502    8.7502       8.2332          8.2332             9.5565                9.5565   70.0000\n",
      "  3007  291.8772   11.1433   11.1433       9.6862          9.6862             7.3658                7.3658   70.0000\n",
      "  3008  213.2791    8.8399    8.8399       8.3284          8.3284             5.4084                5.4084   70.0000\n",
      "  3009  228.2874    9.6932    9.6932      11.1713         11.1713            10.6917               10.6917   70.0000\n",
      "  3010  274.3059   10.6818   10.6818       9.6506          9.6506            10.7411               10.7411   70.0000\n",
      "  3011  224.1222    9.4661    9.4661       9.7030          9.7030             8.1884                8.1884   70.0000\n",
      "  3012  244.9796    9.6486    9.6486      10.0181         10.0181             9.4593                9.4593   70.0000\n",
      "  3013  162.6481    7.9899    7.9899       8.9213          8.9213             9.6053                9.6053   70.0000\n",
      "  3014  250.4926    9.2941    9.2941       8.7912          8.7912            20.7515               20.7515   70.0000\n",
      "  3015  699.8480   15.5199   15.5199      15.9366         15.9366            12.1909               12.1909   70.0000\n",
      "  3016  177.4453    8.2115    8.2115       8.6293          8.6293             8.8567                8.8567   70.0000\n",
      "  3017  287.7815   10.3432   10.3432      11.2707         11.2707             7.8813                7.8813   70.0000\n",
      "  3018  389.2436   12.6627   12.6627      14.2448         14.2448            17.7556               17.7556   70.0000\n",
      "  3019  287.9514   10.2005   10.2005      11.4634         11.4634            15.8806               15.8806   70.0000\n",
      "  3020  205.6998    8.2296    8.2296       7.4072          7.4072             9.0802                9.0802   70.0000\n",
      "  3021  184.2211    8.7521    8.7521       8.7266          8.7266             5.7532                5.7532   70.0000\n",
      "  3022  271.9564   10.1488   10.1488       8.9391          8.9391             6.2554                6.2554   70.0000\n",
      "  3023  107.3611    6.6704    6.6704       6.9507          6.9507             8.9424                8.9424   70.0000\n",
      "  3024  212.6543    8.7035    8.7035       8.5153          8.5153             7.3884                7.3884   70.0000\n",
      "  3025  232.6223    9.3615    9.3615       9.7200          9.7200             9.3796                9.3796   70.0000\n",
      "  3026  326.9179   12.3295   12.3295      11.2442         11.2442            13.6742               13.6742   70.0000\n",
      "  3027  159.6075    7.8926    7.8926       6.7242          6.7242             5.6374                5.6374   70.0000\n",
      "  3028  276.4507   10.1998   10.1998      10.1932         10.1932             8.4668                8.4668   70.0000\n",
      "  3029  155.1671    7.9943    7.9943       7.4312          7.4312             9.6442                9.6442   70.0000\n",
      "  3030  225.0252    9.2086    9.2086      10.2782         10.2782             6.5746                6.5746   70.0000\n",
      "  3031  460.5575   13.1402   13.1402      12.1608         12.1608            10.9315               10.9315   70.0000\n",
      "  3032  181.0585    8.2848    8.2848       8.6224          8.6224             8.3897                8.3897   70.0000\n",
      "  3033  146.8240    7.5834    7.5834       8.7058          8.7058             8.9021                8.9021   70.0000\n",
      "  3034  540.2558   15.0021   15.0021      13.2625         13.2625            10.7725               10.7725   70.0000\n",
      "  3035  212.6378    8.6811    8.6811       8.3798          8.3798             7.1969                7.1969   70.0000\n",
      "  3036  165.5571    7.3969    7.3969       7.0719          7.0719             7.7325                7.7325   70.0000\n",
      "  3037  139.0487    7.3829    7.3829       8.5531          8.5531             8.0422                8.0422   70.0000\n",
      "  3038  312.3863   11.0412   11.0412      12.5548         12.5548             9.2765                9.2765   70.0000\n",
      "  3039  319.2046   12.0085   12.0085      10.9157         10.9157            10.0949               10.0949   70.0000\n",
      "  3040  258.2098   10.0356   10.0356       8.8700          8.8700             8.7031                8.7031   70.0000\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  Iter      loss        ll     ll_70    ll_last_3    ll_70_last_3    ll_first_last_3    ll_70_first_last_3     Sigma\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  3041  209.0456    8.9837    8.9837       9.6851          9.6851             7.2221                7.2221   70.0000\n",
      "  3042  176.3809    7.7720    7.7720       7.7850          7.7850             7.1953                7.1953   70.0000\n",
      "  3043  686.9812   16.0214   16.0214      15.9833         15.9833            12.5659               12.5659   70.0000\n",
      "  3044  101.3924    6.2056    6.2056       6.3062          6.3062             6.7203                6.7203   70.0000\n",
      "  3045  464.6077   14.2716   14.2716      13.4923         13.4923            12.1501               12.1501   70.0000\n",
      "  3046  217.6584    9.0611    9.0611       8.7997          8.7997             6.6476                6.6476   70.0000\n",
      "  3047  588.3960   14.8118   14.8118      16.3130         16.3130            14.9160               14.9160   70.0000\n",
      "  3048  143.6385    7.4885    7.4885       7.3334          7.3334             7.2189                7.2189   70.0000\n",
      "  3049  221.6071    9.2365    9.2365       9.2584          9.2584             7.7048                7.7048   70.0000\n",
      "  3050  327.7467   11.0367   11.0367       9.7562          9.7562            11.1092               11.1092   70.0000\n",
      "  3051  150.0549    7.4643    7.4643       8.7199          8.7199             8.1753                8.1753   70.0000\n",
      "  3052  128.3716    7.2679    7.2679       7.0391          7.0391             7.1914                7.1914   70.0000\n",
      "  3053  207.2727    8.9640    8.9640       9.4487          9.4487             8.7760                8.7760   70.0000\n",
      "  3054  269.0717   10.2990   10.2990      12.7065         12.7065            14.4379               14.4379   70.0000\n",
      "  3055  312.2352   11.2581   11.2581      11.3347         11.3347            10.0700               10.0700   70.0000\n",
      "  3056  443.9686   14.2282   14.2282      12.4081         12.4081            12.2064               12.2064   70.0000\n",
      "  3057  196.8860    7.8414    7.8414       7.4847          7.4847             6.0986                6.0986   70.0000\n",
      "  3058  138.6750    7.3368    7.3368       6.7045          6.7045             6.8767                6.8767   70.0000\n",
      "  3059  194.1123    8.9450    8.9450       8.9041          8.9041             8.5008                8.5008   70.0000\n",
      "  3060  159.4000    7.8373    7.8373       9.5419          9.5419            10.4228               10.4228   70.0000\n",
      "  3061  238.7899    9.2101    9.2101      11.0551         11.0551            10.2474               10.2474   70.0000\n",
      "  3062  139.4777    7.4002    7.4002       7.3600          7.3600             6.9609                6.9609   70.0000\n",
      "  3063  260.2387    9.4215    9.4215      10.7651         10.7651             9.1897                9.1897   70.0000\n",
      "  3064  154.8257    7.9571    7.9571       7.8042          7.8042             6.6993                6.6993   70.0000\n",
      "  3065  280.9226   10.2019   10.2019       8.3078          8.3078            15.2461               15.2461   70.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3066  488.3705   13.7502   13.7502      15.9962         15.9962             9.5354                9.5354   70.0000\n",
      "  3067  134.9703    7.3261    7.3261       7.3428          7.3428            10.6275               10.6275   70.0000\n",
      "  3068  331.1266   10.6005   10.6005      12.2506         12.2506            12.5115               12.5115   70.0000\n",
      "  3069  198.5626    8.7450    8.7450      10.1071         10.1071             8.5896                8.5896   70.0000\n",
      "  3070  323.5677   11.2277   11.2277      13.5495         13.5495            17.9920               17.9920   70.0000\n",
      "  3071  179.8291    8.0928    8.0928       7.9493          7.9493             6.7646                6.7646   70.0000\n",
      "  3072  209.6308    8.6219    8.6219       7.7793          7.7793             9.8976                9.8976   70.0000\n",
      "  3073  209.5891    8.4804    8.4804       8.8111          8.8111             8.6766                8.6766   70.0000\n",
      "  3074  229.0514    9.7493    9.7493       7.5285          7.5285             5.6648                5.6648   70.0000\n",
      "  3075  202.7783    9.1444    9.1444       8.1888          8.1888             8.2686                8.2686   70.0000\n",
      "  3076  319.1554   11.1269   11.1269      10.2526         10.2526            15.2813               15.2813   70.0000\n",
      "  3077  156.3283    8.0072    8.0072       8.1011          8.1011             7.4088                7.4088   70.0000\n",
      "  3078  255.3002    9.7799    9.7799      10.7350         10.7350            11.3614               11.3614   70.0000\n",
      "  3079  124.5687    7.2542    7.2542       6.2928          6.2928             6.1260                6.1260   70.0000\n",
      "  3080  351.6112   13.9259   13.9259      12.4611         12.4611             9.0915                9.0915   70.0000\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  Iter      loss        ll     ll_70    ll_last_3    ll_70_last_3    ll_first_last_3    ll_70_first_last_3     Sigma\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  3081   97.9250    6.5842    6.5842       6.2263          6.2263             6.5718                6.5718   70.0000\n",
      "  3082  290.1002   10.2252   10.2252       7.9831          7.9831             6.9863                6.9863   70.0000\n",
      "  3083  142.3414    7.4975    7.4975       7.9692          7.9692             9.2753                9.2753   70.0000\n",
      "  3084  259.8012    9.8524    9.8524      10.3766         10.3766             8.2294                8.2294   70.0000\n",
      "  3085  204.9499    8.6714    8.6714       8.0447          8.0447            10.6791               10.6791   70.0000\n",
      "  3086  288.3188   10.1701   10.1701       9.4229          9.4229             9.3525                9.3525   70.0000\n",
      "  3087  327.2521   10.8791   10.8791      10.0150         10.0150             7.1198                7.1198   70.0000\n",
      "  3088  182.7811    8.5517    8.5517       9.2679          9.2679            10.6507               10.6507   70.0000\n",
      "  3089  189.2508    8.3524    8.3524       8.3983          8.3983             9.0571                9.0571   70.0000\n",
      "  3090  160.8431    7.8207    7.8207       7.1533          7.1533            11.0143               11.0143   70.0000\n",
      "  3091  144.2961    7.7989    7.7989       7.0469          7.0469             8.1188                8.1188   70.0000\n",
      "  3092  174.3508    8.1086    8.1086       7.5509          7.5509             7.9736                7.9736   70.0000\n",
      "  3093  309.5870   10.6713   10.6713      12.4326         12.4326            19.1851               19.1851   70.0000\n",
      "  3094  507.6276   13.4735   13.4735      12.2761         12.2761            10.2438               10.2438   70.0000\n",
      "  3095  237.0498    9.4909    9.4909       9.0656          9.0656             7.5992                7.5992   70.0000\n",
      "  3096  135.3345    7.4142    7.4142       7.5033          7.5033             9.7877                9.7877   70.0000\n",
      "  3097  100.6883    6.7205    6.7205       7.2098          7.2098             7.0121                7.0121   70.0000\n",
      "  3098  161.4822    7.8849    7.8849       9.4902          9.4902            11.1530               11.1530   70.0000\n",
      "  3099  218.4543    9.0716    9.0716       8.9694          8.9694             8.9256                8.9256   70.0000\n",
      "  3100  196.1596    8.3246    8.3246       8.2404          8.2404             6.8373                6.8373   70.0000\n",
      "  3101  210.9733    8.5085    8.5085       8.7951          8.7951             6.8463                6.8463   70.0000\n",
      "  3102  124.4850    7.0509    7.0509       6.8988          6.8988             6.9410                6.9410   70.0000\n",
      "  3103  405.9214   13.4071   13.4071      10.9315         10.9315            14.0685               14.0685   70.0000\n",
      "  3104  175.1498    8.3851    8.3851      10.0406         10.0406            12.4879               12.4879   70.0000\n",
      "  3105  383.4138   12.4202   12.4202      12.7628         12.7628            11.2527               11.2527   70.0000\n",
      "  3106  214.6947    8.4228    8.4228      10.4548         10.4548            11.6533               11.6533   70.0000\n",
      "  3107  111.1322    6.8232    6.8232       7.3346          7.3346             7.3049                7.3049   70.0000\n",
      "  3108   72.6847    6.0178    6.0178       6.1327          6.1327             6.3108                6.3108   70.0000\n",
      "  3109  249.4462    9.5818    9.5818      10.6090         10.6090             8.9940                8.9940   70.0000\n",
      "  3110  117.3374    6.7999    6.7999       7.1281          7.1281             7.6062                7.6062   70.0000\n",
      "  3111  128.6223    7.1228    7.1228       7.2145          7.2145             7.3677                7.3677   70.0000\n",
      "  3112  180.8104    7.7607    7.7607       6.7861          6.7861             7.9953                7.9953   70.0000\n",
      "  3113  421.1982   12.4561   12.4561      12.5553         12.5553            11.0283               11.0283   70.0000\n",
      "  3114  153.3189    7.8233    7.8233       8.1784          8.1784            10.1593               10.1593   70.0000\n",
      "  3115  250.5404    9.6615    9.6615       8.4620          8.4620             7.9260                7.9260   70.0000\n",
      "  3116  173.7005    8.0835    8.0835       8.0593          8.0593             8.7452                8.7452   70.0000\n",
      "  3117  327.5601   10.9619   10.9619      13.3913         13.3913            17.2150               17.2150   70.0000\n",
      "  3118  181.5395    8.1777    8.1777       9.5234          9.5234            10.3262               10.3262   70.0000\n",
      "  3119  242.6362    9.1373    9.1373       9.6998          9.6998             8.7434                8.7434   70.0000\n",
      "  3120  122.9699    6.9629    6.9629       7.2587          7.2587             6.3323                6.3323   70.0000\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  Iter      loss        ll     ll_70    ll_last_3    ll_70_last_3    ll_first_last_3    ll_70_first_last_3     Sigma\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  3121  292.3577   10.7604   10.7604      10.3810         10.3810            11.2386               11.2386   70.0000\n",
      "  3122  468.4921   14.3167   14.3167      13.2471         13.2471            13.0094               13.0094   70.0000\n",
      "  3123  198.2709    8.4329    8.4329       7.8835          7.8835            15.1147               15.1147   70.0000\n",
      "  3124  289.3527   10.3883   10.3883      12.3073         12.3073            20.9114               20.9114   70.0000\n",
      "  3125  277.6348   10.0828   10.0828       9.7747          9.7747             9.6295                9.6295   70.0000\n",
      "  3126  142.9534    7.4566    7.4566       7.3965          7.3965             7.1159                7.1159   70.0000\n",
      "  3127  173.3359    7.9906    7.9906       8.6298          8.6298             7.8864                7.8864   70.0000\n",
      "  3128  460.0545   13.4428   13.4428      11.2164         11.2164             9.0373                9.0373   70.0000\n",
      "  3129  208.7000    9.2652    9.2652       8.7530          8.7530             8.8801                8.8801   70.0000\n",
      "  3130  452.8970   13.0543   13.0543      13.0604         13.0604            18.8441               18.8441   70.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3131  293.2936   10.6100   10.6100       9.1943          9.1943            12.3366               12.3366   70.0000\n",
      "  3132  172.7339    8.2859    8.2859       8.0475          8.0475            10.1099               10.1099   70.0000\n",
      "     2  244.7411    9.4861    9.4861       9.5475          9.5475             9.8408                9.8408   70.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Test iter loop', max=1.0, style=Progres…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  Iter      loss        ll     ll_70    ll_last_3    ll_70_last_3    ll_first_last_3    ll_70_first_last_3     Sigma\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  3001  154.6093    7.9413    7.9413       7.6898          7.6898             6.1116                6.1116   70.0000\n",
      "  3002  461.7156   13.9080   13.9080      21.1558         21.1558            24.6992               24.6992   70.0000\n",
      "  3003  189.2987    8.6024    8.6024      10.6115         10.6115            12.9308               12.9308   70.0000\n",
      "  3004  454.4660   13.0226   13.0226      14.1817         14.1817            18.4580               18.4580   70.0000\n",
      "  3005  189.9753    8.1783    8.1783       9.0011          9.0011             8.4640                8.4640   70.0000\n",
      "  3006  145.7009    6.4492    6.4492       6.0838          6.0838             5.6704                5.6704   70.0000\n",
      "  3007  353.2494   11.3855   11.3855      15.0668         15.0668            15.2606               15.2606   70.0000\n",
      "  3008  248.0248   10.2601   10.2601       8.3392          8.3392             5.2805                5.2805   70.0000\n",
      "  3009  121.1628    6.6917    6.6917       8.2370          8.2370             7.5852                7.5852   70.0000\n",
      "  3010   95.4496    6.2176    6.2176       6.6241          6.6241             6.4311                6.4311   70.0000\n",
      "  3011  242.9559   10.4000   10.4000       9.6873          9.6873             9.4418                9.4418   70.0000\n",
      "  3012  108.6773    6.9383    6.9383       8.9613          8.9613             9.5584                9.5584   70.0000\n",
      "  3013  164.9999    8.0535    8.0535      10.5755         10.5755            12.1257               12.1257   70.0000\n",
      "  3014  177.4915    8.6202    8.6202       7.3611          7.3611             6.5667                6.5667   70.0000\n",
      "  3015  179.4327    8.3512    8.3512       9.0867          9.0867            11.1679               11.1679   70.0000\n",
      "  3016  234.1854    9.5328    9.5328       8.8398          8.8398             6.0143                6.0143   70.0000\n",
      "  3017  243.5159    9.9673    9.9673       8.0120          8.0120             8.4526                8.4526   70.0000\n",
      "  3018  160.1758    7.7001    7.7001       6.8158          6.8158             8.7718                8.7718   70.0000\n",
      "  3019  149.8243    6.7360    6.7360       6.7114          6.7114             6.2329                6.2329   70.0000\n",
      "  3020  181.6751    8.3719    8.3719       8.6942          8.6942             8.7691                8.7691   70.0000\n",
      "  3021  203.5903    9.4209    9.4209      10.7724         10.7724            12.2542               12.2542   70.0000\n",
      "  3022  185.4753    8.0504    8.0504       8.2706          8.2706             8.2573                8.2573   70.0000\n",
      "  3023   52.9836    5.6365    5.6365       5.5163          5.5163             6.2739                6.2739   70.0000\n",
      "  3024  399.7684   12.8330   12.8330      11.1998         11.1998            16.9655               16.9655   70.0000\n",
      "  3025  435.7614   12.6519   12.6519      14.6801         14.6801            14.9350               14.9350   70.0000\n",
      "  3026  120.8448    7.1806    7.1806       7.9020          7.9020             7.7020                7.7020   70.0000\n",
      "  3027  164.6744    8.0578    8.0578       6.8493          6.8493             5.5368                5.5368   70.0000\n",
      "  3028  200.8921    7.1925    7.1925       6.9035          6.9035             9.2253                9.2253   70.0000\n",
      "  3029  294.3708   11.7986   11.7986      10.3506         10.3506             8.4648                8.4648   70.0000\n",
      "  3030  118.0765    6.0367    6.0367       6.6449          6.6449             5.4219                5.4219   70.0000\n",
      "  3031  135.1185    6.9260    6.9260       9.5060          9.5060             9.8629                9.8629   70.0000\n",
      "  3032  288.4866   11.2281   11.2281      14.2741         14.2741            14.7370               14.7370   70.0000\n",
      "  3033  120.9103    7.6459    7.6459       8.5827          8.5827             8.5652                8.5652   70.0000\n",
      "  3034  194.5781    8.8688    8.8688       7.8971          7.8971            10.7010               10.7010   70.0000\n",
      "  3035  178.2860    8.3199    8.3199       8.0643          8.0643             7.6298                7.6298   70.0000\n",
      "  3036  117.6366    7.2561    7.2561       7.9432          7.9432             7.5730                7.5730   70.0000\n",
      "  3037  264.8086   10.7038   10.7038      10.6186         10.6186             9.7175                9.7175   70.0000\n",
      "  3038  156.9673    7.5315    7.5315       6.7340          6.7340            14.6414               14.6414   70.0000\n",
      "  3039  106.1386    6.1847    6.1847       6.5328          6.5328             6.2654                6.2654   70.0000\n",
      "  3040  130.4535    7.4451    7.4451       8.4338          8.4338            11.3072               11.3072   70.0000\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  Iter      loss        ll     ll_70    ll_last_3    ll_70_last_3    ll_first_last_3    ll_70_first_last_3     Sigma\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  3041  283.1926    8.7541    8.7541       8.7186          8.7186            10.2466               10.2466   70.0000\n",
      "  3042  680.7501   16.4326   16.4326      18.6838         18.6838            24.7981               24.7981   70.0000\n",
      "  3043  208.4060    8.7731    8.7731       8.5362          8.5362            12.9074               12.9074   70.0000\n",
      "  3044  286.1581   11.3748   11.3748      12.5951         12.5951            15.2943               15.2943   70.0000\n",
      "     2  217.8390    8.9462    8.9462       9.4988          9.4988            10.3926               10.3926   70.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Train iter loop', max=1.0, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  Iter      loss        ll     ll_70    ll_last_3    ll_70_last_3    ll_first_last_3    ll_70_first_last_3     Sigma\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  4001  179.5681    8.5220    8.5220      10.9350         10.9350            11.8227               11.8227   70.0000\n",
      "  4002  427.6006   13.0798   13.0798      15.5529         15.5529            20.1210               20.1210   70.0000\n",
      "  4003  252.9620    9.7978    9.7978       8.9795          8.9795             6.6308                6.6308   70.0000\n",
      "  4004  139.3762    7.6746    7.6746       8.0077          8.0077            10.3908               10.3908   70.0000\n",
      "  4005  110.3910    6.7343    6.7343       6.9481          6.9481             5.6681                5.6681   70.0000\n",
      "  4006  191.5389    8.4801    8.4801       8.3239          8.3239             9.7600                9.7600   70.0000\n",
      "  4007  198.3591    8.8232    8.8232       8.8963          8.8963             7.3134                7.3134   70.0000\n",
      "  4008  119.2773    7.0922    7.0922       7.3408          7.3408             7.3046                7.3046   70.0000\n",
      "  4009  166.2256    8.1772    8.1772       9.4990          9.4990            10.3656               10.3656   70.0000\n",
      "  4010  241.5964    9.7156    9.7156       8.9415          8.9415             7.5033                7.5033   70.0000\n",
      "  4011  227.3194    9.2261    9.2261       9.4982          9.4982             9.1599                9.1599   70.0000\n",
      "  4012  237.6232    9.5732    9.5732      10.1248         10.1248            11.2194               11.2194   70.0000\n",
      "  4013  167.2732    7.9691    7.9691       9.0783          9.0783             7.8090                7.8090   70.0000\n",
      "  4014  211.8451    8.5664    8.5664       7.8791          7.8791            15.2989               15.2989   70.0000\n",
      "  4015  729.7384   17.3077   17.3077      16.4856         16.4856             8.8130                8.8130   70.0000\n",
      "  4016  172.9217    8.1385    8.1385       7.9517          7.9517             6.9713                6.9713   70.0000\n",
      "  4017  292.1702   10.4090   10.4090      11.6874         11.6874            10.4306               10.4306   70.0000\n",
      "  4018  362.6470   11.6186   11.6186      11.5077         11.5077            14.8506               14.8506   70.0000\n",
      "  4019  180.5322    8.2336    8.2336       9.0207          9.0207            10.9781               10.9781   70.0000\n",
      "  4020  189.3209    8.2266    8.2266       8.6341          8.6341             6.6476                6.6476   70.0000\n",
      "  4021  123.3202    7.0576    7.0576       7.0318          7.0318             8.4666                8.4666   70.0000\n",
      "  4022  211.5135    8.8791    8.8791       7.9499          7.9499             7.4218                7.4218   70.0000\n",
      "  4023  105.8792    6.7447    6.7447       6.8242          6.8242             7.8664                7.8664   70.0000\n",
      "  4024  211.3995    8.9053    8.9053       8.9777          8.9777             7.9962                7.9962   70.0000\n",
      "  4025  226.5787    9.3963    9.3963      10.0500         10.0500             9.9510                9.9510   70.0000\n",
      "  4026  235.7469    9.3793    9.3793       9.8276          9.8276             8.3295                8.3295   70.0000\n",
      "  4027  135.6661    7.4186    7.4186       6.1581          6.1581             6.0067                6.0067   70.0000\n",
      "  4028  346.2082   11.4328   11.4328      11.6133         11.6133            11.0825               11.0825   70.0000\n",
      "  4029  150.0279    7.9599    7.9599       6.9879          6.9879             8.7156                8.7156   70.0000\n",
      "  4030  233.2659    9.4936    9.4936      10.5146         10.5146             6.5157                6.5157   70.0000\n",
      "  4031  493.2263   13.5801   13.5801      12.4388         12.4388            13.7316               13.7316   70.0000\n",
      "  4032  189.1588    8.6356    8.6356       8.8033          8.8033             8.0228                8.0228   70.0000\n",
      "  4033  136.5381    7.3100    7.3100       7.2891          7.2891             7.3211                7.3211   70.0000\n",
      "  4034  483.8349   14.1832   14.1832      12.3420         12.3420             9.0974                9.0974   70.0000\n",
      "  4035  235.9501    9.3512    9.3512       9.4710          9.4710             7.6963                7.6963   70.0000\n",
      "  4036  183.8492    8.1588    8.1588       8.1291          8.1291             8.7128                8.7128   70.0000\n",
      "  4037  154.6866    7.8004    7.8004       9.3901          9.3901            10.1929               10.1929   70.0000\n",
      "  4038  296.1055   10.4842   10.4842      12.2111         12.2111             6.6284                6.6284   70.0000\n",
      "  4039  218.6708    9.3376    9.3376       8.6581          8.6581             8.1498                8.1498   70.0000\n",
      "  4040  242.5122    9.6177    9.6177       8.3083          8.3083             6.2660                6.2660   70.0000\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  Iter      loss        ll     ll_70    ll_last_3    ll_70_last_3    ll_first_last_3    ll_70_first_last_3     Sigma\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  4041  164.8179    7.9952    7.9952       8.5508          8.5508             6.1813                6.1813   70.0000\n",
      "  4042  164.3981    7.6749    7.6749       7.6928          7.6928             6.4081                6.4081   70.0000\n",
      "  4043  547.4765   12.9854   12.9854      12.3001         12.3001             9.8594                9.8594   70.0000\n",
      "  4044   91.7494    6.1044    6.1044       6.1895          6.1895             6.0829                6.0829   70.0000\n",
      "  4045  430.3493   13.6768   13.6768      14.0008         14.0008            14.2005               14.2005   70.0000\n",
      "  4046  169.5495    8.0726    8.0726       7.1232          7.1232             6.6925                6.6925   70.0000\n",
      "  4047  593.4103   14.8386   14.8386      16.6571         16.6571            16.1329               16.1329   70.0000\n",
      "  4048  174.5358    7.9579    7.9579       8.1322          8.1322             9.1061                9.1061   70.0000\n",
      "  4049  199.1265    8.6036    8.6036       8.7225          8.7225            10.3153               10.3153   70.0000\n",
      "  4050  347.6118   11.4041   11.4041       8.7115          8.7115             6.9504                6.9504   70.0000\n",
      "  4051  138.1063    7.3379    7.3379       8.2525          8.2525             8.2646                8.2646   70.0000\n",
      "  4052  130.9930    7.2967    7.2967       7.8591          7.8591             7.1423                7.1423   70.0000\n",
      "  4053  171.0987    8.1778    8.1778       8.1544          8.1544             8.3762                8.3762   70.0000\n",
      "  4054  219.6940    9.4653    9.4653      10.9804         10.9804            12.0324               12.0324   70.0000\n",
      "  4055  258.9306    9.7979    9.7979       9.7661          9.7661             8.0203                8.0203   70.0000\n",
      "  4056  272.6030   10.6042   10.6042       9.5976          9.5976            10.5297               10.5297   70.0000\n",
      "  4057  212.0362    8.7941    8.7941       8.6743          8.6743             6.7977                6.7977   70.0000\n",
      "  4058  123.0491    6.9647    6.9647       6.5961          6.5961             6.7079                6.7079   70.0000\n",
      "  4059  183.8768    8.4679    8.4679       9.5461          9.5461            10.1515               10.1515   70.0000\n",
      "  4060  125.8419    7.1278    7.1278       8.0246          8.0246            10.0743               10.0743   70.0000\n",
      "  4061  206.2960    8.4908    8.4908      10.5611         10.5611            11.0205               11.0205   70.0000\n",
      "  4062  159.6214    7.9226    7.9226       8.0566          8.0566             6.3867                6.3867   70.0000\n",
      "  4063  271.1959   10.2455   10.2455      10.2403         10.2403            13.9039               13.9039   70.0000\n",
      "  4064  152.3582    7.8353    7.8353       8.3075          8.3075             7.2330                7.2330   70.0000\n",
      "  4065  251.7631    9.6186    9.6186       8.6408          8.6408            16.3917               16.3917   70.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4066  483.2400   15.2002   15.2002      15.8270         15.8270            19.4249               19.4249   70.0000\n",
      "  4067  142.7311    7.5244    7.5244       7.5686          7.5686             9.3371                9.3371   70.0000\n",
      "  4068  273.6845   10.1096   10.1096      11.2529         11.2529            10.3911               10.3911   70.0000\n",
      "  4069  195.5418    8.6896    8.6896       9.6449          9.6449            10.1789               10.1789   70.0000\n",
      "  4070  341.0794   11.7649   11.7649      13.9390         13.9390            15.8189               15.8189   70.0000\n",
      "  4071  166.9360    7.9021    7.9021       7.4462          7.4462             6.2780                6.2780   70.0000\n",
      "  4072  155.0739    7.5147    7.5147       7.0172          7.0172             7.7812                7.7812   70.0000\n",
      "  4073  238.2611    9.4518    9.4518       9.8409          9.8409            10.5056               10.5056   70.0000\n",
      "  4074  166.8099    8.1158    8.1158       7.2904          7.2904             7.7081                7.7081   70.0000\n",
      "  4075  200.8364    9.0206    9.0206       7.6617          7.6617             7.2822                7.2822   70.0000\n",
      "  4076  287.2494   10.5799   10.5799       9.5614          9.5614            13.9801               13.9801   70.0000\n",
      "  4077  102.3577    6.6001    6.6001       6.2800          6.2800             4.9905                4.9905   70.0000\n",
      "  4078  258.0685    9.8195    9.8195      11.0237         11.0237            11.5012               11.5012   70.0000\n",
      "  4079  137.1000    7.7909    7.7909       7.5068          7.5068             7.8880                7.8880   70.0000\n",
      "  4080  364.8282   13.9966   13.9966      12.8394         12.8394            12.7362               12.7362   70.0000\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  Iter      loss        ll     ll_70    ll_last_3    ll_70_last_3    ll_first_last_3    ll_70_first_last_3     Sigma\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  4081  107.1283    6.9071    6.9071       7.1121          7.1121             6.8671                6.8671   70.0000\n",
      "  4082  229.5875    9.0137    9.0137       7.6502          7.6502             6.4869                6.4869   70.0000\n",
      "  4083  184.4748    8.4238    8.4238      10.0178         10.0178            10.6515               10.6515   70.0000\n",
      "  4084  261.3318    9.8509    9.8509       9.8182          9.8182             8.3838                8.3838   70.0000\n",
      "  4085  217.9093    8.9190    8.9190       8.4714          8.4714             9.8750                9.8750   70.0000\n",
      "  4086  313.4139   11.2424   11.2424      10.9124         10.9124             9.7257                9.7257   70.0000\n",
      "  4087  275.3432   10.0965   10.0965       9.1465          9.1465             8.7739                8.7739   70.0000\n",
      "  4088  159.7196    7.7913    7.7913       8.7908          8.7908            10.8232               10.8232   70.0000\n",
      "  4089  173.8883    8.2092    8.2092       8.3776          8.3776             9.4705                9.4705   70.0000\n",
      "  4090  164.2559    7.7057    7.7057       7.9698          7.9698            12.7564               12.7564   70.0000\n",
      "  4091  130.8855    7.2679    7.2679       6.7006          6.7006             8.3742                8.3742   70.0000\n",
      "  4092  137.4412    7.4107    7.4107       7.4748          7.4748             7.5162                7.5162   70.0000\n",
      "  4093  289.9050   10.1689   10.1689      12.1339         12.1339            18.3435               18.3435   70.0000\n",
      "  4094  500.0860   13.0373   13.0373      12.4866         12.4866            14.5031               14.5031   70.0000\n",
      "  4095  257.4145    9.5998    9.5998       8.6996          8.6996             5.5158                5.5158   70.0000\n",
      "  4096  125.9402    7.2723    7.2723       7.0191          7.0191            10.1606               10.1606   70.0000\n",
      "  4097   80.8104    5.9914    5.9914       6.3315          6.3315             7.3014                7.3014   70.0000\n",
      "  4098  149.5103    7.6386    7.6386       9.2297          9.2297             9.5844                9.5844   70.0000\n",
      "  4099  226.0994    9.3790    9.3790       9.5817          9.5817             9.6835                9.6835   70.0000\n",
      "  4100  213.1052    9.0543    9.0543       8.7080          8.7080            11.2426               11.2426   70.0000\n",
      "  4101  216.0540    8.9380    8.9380       8.6002          8.6002            10.7545               10.7545   70.0000\n",
      "  4102  139.5573    7.4542    7.4542       7.3295          7.3295             7.0010                7.0010   70.0000\n",
      "  4103  326.0371   12.0294   12.0294      11.1644         11.1644             8.3453                8.3453   70.0000\n",
      "  4104  153.7972    7.8084    7.8084       9.2127          9.2127            12.5382               12.5382   70.0000\n",
      "  4105  347.9214   11.8413   11.8413      11.9425         11.9425            11.2311               11.2311   70.0000\n",
      "  4106  230.7387    9.0864    9.0864       9.5390          9.5390             7.5744                7.5744   70.0000\n",
      "  4107  111.4147    6.8301    6.8301       8.1522          8.1522             8.9834                8.9834   70.0000\n",
      "  4108   69.2964    5.9842    5.9842       5.8491          5.8491             5.1001                5.1001   70.0000\n",
      "  4109  205.5782    8.7706    8.7706       9.9185          9.9185             8.9340                8.9340   70.0000\n",
      "  4110  101.6114    6.4979    6.4979       6.2073          6.2073             7.7968                7.7968   70.0000\n",
      "  4111  127.4728    7.2110    7.2110       7.1204          7.1204             7.7747                7.7747   70.0000\n",
      "  4112  198.4030    8.5961    8.5961       9.2168          9.2168             7.7713                7.7713   70.0000\n",
      "  4113  451.8185   13.0478   13.0478      12.8423         12.8423            11.4144               11.4144   70.0000\n",
      "  4114  122.7768    7.1101    7.1101       7.2515          7.2515             9.2934                9.2934   70.0000\n",
      "  4115  253.1865    9.5880    9.5880       9.1333          9.1333            10.0149               10.0149   70.0000\n",
      "  4116  173.5967    8.1008    8.1008       7.8665          7.8665             6.0467                6.0467   70.0000\n",
      "  4117  312.9596   10.5606   10.5606      12.7728         12.7728            13.9812               13.9812   70.0000\n",
      "  4118  185.0785    8.4854    8.4854       8.4179          8.4179             8.1330                8.1330   70.0000\n",
      "  4119  254.7235    9.4317    9.4317       9.4756          9.4756            10.6435               10.6435   70.0000\n",
      "  4120  148.1737    7.6203    7.6203       8.6998          8.6998            11.8546               11.8546   70.0000\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  Iter      loss        ll     ll_70    ll_last_3    ll_70_last_3    ll_first_last_3    ll_70_first_last_3     Sigma\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  4121  260.6366    9.9512    9.9512       9.3674          9.3674            10.3108               10.3108   70.0000\n",
      "  4122  407.4178   12.7322   12.7322      12.2107         12.2107            10.2979               10.2979   70.0000\n",
      "  4123  183.8058    8.2830    8.2830       7.6207          7.6207            14.1450               14.1450   70.0000\n",
      "  4124  238.7167    9.5519    9.5519      10.7776         10.7776            12.4706               12.4706   70.0000\n",
      "  4125  253.2040    9.6901    9.6901      10.4425         10.4425            11.1242               11.1242   70.0000\n",
      "  4126  151.1807    7.7434    7.7434       8.0135          8.0135             7.4942                7.4942   70.0000\n",
      "  4127  179.7316    7.9319    7.9319       8.4896          8.4896             7.7968                7.7968   70.0000\n",
      "  4128  371.7900   11.8046   11.8046      12.3112         12.3112             9.4355                9.4355   70.0000\n",
      "  4129  200.0494    8.8297    8.8297       9.5201          9.5201             9.6511                9.6511   70.0000\n",
      "  4130  463.5472   13.1672   13.1672      15.9031         15.9031            21.1082               21.1082   70.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4131  291.8170   10.5521   10.5521       9.3917          9.3917            13.6700               13.6700   70.0000\n",
      "  4132  178.8479    8.2568    8.2568       8.0970          8.0970             9.7545                9.7545   70.0000\n",
      "     3  228.7568    9.1856    9.1856       9.3469          9.3469             9.6703                9.6703   70.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Test iter loop', max=1.0, style=Progres…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  Iter      loss        ll     ll_70    ll_last_3    ll_70_last_3    ll_first_last_3    ll_70_first_last_3     Sigma\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  4001  151.9274    7.4323    7.4323       7.1170          7.1170             5.3989                5.3989   70.0000\n",
      "  4002  426.3805   13.3478   13.3478      19.6571         19.6571            24.3221               24.3221   70.0000\n",
      "  4003  159.0306    7.5806    7.5806       9.2112          9.2112            11.3564               11.3564   70.0000\n",
      "  4004  435.6773   12.6288   12.6288      13.8769         13.8769            18.2890               18.2890   70.0000\n",
      "  4005  229.1838    9.6789    9.6789      10.4357         10.4357            10.2811               10.2811   70.0000\n",
      "  4006  135.3566    6.3951    6.3951       6.1974          6.1974             5.8567                5.8567   70.0000\n",
      "  4007  288.3587   10.4977   10.4977      13.9061         13.9061            14.0017               14.0017   70.0000\n",
      "  4008  243.8019    9.8544    9.8544       8.3059          8.3059             5.6049                5.6049   70.0000\n",
      "  4009  136.4625    7.3402    7.3402       8.8598          8.8598             8.3429                8.3429   70.0000\n",
      "  4010   87.1189    6.0270    6.0270       6.8196          6.8196             6.6465                6.6465   70.0000\n",
      "  4011  282.4965   10.6695   10.6695       9.8870          9.8870             9.8441                9.8441   70.0000\n",
      "  4012  110.1746    6.8506    6.8506       8.7358          8.7358             9.4614                9.4614   70.0000\n",
      "  4013  175.4067    8.2085    8.2085      10.7081         10.7081            12.4636               12.4636   70.0000\n",
      "  4014  179.2937    8.3665    8.3665       7.0708          7.0708             6.1265                6.1265   70.0000\n",
      "  4015  168.5773    8.0273    8.0273       8.8756          8.8756            10.7922               10.7922   70.0000\n",
      "  4016  181.1461    8.1792    8.1792       7.6629          7.6629             6.2447                6.2447   70.0000\n",
      "  4017  194.1700    8.5800    8.5800       7.1978          7.1978             6.6940                6.6940   70.0000\n",
      "  4018  138.8943    7.1336    7.1336       6.5209          6.5209             8.2705                8.2705   70.0000\n",
      "  4019  159.1761    7.1584    7.1584       7.3436          7.3436             6.8976                6.8976   70.0000\n",
      "  4020  180.1959    8.4775    8.4775       8.6829          8.6829             8.7575                8.7575   70.0000\n",
      "  4021  162.3890    8.0045    8.0045       9.0697          9.0697            10.5740               10.5740   70.0000\n",
      "  4022  181.2470    8.0756    8.0756       8.2461          8.2461             8.3647                8.3647   70.0000\n",
      "  4023  130.3353    7.0702    7.0702       6.8711          6.8711             5.8237                5.8237   70.0000\n",
      "  4024  323.5617   10.9218   10.9218       9.5062          9.5062            13.9317               13.9317   70.0000\n",
      "  4025  438.5836   12.6738   12.6738      14.7886         14.7886            15.5669               15.5669   70.0000\n",
      "  4026  118.0457    6.9945    6.9945       7.5834          7.5834             7.4828                7.4828   70.0000\n",
      "  4027  163.6288    7.8687    7.8687       6.7826          6.7826             5.6388                5.6388   70.0000\n",
      "  4028  172.6685    6.6043    6.6043       6.1548          6.1548             8.2268                8.2268   70.0000\n",
      "  4029  300.9494   11.3166   11.3166       9.8714          9.8714             8.0176                8.0176   70.0000\n",
      "  4030  126.6172    6.5623    6.5623       7.6653          7.6653             6.3943                6.3943   70.0000\n",
      "  4031  138.8598    6.8292    6.8292       9.2117          9.2117             9.6449                9.6449   70.0000\n",
      "  4032  294.8876   10.8111   10.8111      13.7186         13.7186            14.2450               14.2450   70.0000\n",
      "  4033  107.7515    7.0423    7.0423       7.8728          7.8728             7.9570                7.9570   70.0000\n",
      "  4034  203.4151    8.8671    8.8671       8.4062          8.4062            11.9827               11.9827   70.0000\n",
      "  4035  155.8800    7.7844    7.7844       7.6951          7.6951             7.4282                7.4282   70.0000\n",
      "  4036  151.0027    7.8953    7.8953       8.5345          8.5345             8.3075                8.3075   70.0000\n",
      "  4037  257.5796   10.1121   10.1121      10.2449         10.2449             9.3794                9.3794   70.0000\n",
      "  4038  147.2831    7.3889    7.3889       6.4978          6.4978            13.6938               13.6938   70.0000\n",
      "  4039  108.3585    6.2077    6.2077       6.6426          6.6426             6.3049                6.3049   70.0000\n",
      "  4040  140.2379    7.5892    7.5892       8.6860          8.6860            11.7995               11.7995   70.0000\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  Iter      loss        ll     ll_70    ll_last_3    ll_70_last_3    ll_first_last_3    ll_70_first_last_3     Sigma\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  4041  254.6851    8.2060    8.2060       8.1803          8.1803             9.6871                9.6871   70.0000\n",
      "  4042  637.8815   15.6918   15.6918      18.4960         18.4960            24.7981               24.7981   70.0000\n",
      "  4043  178.4984    8.0942    8.0942       8.3264          8.3264            12.9233               12.9233   70.0000\n",
      "  4044  203.7576    9.0317    9.0317       9.9874          9.9874            12.0045               12.0045   70.0000\n",
      "     3  208.2030    8.6381    8.6381       9.2298          9.2298            10.1325               10.1325   70.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d967ff4e4cc54eadbf095b97c9dd4f31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Train iter loop', max=1.0, style=Progre…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  Iter      loss        ll     ll_70    ll_last_3    ll_70_last_3    ll_first_last_3    ll_70_first_last_3     Sigma\n",
      "------  --------  --------  --------  -----------  --------------  -----------------  --------------------  --------\n",
      "  5001  200.9144    8.8652    8.8652      10.3857         10.3857            13.2839               13.2839   70.0000\n",
      "  5002  394.7421   12.0438   12.0438      14.0752         14.0752            23.4958               23.4958   70.0000\n",
      "  5003  205.4247    8.7072    8.7072       8.8621          8.8621             8.3482                8.3482   70.0000\n",
      "  5004  149.3611    7.6617    7.6617       7.3203          7.3203             8.5253                8.5253   70.0000\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-afca016ef698>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mcur_iter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Train iter loop'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mcur_start_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alexander\\envs\\pew\\lib\\site-packages\\tqdm\\notebook.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtqdm_notebook\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m                 \u001b[1;31m# return super(tqdm...) will not catch exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alexander\\envs\\pew\\lib\\site-packages\\tqdm\\std.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1102\u001b[0m                 fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[0;32m   1103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1104\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1105\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1106\u001b[0m             \u001b[1;31m# Update and possibly print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\GitHub\\Kaggle\\KaggleOSICPulmonaryFibrosisProgression\\utils.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, index, transform)\u001b[0m\n\u001b[0;32m    734\u001b[0m                 \u001b[0mimages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorchio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mScalarImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m             )\n\u001b[1;32m--> 736\u001b[1;33m             \u001b[0mtransformed_subject\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m             \u001b[0mmasks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformed_subject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmasks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m             \u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformed_subject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alexander\\envs\\pew\\lib\\site-packages\\torchio\\transforms\\transform.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'raise'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m             \u001b[0mtransformed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransformed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mintensity_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alexander\\envs\\pew\\lib\\site-packages\\torchio\\transforms\\augmentation\\composition.py\u001b[0m in \u001b[0;36mapply_transform\u001b[1;34m(self, sample)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mSubject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alexander\\envs\\pew\\lib\\site-packages\\torchvision\\transforms\\transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alexander\\envs\\pew\\lib\\site-packages\\torchio\\transforms\\augmentation\\random_transform.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, sample)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mSubject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m     def parse_degrees(\n",
      "\u001b[1;32mc:\\users\\alexander\\envs\\pew\\lib\\site-packages\\torchio\\transforms\\transform.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'raise'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m             \u001b[0mtransformed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtransformed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mintensity_only\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alexander\\envs\\pew\\lib\\site-packages\\torchio\\transforms\\augmentation\\spatial\\random_affine.py\u001b[0m in \u001b[0;36mapply_transform\u001b[1;34m(self, sample)\u001b[0m\n\u001b[0;32m    191\u001b[0m                     \u001b[0mtranslation_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m                     \u001b[0minterpolation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m                     \u001b[0mcenter_lps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcenter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m                 )\n\u001b[0;32m    195\u001b[0m                 \u001b[0mtransformed_tensors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformed_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alexander\\envs\\pew\\lib\\site-packages\\torchio\\transforms\\augmentation\\spatial\\random_affine.py\u001b[0m in \u001b[0;36mapply_affine_transform\u001b[1;34m(self, tensor, affine, scaling_params, rotation_params, translation_params, interpolation, center_lps)\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m         \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnib_to_sitk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maffine\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_3d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m         \u001b[0mfloating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreference\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alexander\\envs\\pew\\lib\\site-packages\\torchio\\utils.py\u001b[0m in \u001b[0;36mnib_to_sitk\u001b[1;34m(data, affine, squeeze, force_3d, force_4d)\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (W, H, D, C) or (W, H, D)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m     \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msitk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGetImageFromArray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0misVector\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_multichannel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m     \u001b[0mrotation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspacing\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_rotation_and_spacing_from_affine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maffine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\alexander\\envs\\pew\\lib\\site-packages\\SimpleITK\\SimpleITK.py\u001b[0m in \u001b[0;36mGetImageFromArray\u001b[1;34m(arr, isVector)\u001b[0m\n\u001b[0;32m   3444\u001b[0m     \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_components\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3445\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3446\u001b[1;33m     \u001b[0m_SimpleITK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_SetImageFromArray\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtostring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3448\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "MAX_EPOCHS = 10\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in tqdm(range(MAX_EPOCHS), desc='Epoch loop'):\n",
    "    metrics = {\n",
    "        'loss': [],\n",
    "        'laplace_loss': [],\n",
    "        'laplace_loss_70': [],\n",
    "        'laplace_loss_last_3': [],\n",
    "        'laplace_loss_70_last_3': [],\n",
    "        'laplace_loss_first_last_3': [],\n",
    "        'laplace_loss_70_first_last_3': []\n",
    "    }\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for cur_iter, data in tqdm(enumerate(train_dataset), desc='Train iter loop', leave=False):\n",
    "        cur_start_time = time.time()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        FVC_true = data[2]\n",
    "\n",
    "        all_preds = model(data)\n",
    "\n",
    "        agg_loss = 0\n",
    "\n",
    "        agg_metrics = {\n",
    "            'loss': [],\n",
    "            'laplace_loss': [],\n",
    "            'laplace_loss_70': [],\n",
    "            'laplace_loss_last_3': [],\n",
    "            'laplace_loss_70_last_3': [],\n",
    "            'laplace_loss_first_last_3': [],\n",
    "            'laplace_loss_70_first_last_3': []\n",
    "        }\n",
    "        \n",
    "        for FVC_low, FVC_preds, FVC_high in all_preds:\n",
    "            log70 = torch.tensor(np.log(70))\n",
    "\n",
    "            pinball_loss =  PinballLoss(0.2)(FVC_true, FVC_low)\n",
    "            pinball_loss += PinballLoss(0.5)(FVC_true, FVC_preds)\n",
    "            pinball_loss += PinballLoss(0.8)(FVC_true, FVC_high)\n",
    "\n",
    "            log_sigmas = torch.log(torch.clamp_min(FVC_high - FVC_low, 1e-7))\n",
    "\n",
    "            laplace_loss = LaplaceLoss()(FVC_true, FVC_preds, log_sigmas)  #  if epoch >= 3 else log70)\n",
    "\n",
    "            cur_loss = 0.8 * pinball_loss + 0.2 * laplace_loss\n",
    "\n",
    "            agg_loss += cur_loss\n",
    "#             agg_loss += nn.MSELoss()(FVC_true, FVC_preds).pow(0.5)  # , log_sigmas)\n",
    "            with torch.no_grad():\n",
    "                agg_metrics['loss'] += [cur_loss.item()]\n",
    "                \n",
    "                agg_metrics['laplace_loss'] += [LaplaceLoss()(FVC_true, FVC_preds, log_sigmas, metric=True).item()]\n",
    "                agg_metrics['laplace_loss_70'] += [LaplaceLoss()(FVC_true, FVC_preds, log70, metric=True).item()]\n",
    "                \n",
    "                agg_metrics['laplace_loss_last_3'] += [LaplaceLoss()(FVC_true[-3:], FVC_preds[-3:],\n",
    "                                                                     log_sigmas[-3:], metric=True).item()]\n",
    "                agg_metrics['laplace_loss_70_last_3'] += [LaplaceLoss()(FVC_true[-3:], FVC_preds[-3:],\n",
    "                                                                        log70, metric=True).item()]\n",
    "                \n",
    "                agg_metrics['laplace_loss_first_last_3'] += [agg_metrics['laplace_loss_last_3'][0]]\n",
    "                agg_metrics['laplace_loss_70_first_last_3'] += [agg_metrics['laplace_loss_70_last_3'][0]]\n",
    "\n",
    "        loss = agg_loss / FVC_true.shape[0]\n",
    "\n",
    "        for metric, values in agg_metrics.items():\n",
    "            metrics[metric] += [np.mean(values)]\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        cur_end_time = time.time()\n",
    "\n",
    "        print_results('train', log_writer, (epoch + 1) * 1000 + cur_iter + 1, metrics, log_sigmas)\n",
    "\n",
    "    print_results('epoch_train', log_writer, epoch, {k: [np.mean(v)] for k, v in metrics.items()}, log_sigmas)\n",
    "\n",
    "\n",
    "    torch.save(model.state_dict(), f'quantile_model{epoch}.npz')\n",
    "\n",
    "    ############   TEST   ############\n",
    "    metrics_test = {\n",
    "        'loss': [],\n",
    "        'laplace_loss': [],\n",
    "        'laplace_loss_70': [],\n",
    "        'laplace_loss_last_3': [],\n",
    "        'laplace_loss_70_last_3': [],\n",
    "        'laplace_loss_first_last_3': [],\n",
    "        'laplace_loss_70_first_last_3': []\n",
    "    }\n",
    "\n",
    "    model.eval()\n",
    "    for cur_iter, data in tqdm(enumerate(test_dataset), desc='Test iter loop', leave=False):\n",
    "        cur_start_time = time.time()\n",
    "\n",
    "        FVC_true = data[2]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            all_preds = model(data)\n",
    "\n",
    "        agg_metrics = {\n",
    "            'loss': [],\n",
    "            'laplace_loss': [],\n",
    "            'laplace_loss_70': [],\n",
    "            'laplace_loss_last_3': [],\n",
    "            'laplace_loss_70_last_3': [],\n",
    "            'laplace_loss_first_last_3': [],\n",
    "            'laplace_loss_70_first_last_3': []\n",
    "        }\n",
    "        \n",
    "        for FVC_low, FVC_preds, FVC_high in all_preds:\n",
    "            log70 = torch.tensor(np.log(70))\n",
    "\n",
    "            pinball_loss =  PinballLoss(0.2)(FVC_true, FVC_low)\n",
    "            pinball_loss += PinballLoss(0.5)(FVC_true, FVC_preds)\n",
    "            pinball_loss += PinballLoss(0.8)(FVC_true, FVC_high)\n",
    "\n",
    "            log_sigmas = torch.log(torch.clamp_min(FVC_high - FVC_low, 1e-7))\n",
    "\n",
    "            laplace_loss = LaplaceLoss()(FVC_true, FVC_preds, log_sigmas)\n",
    "\n",
    "            cur_loss = 0.8 * pinball_loss + 0.2 * laplace_loss\n",
    "\n",
    "            agg_metrics['loss'] += [cur_loss.item()]\n",
    "\n",
    "            agg_metrics['laplace_loss'] += [LaplaceLoss()(FVC_true, FVC_preds, log_sigmas, metric=True).item()]\n",
    "            agg_metrics['laplace_loss_70'] += [LaplaceLoss()(FVC_true, FVC_preds, log70, metric=True).item()]\n",
    "\n",
    "            agg_metrics['laplace_loss_last_3'] += [LaplaceLoss()(FVC_true[-3:], FVC_preds[-3:],\n",
    "                                                                 log_sigmas[-3:], metric=True).item()]\n",
    "            agg_metrics['laplace_loss_70_last_3'] += [LaplaceLoss()(FVC_true[-3:], FVC_preds[-3:],\n",
    "                                                                    log70, metric=True).item()]\n",
    "\n",
    "            agg_metrics['laplace_loss_first_last_3'] += [agg_metrics['laplace_loss_last_3'][0]]\n",
    "            agg_metrics['laplace_loss_70_first_last_3'] += [agg_metrics['laplace_loss_70_last_3'][0]]\n",
    "\n",
    "        for metric, values in agg_metrics.items():\n",
    "            metrics_test[metric] += [np.mean(values)]\n",
    "\n",
    "        cur_end_time = time.time()\n",
    "\n",
    "        print_results('test', log_writer, (epoch + 1) * 1000 + cur_iter + 1, metrics_test, log_sigmas)\n",
    "\n",
    "    print_results('epoch_test', log_writer, epoch, {k: [np.mean(v)] for k, v in metrics_test.items()}, log_sigmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'quantile_model{epoch}.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_dataset[40]\n",
    "with torch.no_grad():\n",
    "    all_preds = model(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x178e0009c48>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(data[1], all_preds[3][0])\n",
    "plt.plot(data[1], data[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for g in optimizer.param_groups:\n",
    "#     g['lr'] *= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for name, p in model.named_parameters():\n",
    "#     if p.requires_grad:\n",
    "#         if p.grad is None:\n",
    "#             print(f'Has grad but it is None: {name[20:]:50}')\n",
    "#         else:\n",
    "#             print(f'{name[20:]:50} : {p.grad.data.cpu().min().item():15.3e}, {p.grad.data.cpu().max().item():15.3e}')\n",
    "#     else:\n",
    "#         print(f'No grad: {name[20:]:50}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
