{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: add crop for log_sigma and for abs diff for FVC\n",
    "# TODO: add normalization for data\n",
    "# TODO: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False\n",
    "\n",
    "import os\n",
    "import platform\n",
    "from collections import namedtuple\n",
    "import time\n",
    "\n",
    "# import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sparse\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.modules.loss import _Loss\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "# from torchvision import transforms\n",
    "# from torchsummary import summary\n",
    "# from efficientnet_pytorch_3d import EfficientNet3D\n",
    "from my_efficientnet_pytorch_3d import EfficientNet3D\n",
    "\n",
    "# from utils import *\n",
    "\n",
    "\n",
    "########################\n",
    "\n",
    "RUNNING_IN_KAGGLE = 'linux' in platform.platform().lower()\n",
    "IMAGE_PATH = \"../input/osic-pulmonary-fibrosis-progression/\" if RUNNING_IN_KAGGLE else 'data/'\n",
    "PROCESSED_PATH = 'FIX IT!' if RUNNING_IN_KAGGLE else 'data/processed-data/'  # TODO: fix this line\n",
    "\n",
    "dtype = torch.float32\n",
    "USE_GPU = True\n",
    "if USE_GPU and torch.cuda.is_available():\n",
    "    device = 'cuda:0'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "device = torch.device(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTDataset(Dataset):\n",
    "    _ReturnValue = namedtuple('ReturnValue', ['weeks', 'fvcs', 'features', 'masks', 'images'])\n",
    "\n",
    "    def __init__(\n",
    "            self, root, csv_path, train=True, test_size=0.25, random_state=42):\n",
    "        \"\"\"\n",
    "        :param dataset:\n",
    "\n",
    "        :param root:\n",
    "        :param train:\n",
    "        :param train_test_split:\n",
    "        :param random_state:\n",
    "        \"\"\"\n",
    "        assert test_size is not None\n",
    "\n",
    "        self.root = root\n",
    "        self.train = train\n",
    "        self.csv_path = csv_path\n",
    "        self.test_size = test_size\n",
    "        self.random_state = random_state\n",
    "\n",
    "        if not os.path.exists(self.root):\n",
    "            raise ValueError('Data is missing')\n",
    "\n",
    "        self._patients = list(sorted(os.listdir(self.root)))\n",
    "\n",
    "        if self.test_size == 0:\n",
    "            self._train_patients, self._test_patients = self._patients, []\n",
    "        else:\n",
    "            self._train_patients, self._test_patients = train_test_split(\n",
    "                self._patients, test_size=self.test_size, random_state=random_state\n",
    "            )\n",
    "\n",
    "        self._table_features = dict()\n",
    "        table_data = pd.read_csv(self.csv_path)\n",
    "        for patient in self._patients:\n",
    "            patient_data = table_data[table_data.Patient == patient]\n",
    "\n",
    "            all_weeks = patient_data.Weeks.tolist()\n",
    "            all_fvcs = patient_data.FVC.tolist()\n",
    "\n",
    "            all_weeks, all_fvcs = zip(*sorted(zip(all_weeks, all_fvcs), key=lambda x: x[0]))\n",
    "\n",
    "            age = sorted(zip(*np.unique(patient_data.Age, return_counts=True)), key=lambda x: x[1])[-1][0]\n",
    "            sex = sorted(zip(*np.unique(patient_data.Sex, return_counts=True)), key=lambda x: x[1])[-1][0]\n",
    "            smoking_status = sorted(zip(*np.unique(patient_data.SmokingStatus, return_counts=True)), key=lambda x: x[1])[-1][0]\n",
    "\n",
    "            sex = [0, 1] if sex == 'Female' else [1, 0]\n",
    "            smoking_status = (\n",
    "                [1, 0, 0] if smoking_status == 'Ex-smoker' else\n",
    "                [0, 1, 0] if smoking_status == 'Never smoked' else\n",
    "                [0, 0, 1] if smoking_status == 'Currently smokes' else\n",
    "                [0, 0, 0]\n",
    "            )\n",
    "            self._table_features[patient] = (\n",
    "                all_weeks, all_fvcs, [age] + sex + smoking_status\n",
    "            )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        patient = self._train_patients[index] if self.train else self._test_patients[index]\n",
    "        base_path = os.path.join(self.root, patient)\n",
    "\n",
    "        meta = np.load(os.path.join(base_path, 'meta.npy'), allow_pickle=True).tolist()\n",
    "        masks = sparse.load_npz(os.path.join(base_path, 'masks.npz'))\n",
    "        images = np.load(os.path.join(base_path, 'images.npy'))\n",
    "\n",
    "        meta_processed = dict()\n",
    "        for key, values in meta.items():\n",
    "            if key in {'SliceLocation', 'InstanceNumber'}:\n",
    "                continue\n",
    "            else:\n",
    "                unique_values, values_cnt = np.unique(values, return_counts=True, axis=0)\n",
    "                most_frequent = sorted(zip(unique_values, values_cnt), key=lambda x: x[1])[-1][0]\n",
    "                most_frequent = np.array(most_frequent).reshape(-1)\n",
    "                if key in {\n",
    "                    'SliceThickness', 'TableHeight', 'WindowCenter', 'WindowWidth'\n",
    "                }:\n",
    "                    meta_processed[key] = most_frequent[0]\n",
    "                elif key == 'PixelSpacing':\n",
    "                    if len(most_frequent) == 1:\n",
    "                        meta_processed['PixelSpacingX'], meta_processed['PixelSpacingY'] = (\n",
    "                            most_frequent[0], most_frequent[0]\n",
    "                        )\n",
    "                    else:\n",
    "                        meta_processed['PixelSpacingX'], meta_processed['PixelSpacingY'] = (\n",
    "                            most_frequent[0], most_frequent[1]\n",
    "                        )\n",
    "                elif key == 'PatientPosition':\n",
    "                    pass\n",
    "                elif key == 'PositionReferenceIndicator':\n",
    "                    pass\n",
    "\n",
    "        all_weeks, all_fvcs, features = self._table_features[patient]\n",
    "        features = [value for key, value in meta_processed.items()] + features\n",
    "\n",
    "        return CTDataset._ReturnValue(weeks=all_weeks, fvcs=all_fvcs, features=features, masks=masks, images=images)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._train_patients if self.train else self._test_patients)\n",
    "\n",
    "    def __repr__(self):\n",
    "        fmt_str = 'OSIC Pulmonary Fibrosis Progression Dataset ' + self.__class__.__name__ + '\\n'\n",
    "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
    "        tmp = 'train' if self.train is True else 'test'\n",
    "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
    "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
    "        return fmt_str\n",
    "\n",
    "\n",
    "class LaplaceLoss(_Loss):\n",
    "    def forward(self, y_true, preds, log_sigma):\n",
    "        abs_diff = (y_true - preds).abs()\n",
    "#         abs_diff.clamp_max_(1_000)\n",
    "        log_sigma.clamp_(-5, 5)  # -np.log(70), np.log(70)\n",
    "#         log_sigma.clamp_min_(-5)\n",
    "        losses = np.sqrt(2) * abs_diff / log_sigma.exp() + log_sigma + np.log(2) / 2\n",
    "        return losses.mean()\n",
    "\n",
    "\n",
    "class SqueezeLayer(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.squeeze()\n",
    "\n",
    "\n",
    "class FeatureExtractor(nn.Module):\n",
    "    def __init__(self, net):\n",
    "        super().__init__()\n",
    "        self.net = net\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net.extract_features(x.unsqueeze(0).unsqueeze(0))\n",
    "\n",
    "\n",
    "class OSICNet(nn.Module):\n",
    "    def __init__(self, dtype, device, efficient_net_model_number, hidden_size, dropout_rate):  # , output_size\n",
    "        super().__init__()\n",
    "\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "\n",
    "        self.CT_features_extractor = nn.Sequential(\n",
    "            FeatureExtractor(\n",
    "                EfficientNet3D.from_name(\n",
    "                    f'efficientnet-b{efficient_net_model_number}', override_params={'num_classes': 1}, in_channels=1\n",
    "                )\n",
    "            ),\n",
    "            nn.AdaptiveAvgPool3d(1),\n",
    "            SqueezeLayer()\n",
    "        )\n",
    "\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(1280, hidden_size),  # 1294\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(hidden_size, 5)  # output_size\n",
    "        )\n",
    "        \n",
    "        self._initialize_weights()\n",
    "\n",
    "        self.CT_features_extractor.to(self.device)\n",
    "        self.predictor.to(self.device)\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Conv3d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.kernel_size[2] * m.out_channels\n",
    "                m.weight.data.normal_(0, np.sqrt(2. / n))\n",
    "            elif isinstance(m, torch.nn.BatchNorm3d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        \n",
    "#     def forward(self, data):\n",
    "#         mean_dataset, std_dataset = -971.4692260919278, 117.84143467421829\n",
    "#         lungs = -1000 * (1.0 - data.masks) + data.masks * data.images\n",
    "#         lungs = (lungs - mean_dataset) / std_dataset\n",
    "#         lungs = torch.tensor(lungs, dtype=self.dtype, device=self.device)\n",
    "#         lungs_features = self.CT_features_extractor(lungs)\n",
    "\n",
    "#         data_weeks = torch.tensor(data.weeks, dtype=self.dtype)\n",
    "#         weeks = torch.empty(len(data.weeks), 4, dtype=self.dtype)\n",
    "#         weeks[:, 3] = 1\n",
    "#         weeks[:, 2] = data_weeks\n",
    "#         weeks[:, 1] = data_weeks ** 2\n",
    "#         weeks[:, 0] = data_weeks ** 3\n",
    "\n",
    "#         agg_loss = 0\n",
    "#         for week, FVC in zip(data.weeks, data.fvcs):\n",
    "#             table_features = torch.tensor(np.r_[week, FVC, data.features], dtype=self.dtype, device=self.device)\n",
    "#             X = lungs_features  # torch.cat([lungs_features, table_features])\n",
    "\n",
    "#             pred_numbers = self.predictor(X).cpu()\n",
    "#             coefs = pred_numbers[:4]\n",
    "#             log_sigma = pred_numbers[4]\n",
    "\n",
    "#             FVC_preds = (weeks * coefs).sum(dim=1)\n",
    "#             FVC_true = torch.tensor(data.fvcs, dtype=self.dtype)\n",
    "\n",
    "#             agg_loss += LaplaceLoss()(FVC_true, FVC_preds, log_sigma)\n",
    "\n",
    "#         return agg_loss / len(data.weeks)\n",
    "\n",
    "    def forward(self, data):\n",
    "        mean_dataset, std_dataset = -971.4692260919278, 117.84143467421829\n",
    "        lungs = -1000 * (1.0 - data.masks) + data.masks * data.images\n",
    "        lungs = (lungs - mean_dataset) / std_dataset\n",
    "        lungs = torch.tensor(lungs, dtype=self.dtype, device=self.device)\n",
    "        lungs_features = self.CT_features_extractor(lungs)\n",
    "\n",
    "#         data_weeks = torch.tensor(data.weeks, dtype=self.dtype)\n",
    "#         weeks = torch.empty(len(data.weeks), 4, dtype=self.dtype)\n",
    "#         weeks[:, 3] = 1\n",
    "#         weeks[:, 2] = data_weeks\n",
    "#         weeks[:, 1] = data_weeks ** 2\n",
    "#         weeks[:, 0] = data_weeks ** 3\n",
    "\n",
    "#         agg_loss = 0\n",
    "        all_preds = []\n",
    "        for week, FVC in zip(data.weeks, data.fvcs):\n",
    "            table_features = torch.tensor(np.r_[week, FVC, data.features], dtype=self.dtype, device=self.device)\n",
    "            X = lungs_features  # torch.cat([lungs_features, table_features])\n",
    "\n",
    "            pred_numbers = self.predictor(X).cpu()\n",
    "            all_preds.append(pred_numbers)\n",
    "#             coefs = pred_numbers[:4]\n",
    "#             log_sigma = pred_numbers[4]\n",
    "\n",
    "#             FVC_preds = (weeks * coefs).sum(dim=1)\n",
    "#             FVC_true = torch.tensor(data.fvcs, dtype=self.dtype)\n",
    "            \n",
    "#             agg_loss += LaplaceLoss()(FVC_true, FVC_preds, log_sigma)\n",
    "\n",
    "#         return agg_loss / len(data.weeks)\n",
    "        return all_preds\n",
    "\n",
    "\n",
    "class LinearDecayLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, start_epoch, stop_epoch, start_lr, stop_lr, last_epoch=-1):\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.start_epoch = start_epoch\n",
    "        self.stop_epoch = stop_epoch\n",
    "\n",
    "        self.start_lr = start_lr\n",
    "        self.stop_lr = stop_lr\n",
    "\n",
    "        self.last_epoch = last_epoch\n",
    "\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self) -> list:\n",
    "        if self.last_epoch < self.start_epoch:\n",
    "            new_lr = self.start_lr\n",
    "        elif self.last_epoch > self.stop_epoch:\n",
    "            new_lr = self.stop_lr\n",
    "        else:\n",
    "            new_lr = self.start_lr + (\n",
    "                (self.stop_lr - self.start_lr) *\n",
    "                (self.last_epoch - self.start_epoch) /\n",
    "                (self.stop_epoch - self.start_epoch)\n",
    "            )\n",
    "        return [new_lr for _ in self.optimizer.param_groups]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset = CTDataset(\n",
    "#     f'{PROCESSED_PATH}/train',\n",
    "#     f'{IMAGE_PATH}/train.csv',\n",
    "#     train=True, test_size=0.25, random_state=42\n",
    "# )\n",
    "\n",
    "# test_dataset = CTDataset(\n",
    "#     f'{PROCESSED_PATH}/train',\n",
    "#     f'{IMAGE_PATH}/train.csv',\n",
    "#     train=False, test_size=0.25, random_state=42\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_all = CTDataset(\n",
    "    f'{PROCESSED_PATH}/train',\n",
    "    f'{IMAGE_PATH}/train.csv',\n",
    "    train=True, test_size=0.0, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ReturnValue(weeks=(-4, 5, 7, 9, 11, 17, 29, 41, 57), fvcs=(2315, 2214, 2061, 2144, 2069, 2101, 2000, 2064, 2057), features=[0.390625, 1.3046879768371582, 0.1953125, 130.0, -500.0, -1500.0, 79, 1, 0, 1, 0, 0], masks=<COO: shape=(192, 256, 256), dtype=bool, nnz=1535057, fill_value=False>, images=array([[[-1000., -1000., -1000., ..., -1000., -1000., -1000.],\n",
       "        [-1000., -1000., -1000., ..., -1000., -1000., -1000.],\n",
       "        [-1000., -1000., -1000., ..., -1000., -1000., -1000.],\n",
       "        ...,\n",
       "        [-1000., -1000., -1000., ..., -1000., -1000., -1000.],\n",
       "        [-1000., -1000., -1000., ..., -1000., -1000., -1000.],\n",
       "        [-1000., -1000., -1000., ..., -1000., -1000., -1000.]],\n",
       "\n",
       "       [[-1000., -1000., -1000., ..., -1000., -1000., -1000.],\n",
       "        [-1000., -1000., -1000., ..., -1000., -1000., -1000.],\n",
       "        [-1000., -1000., -1000., ..., -1000., -1000., -1000.],\n",
       "        ...,\n",
       "        [-1000., -1000., -1000., ..., -1000., -1000., -1000.],\n",
       "        [-1000., -1000., -1000., ..., -1000., -1000., -1000.],\n",
       "        [-1000., -1000., -1000., ..., -1000., -1000., -1000.]],\n",
       "\n",
       "       [[-1000., -1000., -1000., ..., -1000., -1000., -1000.],\n",
       "        [-1000., -1000., -1000., ..., -1000., -1000., -1000.],\n",
       "        [-1000., -1000., -1000., ..., -1000., -1000., -1000.],\n",
       "        ...,\n",
       "        [-1000., -1000., -1000., ..., -1000., -1000., -1000.],\n",
       "        [-1000., -1000., -1000., ..., -1000., -1000., -1000.],\n",
       "        [-1000., -1000., -1000., ..., -1000., -1000., -1000.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-1000., -1000., -1000., ..., -1000., -1000., -1000.],\n",
       "        [-1000., -1000., -1000., ..., -1000., -1000., -1000.],\n",
       "        [-1000., -1000., -1000., ..., -1000., -1000., -1000.],\n",
       "        ...,\n",
       "        [-1000., -1000., -1000., ..., -1000., -1000., -1000.],\n",
       "        [-1000., -1000., -1000., ..., -1000., -1000., -1000.],\n",
       "        [-1000., -1000., -1000., ..., -1000., -1000., -1000.]],\n",
       "\n",
       "       [[-1000., -1000., -1000., ..., -1000., -1000., -1000.],\n",
       "        [-1000., -1000., -1000., ..., -1000., -1000., -1000.],\n",
       "        [-1000., -1000., -1000., ..., -1000., -1000., -1000.],\n",
       "        ...,\n",
       "        [-1000., -1000., -1000., ..., -1000., -1000., -1000.],\n",
       "        [-1000., -1000., -1000., ..., -1000., -1000., -1000.],\n",
       "        [-1000., -1000., -1000., ..., -1000., -1000., -1000.]],\n",
       "\n",
       "       [[-1000., -1000., -1000., ..., -1000., -1000., -1000.],\n",
       "        [-1000., -1000., -1000., ..., -1000., -1000., -1000.],\n",
       "        [-1000., -1000., -1000., ..., -1000., -1000., -1000.],\n",
       "        ...,\n",
       "        [-1000., -1000., -1000., ..., -1000., -1000., -1000.],\n",
       "        [-1000., -1000., -1000., ..., -1000., -1000., -1000.],\n",
       "        [-1000., -1000., -1000., ..., -1000., -1000., -1000.]]],\n",
       "      dtype=float32))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_all[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# images = [-1000 * (1.0 - dataset_all[i].masks) + dataset_all[i].masks * dataset_all[i].images\n",
    "#           for i in range(len(dataset_all))]\n",
    "\n",
    "data = [(dataset_all[i].weeks, dataset_all[i].fvcs, dataset_all[i].features) for i in range(len(dataset_all))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((-4, 5, 7, 9, 11, 17, 29, 41, 57),\n",
       " (2315, 2214, 2061, 2144, 2069, 2101, 2000, 2064, 2057),\n",
       " [0.390625,\n",
       "  1.3046879768371582,\n",
       "  0.1953125,\n",
       "  130.0,\n",
       "  -500.0,\n",
       "  -1500.0,\n",
       "  79,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31.861846352485475, 23.240045178171002)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_weeks = []\n",
    "for i in range(len(data)):\n",
    "    all_weeks += list(data[i][0])\n",
    "\n",
    "weeks = np.array(all_weeks)\n",
    "weeks.mean(), weeks.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2690.479018721756, 832.5021066817238)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_FVCs = []\n",
    "for i in range(len(data)):\n",
    "    all_FVCs += list(data[i][1])\n",
    "\n",
    "FVCs = np.array(all_FVCs)\n",
    "FVCs.mean(), FVCs.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_ftrs = []\n",
    "for i in range(len(data)):\n",
    "    other_ftrs.append(list(data[i][2]))\n",
    "\n",
    "other_ftrs = np.array(other_ftrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.765619</td>\n",
       "      <td>1.423738</td>\n",
       "      <td>1.256083</td>\n",
       "      <td>133.766080</td>\n",
       "      <td>-523.857955</td>\n",
       "      <td>-1241.545455</td>\n",
       "      <td>67.261364</td>\n",
       "      <td>0.789773</td>\n",
       "      <td>0.210227</td>\n",
       "      <td>0.670455</td>\n",
       "      <td>0.278409</td>\n",
       "      <td>0.051136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.354473</td>\n",
       "      <td>0.147985</td>\n",
       "      <td>0.993669</td>\n",
       "      <td>58.766154</td>\n",
       "      <td>192.607394</td>\n",
       "      <td>850.688735</td>\n",
       "      <td>7.088009</td>\n",
       "      <td>0.408632</td>\n",
       "      <td>0.408632</td>\n",
       "      <td>0.471389</td>\n",
       "      <td>0.449495</td>\n",
       "      <td>0.220904</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2           3           4            5   \\\n",
       "mean  2.765619  1.423738  1.256083  133.766080 -523.857955 -1241.545455   \n",
       "std   2.354473  0.147985  0.993669   58.766154  192.607394   850.688735   \n",
       "\n",
       "             6         7         8         9         10        11  \n",
       "mean  67.261364  0.789773  0.210227  0.670455  0.278409  0.051136  \n",
       "std    7.088009  0.408632  0.408632  0.471389  0.449495  0.220904  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.DataFrame(other_ftrs).describe().loc[['mean', 'std']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.76561876e+00,  1.42373805e+00,  1.25608294e+00,  1.33766080e+02,\n",
       "       -5.23857955e+02, -1.24154545e+03,  6.72613636e+01,  7.89772727e-01,\n",
       "        2.10227273e-01,  6.70454545e-01,  2.78409091e-01,  5.11363636e-02])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_ftrs.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.34777445e+00, 1.47563586e-01, 9.90841780e-01, 5.85989667e+01,\n",
       "       1.92059435e+02, 8.48268563e+02, 7.06784382e+00, 4.07469958e-01,\n",
       "       4.07469958e-01, 4.70048134e-01, 4.48215873e-01, 2.20275818e-01])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_ftrs.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(176, 12)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_ftrs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.18618464e+01,  2.69047902e+03,  2.76561876e+00,  1.42373805e+00,\n",
       "        1.25608294e+00,  1.33766080e+02, -5.23857955e+02, -1.24154545e+03,\n",
       "        6.72613636e+01,  7.89772727e-01,  2.10227273e-01,  6.70454545e-01,\n",
       "        2.78409091e-01,  5.11363636e-02])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_means = np.r_[weeks.mean(), FVCs.mean(), other_ftrs.mean(axis=0)]\n",
    "all_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.32400452e+01, 8.32502107e+02, 2.34777445e+00, 1.47563586e-01,\n",
       "       9.90841780e-01, 5.85989667e+01, 1.92059435e+02, 8.48268563e+02,\n",
       "       7.06784382e+00, 4.07469958e-01, 4.07469958e-01, 4.70048134e-01,\n",
       "       4.48215873e-01, 2.20275818e-01])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_stds = np.r_[weeks.std(), FVCs.std(), other_ftrs.std(axis=0)]\n",
    "all_stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_image = 0\n",
    "sum_sq_image = 0\n",
    "for image in images:\n",
    "    sum_image += image.sum()\n",
    "    sum_sq_image += (image ** 2).sum()\n",
    "\n",
    "N = np.prod((176., 192., 256., 256.))\n",
    "\n",
    "mean = sum_image / N\n",
    "\n",
    "mean\n",
    "\n",
    "var = sum_sq_image / N + mean ** 2 - 2 * mean * sum_image / N\n",
    "\n",
    "std = var ** 0.5\n",
    "\n",
    "mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
